{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerritgr/MaskedGraphGen/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulnOzsCCmA8g"
      },
      "source": [
        "\n",
        "# <img src=\"https://mario.wiki.gallery/images/thumb/8/89/MPS_Toad_Artwork.png/170px-MPS_Toad_Artwork.png\" alt=\"Dataset Summary\" width=\"3%\">  Batched Toad  <img src=\"https://mario.wiki.gallery/images/thumb/8/89/MPS_Toad_Artwork.png/170px-MPS_Toad_Artwork.png\" alt=\"Dataset Summary\" width=\"3%\"> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCWLJT5DLH78"
      },
      "source": [
        "Todos:\n",
        "- weightes sampling\n",
        "- multi-channel\n",
        "- uncertainty\n",
        "- better ensable like skip one during training or learn weights\n",
        "- to make network better: virutal node, degree input, spectral input, edge features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-k6-4CDmEwH"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csn1_ff8mJCE"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  os.system(\"pip install wandb -Uq\")\n",
        "  #!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "  #os.system(\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\")\n",
        "  os.system(\"pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\")\n",
        "  os.system(\"pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\")\n",
        "  os.system(\"pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\")\n",
        "  os.system(\"pip install pyemd\")\n",
        "\n",
        "\n",
        "PATH_TO_NOTEBOOK = \"Batched Toad 3.ipynb\"\n",
        "PROJECT_NAME = \"toad_planar_docker\"\n",
        "KEY = \"\"\n",
        "WANDB_MODE = \"online\"   # \"online\" or \"dryrun\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN9PW__MvfFk"
      },
      "outputs": [],
      "source": [
        "def get_wand_api_key():\n",
        "  api_key = KEY\n",
        "  # use   echo \"your_api_key_here\" > ~/api_key.txt   to write the api key to the home file dir    \n",
        "  if api_key != \"\":\n",
        "    return api_key\n",
        "  home_dir = os.path.expanduser('~')\n",
        "  file_path = os.path.join(home_dir, 'api_key.txt')\n",
        "  with open(file_path, 'r') as file:\n",
        "      api_key = file.read().strip()\n",
        "  return api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUcQ44Bj42GY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKbKpbiSmMfW"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 100 # Set this to 300 to get better image quality\n",
        "from PIL import Image # We use PIL to load images\n",
        "import seaborn as sns\n",
        "import imageio # to generate .gifs\n",
        "\n",
        "# always good to have\n",
        "import glob, random, os, traceback\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import math\n",
        "\n",
        "# the typical pytorch imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear as Lin\n",
        "from torch.nn import Sequential as Seq\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyG\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "#Video\n",
        "from PIL import Image, ImageDraw\n",
        "#import cv2\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAyrOuvomPAE"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaElc28pmSzf"
      },
      "outputs": [],
      "source": [
        "NUM_NODES = 12 #12   \n",
        "DEGREE = 3 #3\n",
        "NUM_EPOCHS = 2000\n",
        "\n",
        "NUM_SAMPLES = 1000\n",
        "NUM_GRAPHS_GENERATE = 40\n",
        "\n",
        "NO_EDGE_INDICATOR = 3.0\n",
        "EDGE_INDICATOR = 1.0 \n",
        "DUMMY = 0.0 # this node is an actual node not a placeholder for an edge\n",
        "MASK = 2.0\n",
        "NUM_CLASSES = 4  #todo change to 5 an\n",
        "\n",
        "EPSILON = 0.000000001\n",
        "\n",
        "NODE_FEATURE_EXTEND = 3\n",
        "\n",
        "# do i need this\n",
        "#BATCH_SIZE_TEST = 10\n",
        "\n",
        "#TIME_EMBEDDING_DIM = 1\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#DEVICE = torch.device('cpu')\n",
        "DEVICE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWpdimGn3fDH"
      },
      "source": [
        "### Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCtxrGI4mVz2"
      },
      "outputs": [],
      "source": [
        "# from stack overflow\n",
        "def str_to_float(s, encoding=\"utf-8\"):\n",
        "  from zlib import crc32\n",
        "  def bytes_to_float(b):\n",
        "    return float(crc32(b) & 0xffffffff) / 2**32\n",
        "  return bytes_to_float(s.encode(encoding))\n",
        "\n",
        "    \n",
        "def set_seeds(seed=42):\n",
        "  if not \"int\" in str(type(seed)):\n",
        "    seed = int(str_to_float(str(seed))*1000 % 100000000)\n",
        "  np.random.seed(seed)\n",
        "  torch.random.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32hyXlJG8360"
      },
      "source": [
        "## Weights and Biases Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_pixfTW84rq"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=get_wand_api_key())\n",
        "\n",
        "sweep_config = {\n",
        "    \"name\": \"graph-unmasking\",\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"graph-unmasking/gen-loss\",\n",
        "        \"goal\": \"minimize\",\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"batch_size\": {\"values\": [25, 50]},  # write only\n",
        "        \"hidden_dim\": {\"values\": [16,32, 64]},  \n",
        "        \"hidden_layer\": {\"values\": [4,5,6]},\n",
        "        \"learning_rate\": {\"values\": [0.00001,0.0001]},\n",
        "        \"dropout\": {\"values\": [0.0]},\n",
        "        \"normalization\": {\"values\": [True, False]},\n",
        "        \"model\": {\"values\": [\"pna\"]},  # [ \"pnamulti\", \"pna\",\"pna2\", \"attention\",\"transformer\", \"unet\"]}, \n",
        "        \"candidate_selection_radius\": {\"values\": [1, 5]},\n",
        "        \"loss\": {\"values\": [\"bce\"]},   # \"loss\": {\"values\": [\"l2\",\"l1\", \"bce\"]}, \n",
        "        \"single_pass\": {\"values\": [True]},  \n",
        "        \"towers\": {\"values\": [1, 2]},   \n",
        "        \"noise_probability\": {\"values\": [0.0, 0.05, 0.1]},\n",
        "        \"graph_transform\": {\"values\": [True, False]},    \n",
        "        \"pre_post_layers\": {\"values\": [1]}, \n",
        "    },\n",
        "}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(sweep_config)\n",
        "torch.set_printoptions(threshold=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQyrEbzkmMCL"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_zto3HQmp5v"
      },
      "outputs": [],
      "source": [
        "def lift_nx_to_pyg(g_nx):\n",
        "  import torch_geometric.transforms as T\n",
        "  transform = T.Compose([T.ToUndirected()])\n",
        "\n",
        "  # g_nx has to have node_labels 0 ... number_of_nodes-1\n",
        "  num_nodes = g_nx.number_of_nodes()\n",
        "  g_complete_nx = nx.complete_graph(num_nodes)\n",
        "\n",
        "  edges_new = list()\n",
        "  node_value_dict = dict()\n",
        "  for v1, v2 in g_complete_nx.edges():\n",
        "    v3 = (v1+1)*10000+(v2+1)*100000000000\n",
        "    edges_new.append((v1, v3))\n",
        "    edges_new.append((v2, v3))\n",
        "    node_value_dict[v1] = DUMMY\n",
        "    node_value_dict[v2] = DUMMY\n",
        "    node_value_dict[v3] = EDGE_INDICATOR if g_nx.has_edge(v1,v2) else NO_EDGE_INDICATOR\n",
        "\n",
        "  graph_new = nx.from_edgelist(edges_new)\n",
        "  for node, x_value in node_value_dict.items():\n",
        "    graph_new.nodes[node]['x'] = x_value\n",
        "\n",
        "  graph_new = nx.convert_node_labels_to_integers(graph_new, ordering='sorted')\n",
        "\n",
        "  g = from_networkx(graph_new, group_node_attrs=['x'])\n",
        "  g = transform(g)  # probably unnecs.\n",
        "  return g\n",
        "\n",
        "\n",
        "def reduce_nx_graph(g_old):\n",
        "  g_new = nx.Graph()\n",
        "  for v_i in g_old.nodes():\n",
        "    if g_old.nodes[v_i]['x'] == DUMMY:\n",
        "      g_new.add_node(v_i)\n",
        "      g_new.nodes[v_i]['x'] = DUMMY\n",
        "\n",
        "  for v_i in g_old.nodes():\n",
        "    if g_old.nodes[v_i]['x'] == EDGE_INDICATOR:\n",
        "      neigh_list = list(g_old.neighbors(v_i))\n",
        "      assert(len(neigh_list) == 2)\n",
        "      g_new.add_edge(neigh_list[0], neigh_list[1])\n",
        "\n",
        "  return g_new\n",
        "\n",
        "def pyg_graph_to_nx(g_pyg):\n",
        "  g_nx = to_networkx(g_pyg, node_attrs=['x'], to_undirected=True,)\n",
        "  g_nx = reduce_nx_graph(g_nx)\n",
        "  return g_nx\n",
        "\n",
        "def draw_pyg(g_pyg, ax = None, filename = None):\n",
        "  if ax is None:\n",
        "    plt.clf()\n",
        "\n",
        "  g_nx = pyg_graph_to_nx(g_pyg)\n",
        "\n",
        "  node_labels = {i: g_nx.nodes[i]['x'] for i in g_nx.nodes}\n",
        "  pos = nx.spring_layout(g_nx, seed=1234)\n",
        "  try:\n",
        "    nx.draw(g_nx, with_labels = True, labels=node_labels, ax=ax, pos=pos)\n",
        "  except:\n",
        "    pass\n",
        "  if filename is not None:\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "  return g_nx\n",
        "\n",
        "\n",
        "def shuffle_tensor(x):\n",
        "  x = x.clone()\n",
        "  x_shape = x.shape\n",
        "  x = x.flatten()\n",
        "  x = x[torch.randperm(x.numel())]\n",
        "  return x.reshape(x.shape)\n",
        "\n",
        "\n",
        "def note_features_to_one_hot(x):\n",
        "  # create indicator vector for NUM_CLASSES classes, values outside are set to all zeros (first to special class that then gets truncated)\n",
        "  x_cut = torch.where((x < -0.1) | (x > NUM_CLASSES-0.5), torch.tensor(NUM_CLASSES, device = DEVICE, dtype=torch.float), x)\n",
        "  x_one_hot = torch.zeros(x.shape[0], NUM_CLASSES+1, device = DEVICE)\n",
        "  x_one_hot.scatter_(1, x_cut.round().long().view(-1).unsqueeze(1), 1)\n",
        "  x_one_hot = x_one_hot[:,0:NUM_CLASSES]\n",
        "  return x_one_hot\n",
        "\n",
        "def global_mask_pool(x, x_in, batch):\n",
        "  mask_indicator = torch.lt(torch.sum(x_in, dim=1), 0.5).view(-1, 1)\n",
        "  x = x * mask_indicator\n",
        "  x = global_add_pool(x, batch)\n",
        "  return x \n",
        "\n",
        "\n",
        "def flip_edges_randomly(tensor, probability):\n",
        "  if probability < EPSILON:\n",
        "    return tensor\n",
        "\n",
        "  # Find the positions where the tensor has values 1.0 or 3.0\n",
        "  ones_positions = (tensor == EDGE_INDICATOR).to(DEVICE)\n",
        "  threes_positions = (tensor == NO_EDGE_INDICATOR).to(DEVICE)\n",
        "\n",
        "  # Generate a random binary mask for switching values\n",
        "  mask = torch.rand(tensor.shape, device=DEVICE) < probability\n",
        "\n",
        "  # Switch 1.0s to 3.0s using the random binary mask\n",
        "  tensor = torch.where(ones_positions & mask, torch.tensor(NO_EDGE_INDICATOR, device=DEVICE), tensor)\n",
        "\n",
        "  # Switch 3.0s to 1.0s using the inverted random binary mask\n",
        "  tensor = torch.where(threes_positions & mask, torch.tensor(EDGE_INDICATOR, device=DEVICE), tensor)\n",
        "\n",
        "  return tensor\n",
        "\n",
        "def extend_node_features(x, edge_index, batch):\n",
        "  assert(NODE_FEATURE_EXTEND == 3)\n",
        "\n",
        "  num_graphs = batch.max()+1\n",
        "  num_nodes = x.shape[0]\n",
        "  degree_feature = torch.zeros([num_nodes, NODE_FEATURE_EXTEND], device=DEVICE)\n",
        "  for i in range(edge_index.shape[1]):\n",
        "    src_node = edge_index[0,i]\n",
        "    target_node = edge_index[1,i]\n",
        "    # if src is real node and connection is edge indicator \n",
        "    if x[src_node, round(DUMMY)].item() > 0.5 and x[target_node, round(EDGE_INDICATOR)].item() > 0.5:\n",
        "      degree_feature[src_node,0] += 1.0\n",
        "    if x[src_node, round(DUMMY)].item() > 0.5 and x[target_node, round(MASK)].item() > 0.5:\n",
        "      degree_feature[src_node,1] += 1.0\n",
        "  for i in range(num_graphs):\n",
        "    num_mask_in_this_graph = torch.sum(x[batch == i,round(MASK)])\n",
        "    degree_feature[batch == i, 2] = num_mask_in_this_graph\n",
        "  x = torch.cat([x, degree_feature], dim=1)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEusLk_fd7hC"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.transforms as T\n",
        "transform = T.Compose([T.ToUndirected()])\n",
        "\n",
        "edge_index = torch.tensor([[0, 1, 2, 3, 1, 2, 3],\n",
        "                           [1, 2, 3, 1, 0, 1, 2]], dtype=torch.long).to(DEVICE)\n",
        "x = torch.tensor([[0,1,0,0], [1,0,0,1], [0,0,1,1], [0,1,0,0]], dtype=torch.float).to(DEVICE)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "data = transform(data)\n",
        "data\n",
        "#extend_node_features(data.x, data.edge_index, batch=torch.zeros(data.x.shape[0], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ3hIT0pmBXp"
      },
      "outputs": [],
      "source": [
        "edge_index = torch.tensor([[0, 1, 2, 3, 1, 2, 3, 1],\n",
        "                           [1, 2, 3, 1, 0, 1, 2, 3]], dtype=torch.long).to(DEVICE)\n",
        "x = torch.tensor([[-1], [0], [1], [1]], dtype=torch.float).to(DEVICE)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "g = torch_geometric.utils.to_networkx(data, to_undirected=True, node_attrs=['x'])\n",
        "try:\n",
        "  nx.draw(g, pos=nx.spring_layout(g),  with_labels = True)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiLapF9HnilF"
      },
      "outputs": [],
      "source": [
        "data = lift_nx_to_pyg(g)\n",
        "data, data.x, data.edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgkOkx4amnfY"
      },
      "outputs": [],
      "source": [
        "draw_pyg(data, filename='examplegraph.png')\n",
        "if IN_COLAB:\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaElQYAbWu8M"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwdGZitSyjKi"
      },
      "source": [
        "### d-Regular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCpXui1mW9mp"
      },
      "outputs": [],
      "source": [
        "class ShuffleList:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        random.shuffle(self.data)\n",
        "        self.index = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.index < len(self.data):\n",
        "            value = self.data[self.index]\n",
        "            self.index += 1\n",
        "            return value\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgbW4Z0tWuN1"
      },
      "outputs": [],
      "source": [
        "def build_dataset(num_nodes=NUM_NODES, num_samples=NUM_SAMPLES, degree=DEGREE, seed=1234):\n",
        "  graph_dataset = list() \n",
        "\n",
        "  for _ in range(num_samples):\n",
        "    while True:\n",
        "      seed += 1\n",
        "      graph = nx.random_regular_graph(d=degree, n=num_nodes, seed=seed)\n",
        "      if nx.is_connected(graph):\n",
        "        graph.x = torch.ones(num_nodes).t()\n",
        "        graph_dataset.append(lift_nx_to_pyg(graph))\n",
        "        break\n",
        "\n",
        "  graph_dataset_train = graph_dataset[:int(len(graph_dataset)*0.8)]\n",
        "  graph_dataset_test = graph_dataset[int(len(graph_dataset)*0.8):]\n",
        "  return ShuffleList(graph_dataset_train), ShuffleList(graph_dataset_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QWWQlO36fzH"
      },
      "outputs": [],
      "source": [
        "#def build_test_set_nx(seed):\n",
        "#  graph_dataset = build_dataset(num_samples=10, seed=seed)\n",
        "#  graph_dataset_nx = list()\n",
        "#  for i, g in enumerate(graph_dataset):\n",
        "#    nx_orig_graph = draw_pyg(g, filename=f\"test_graph_sample{str(i).zfill(5)}.jpg\")\n",
        "#    graph_dataset_nx.append(nx_orig_graph)\n",
        "#  return graph_dataset_nx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKqFT6D7XHE7"
      },
      "outputs": [],
      "source": [
        "dataset, _  = build_dataset()\n",
        "data = dataset[0]\n",
        "draw_pyg(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2FUnVjG4Cwh"
      },
      "outputs": [],
      "source": [
        "# TODO dataloader\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE_TEST, shuffle=True)\n",
        "for step, data in enumerate(train_loader):\n",
        "  print(f'Step {step + 1}:')\n",
        "  print('=======')\n",
        "  print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "  print(data)\n",
        "  print()\n",
        "  print(data.x.flatten(), data.edge_index, data.batch)\n",
        "  break\n",
        "\n",
        "\n",
        "# for gen of training sample:\n",
        "# 1) mask 0 ... (all -1) (inclusivly) \n",
        "# 2) swich exactly 1 of the unmasked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS8w-Xw0ynzU"
      },
      "source": [
        "### Planar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlDyM555ypd8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import random_split, Dataset\n",
        "import torch_geometric.utils\n",
        "\n",
        "def load_dataset_planar():\n",
        "  nx_set = list()\n",
        "  num_nodes = 64\n",
        "  graph_dataset = list() \n",
        "  if not os.path.exists(\"SPECTRE\"):\n",
        "    os.system(\"git clone https://github.com/KarolisMart/SPECTRE.git\")\n",
        "  adjs, eigvals, eigvecs, n_nodes, max_eigval, min_eigval, same_sample, n_max = torch.load('SPECTRE/data/planar_64_200.pt')\n",
        "  for adj in adjs:\n",
        "    # from adj matrix to nx\n",
        "    graph = nx.from_numpy_array(adj.numpy())\n",
        "    # from nx to pyg\n",
        "    graph.x = torch.ones(num_nodes).t()\n",
        "    graph_dataset.append(lift_nx_to_pyg(graph))\n",
        "\n",
        "  #graph_dataset_train = graph_dataset[:int(len(graph_dataset)*0.8)]\n",
        "  #graph_dataset_test = graph_dataset[int(len(graph_dataset)*0.8):]\n",
        "  #return ShuffleList(graph_dataset_train), ShuffleList(graph_dataset_test)\n",
        "\n",
        "  test_len = int(round(len(graph_dataset) * 0.2))\n",
        "  train_len = int(round((len(graph_dataset) - test_len) * 0.8))\n",
        "  val_len = len(graph_dataset) - train_len - test_len\n",
        "  print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
        "  splits = random_split(graph_dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))\n",
        "  datasets = {'train': splits[0], 'val': splits[1], 'test': splits[2]}\n",
        "  return ShuffleList(list(datasets['train'])), ShuffleList(list(datasets['test']))\n",
        "\n",
        "\n",
        "load_dataset_planar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvjLUJSMUb_E"
      },
      "outputs": [],
      "source": [
        "graph_dataset, _ = load_dataset_planar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OIcsvOpU9AC"
      },
      "outputs": [],
      "source": [
        "graph_dataset[0],graph_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obBv2OMEWaKA"
      },
      "outputs": [],
      "source": [
        "#draw_pyg(graph_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL2Kr3jD-WwW"
      },
      "source": [
        "## Loss & Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bFAJnqX-ZO_"
      },
      "outputs": [],
      "source": [
        "import pyemd\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "from scipy.linalg import toeplitz\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "PRINT_TIME = False\n",
        "\n",
        "def degree_worker(G):\n",
        "    return np.array(nx.degree_histogram(G))\n",
        "\n",
        "def degree_stats(graph_ref_list, graph_pred_list, is_parallel=True, compute_emd=False):\n",
        "    ''' Compute the distance between the degree distributions of two unordered sets of graphs.\n",
        "        Args:\n",
        "            graph_ref_list, graph_target_list: two lists of networkx graphs to be evaluated\n",
        "        '''\n",
        "    sample_ref = []\n",
        "    sample_pred = []\n",
        "    # in case an empty graph is generated\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    prev = datetime.now()\n",
        "    if is_parallel:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for deg_hist in executor.map(degree_worker, graph_ref_list):\n",
        "                sample_ref.append(deg_hist)\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for deg_hist in executor.map(degree_worker, graph_pred_list_remove_empty):\n",
        "                sample_pred.append(deg_hist)\n",
        "    else:\n",
        "        for i in range(len(graph_ref_list)):\n",
        "            degree_temp = np.array(nx.degree_histogram(graph_ref_list[i]))\n",
        "            sample_ref.append(degree_temp)\n",
        "        for i in range(len(graph_pred_list_remove_empty)):\n",
        "            degree_temp = np.array(\n",
        "                nx.degree_histogram(graph_pred_list_remove_empty[i]))\n",
        "            sample_pred.append(degree_temp)\n",
        "\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
        "    if compute_emd:\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv)\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian)\n",
        "\n",
        "    elapsed = datetime.now() - prev\n",
        "    if PRINT_TIME:\n",
        "        print('Time computing degree mmd: ', elapsed)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "def clustering_worker(param):\n",
        "    G, bins = param\n",
        "    clustering_coeffs_list = list(nx.clustering(G).values())\n",
        "    hist, _ = np.histogram(\n",
        "        clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def clustering_stats(graph_ref_list,\n",
        "                     graph_pred_list,\n",
        "                     bins=100,\n",
        "                     is_parallel=True, compute_emd=False):\n",
        "    sample_ref = []\n",
        "    sample_pred = []\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    prev = datetime.now()\n",
        "    if is_parallel:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for clustering_hist in executor.map(clustering_worker,\n",
        "                                                [(G, bins) for G in graph_ref_list]):\n",
        "                sample_ref.append(clustering_hist)\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for clustering_hist in executor.map(\n",
        "                    clustering_worker, [(G, bins) for G in graph_pred_list_remove_empty]):\n",
        "                sample_pred.append(clustering_hist)\n",
        "\n",
        "        # check non-zero elements in hist\n",
        "        # total = 0\n",
        "        # for i in range(len(sample_pred)):\n",
        "        #    nz = np.nonzero(sample_pred[i])[0].shape[0]\n",
        "        #    total += nz\n",
        "        # print(total)\n",
        "    else:\n",
        "        for i in range(len(graph_ref_list)):\n",
        "            clustering_coeffs_list = list(nx.clustering(graph_ref_list[i]).values())\n",
        "            hist, _ = np.histogram(\n",
        "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "            sample_ref.append(hist)\n",
        "\n",
        "        for i in range(len(graph_pred_list_remove_empty)):\n",
        "            clustering_coeffs_list = list(\n",
        "                nx.clustering(graph_pred_list_remove_empty[i]).values())\n",
        "            hist, _ = np.histogram(\n",
        "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "            sample_pred.append(hist)\n",
        "\n",
        "    if compute_emd:\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd, sigma=1.0 / 10)\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd, sigma=1.0 / 10, distance_scaling=bins)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv, sigma=1.0 / 10)\n",
        "\n",
        "    elapsed = datetime.now() - prev\n",
        "    if PRINT_TIME:\n",
        "        print('Time computing clustering mmd: ', elapsed)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "# maps motif/orbit name string to its corresponding list of indices from orca output\n",
        "motif_to_indices = {\n",
        "    '3path': [1, 2],\n",
        "    '4cycle': [8],\n",
        "}\n",
        "COUNT_START_STR = 'orbit counts:'\n",
        "\n",
        "\n",
        "def edge_list_reindexed(G):\n",
        "    idx = 0\n",
        "    id2idx = dict()\n",
        "    for u in G.nodes():\n",
        "        id2idx[str(u)] = idx\n",
        "        idx += 1\n",
        "\n",
        "    edges = []\n",
        "    for (u, v) in G.edges():\n",
        "        edges.append((id2idx[str(u)], id2idx[str(v)]))\n",
        "    return edges\n",
        "\n",
        "def orca(graph):\n",
        "    # tmp_fname = f'analysis/orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
        "    tmp_fname = f'orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
        "    tmp_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)), tmp_fname)\n",
        "    # print(tmp_fname, flush=True)\n",
        "    f = open(tmp_fname, 'w')\n",
        "    f.write(\n",
        "        str(graph.number_of_nodes()) + ' ' + str(graph.number_of_edges()) + '\\n')\n",
        "    for (u, v) in edge_list_reindexed(graph):\n",
        "        f.write(str(u) + ' ' + str(v) + '\\n')\n",
        "    f.close()\n",
        "    output = sp.check_output(\n",
        "        [str(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'orca/orca')), 'node', '4', tmp_fname, 'std'])\n",
        "    output = output.decode('utf8').strip()\n",
        "    idx = output.find(COUNT_START_STR) + len(COUNT_START_STR) + 2\n",
        "    output = output[idx:]\n",
        "    node_orbit_counts = np.array([\n",
        "        list(map(int,\n",
        "                 node_cnts.strip().split(' ')))\n",
        "        for node_cnts in output.strip('\\n').split('\\n')\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        os.remove(tmp_fname)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    return node_orbit_counts\n",
        "\n",
        "\n",
        "def orbit_stats_all(graph_ref_list, graph_pred_list, compute_emd=False):\n",
        "    total_counts_ref = []\n",
        "    total_counts_pred = []\n",
        "\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    for G in graph_ref_list:\n",
        "        orbit_counts = orca(G)\n",
        "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
        "        total_counts_ref.append(orbit_counts_graph)\n",
        "\n",
        "    for G in graph_pred_list:\n",
        "        orbit_counts = orca(G)\n",
        "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
        "        total_counts_pred.append(orbit_counts_graph)\n",
        "\n",
        "    total_counts_ref = np.array(total_counts_ref)\n",
        "    total_counts_pred = np.array(total_counts_pred)\n",
        "\n",
        "    # mmd_dist = compute_mmd(\n",
        "    #     total_counts_ref,\n",
        "    #     total_counts_pred,\n",
        "    #     kernel=gaussian,\n",
        "    #     is_hist=False,\n",
        "    #     sigma=30.0)\n",
        "\n",
        "    # mmd_dist = compute_mmd(\n",
        "    #         total_counts_ref,\n",
        "    #         total_counts_pred,\n",
        "    #         kernel=gaussian_tv,\n",
        "    #         is_hist=False,\n",
        "    #         sigma=30.0)  \n",
        "\n",
        "    if compute_emd:\n",
        "        # mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=emd, sigma=30.0)\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian, is_hist=False, sigma=30.0)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian_tv, is_hist=False, sigma=30.0)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gaussian(x, y, sigma=1.0):\n",
        "    support_size = max(len(x), len(y))\n",
        "    # convert histogram values x and y to float, and make them equal len\n",
        "    x = x.astype(float)\n",
        "    y = y.astype(float)\n",
        "    if len(x) < len(y):\n",
        "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
        "    elif len(y) < len(x):\n",
        "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
        "\n",
        "    dist = np.linalg.norm(x - y, 2)\n",
        "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
        "\n",
        "\n",
        "def gaussian_tv(x, y, sigma=1.0):  \n",
        "    support_size = max(len(x), len(y))\n",
        "    # convert histogram values x and y to float, and make them equal len\n",
        "    x = x.astype(float)\n",
        "    y = y.astype(float)\n",
        "    if len(x) < len(y):\n",
        "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
        "    elif len(y) < len(x):\n",
        "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
        "\n",
        "    dist = np.abs(x - y).sum() / 2.0\n",
        "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
        "\n",
        "\n",
        "def kernel_parallel_unpacked(x, samples2, kernel):\n",
        "    d = 0\n",
        "    for s2 in samples2:\n",
        "        d += kernel(x, s2)\n",
        "    return d\n",
        "\n",
        "\n",
        "def kernel_parallel_worker(t):\n",
        "    return kernel_parallel_unpacked(*t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def disc(samples1, samples2, kernel, is_parallel=True, *args, **kwargs):\n",
        "    ''' Discrepancy between 2 samples '''\n",
        "    d = 0\n",
        "\n",
        "    if not is_parallel:\n",
        "        for s1 in samples1:\n",
        "            for s2 in samples2:\n",
        "                d += kernel(s1, s2, *args, **kwargs)\n",
        "    else:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for dist in executor.map(kernel_parallel_worker, [\n",
        "                    (s1, samples2, partial(kernel, *args, **kwargs)) for s1 in samples1\n",
        "            ]):\n",
        "                d += dist\n",
        "    if len(samples1) * len(samples2) > 0:\n",
        "        d /= len(samples1) * len(samples2)\n",
        "    else:\n",
        "        d = 1e+6\n",
        "    #print('error')\n",
        "    return d\n",
        "\n",
        "\n",
        "def compute_mmd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
        "    ''' MMD between two samples '''\n",
        "    # normalize histograms into pmf\n",
        "    if is_hist:\n",
        "        samples1 = [s1 / (np.sum(s1) + 1e-6) for s1 in samples1]\n",
        "        samples2 = [s2 / (np.sum(s2) + 1e-6) for s2 in samples2]\n",
        "    #print(samples1)\n",
        "    return disc(samples1, samples1, kernel, *args, **kwargs) + disc(samples2, samples2, kernel, *args, **kwargs) - \\\n",
        "                2 * disc(samples1, samples2, kernel, *args, **kwargs)\n",
        "\n",
        "\n",
        "def compute_emd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
        "    ''' EMD between average of two samples '''\n",
        "    # normalize histograms into pmf\n",
        "    if is_hist:\n",
        "        samples1 = [np.mean(samples1)]\n",
        "        samples2 = [np.mean(samples2)]\n",
        "    return disc(samples1, samples2, kernel, *args,\n",
        "                            **kwargs), [samples1[0], samples2[0]]\n",
        "\n",
        "\n",
        "\n",
        "def compute_mmd_stats(graphs_ref_nx, graphs_pred_nx):\n",
        "  if 'networkx' not in str(type(graphs_ref_nx[0])):\n",
        "    graphs_ref_nx = [pyg_graph_to_nx(g) for g in graphs_ref_nx]\n",
        "  if 'networkx' not in str(type(graphs_pred_nx[0])):\n",
        "    graphs_pred_nx = [pyg_graph_to_nx(g) for g in graphs_pred_nx]\n",
        "  return degree_stats(graphs_ref_nx, graphs_pred_nx), clustering_stats(graphs_ref_nx, graphs_pred_nx)\n",
        "\n",
        "\n",
        "def compute_expected_optimal_loss(graph_dataset_train, graph_dataset_test):\n",
        "  graph_dataset_train_nx = [pyg_graph_to_nx(g) for g in graph_dataset_train]\n",
        "  graph_dataset_test_nx = [pyg_graph_to_nx(g) for g in graph_dataset_test]\n",
        "  return compute_mmd_stats(graph_dataset_train_nx, graph_dataset_test_nx)\n",
        "\n",
        "#def compute_expected_optimal_loss(graphs_ref_nx):\n",
        "#  graphs_ref_nx2 = build_test_set_nx(1337)\n",
        "#  return compute_mmd_stats(graphs_ref_nx, graphs_ref_nx2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqztDRIj0UdF"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_DnkKhs9P-"
      },
      "source": [
        "#### AttentionNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoLXRG6utBBU"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATv2Conv, GraphNorm, BatchNorm\n",
        "\n",
        "class AttentionNet(torch.nn.Module):\n",
        "  def __init__(self, hidden_dim=32, layer_num=6, dropout=0.1, normalization=False, single_pass_pooling=False):\n",
        "    super(AttentionNet, self).__init__()\n",
        "\n",
        "    self.mlp_list = nn.ModuleList()\n",
        "    self.conv_list = nn.ModuleList()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.dropout = dropout\n",
        "    self.graph_norms = nn.ModuleList()\n",
        "    self.normalization = normalization\n",
        "    self.single_pass_pooling = single_pass_pooling\n",
        "    assert(hidden_dim % 2 == 0)\n",
        "    input_channels = NUM_CLASSES\n",
        "\n",
        "    for i in range(layer_num):\n",
        "      in_dim = hidden_dim if i > 0 else input_channels \n",
        "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
        "      self.conv_list.append(GATv2Conv(in_channels=in_dim, out_channels=hidden_dim // 2, heads=2))\n",
        "      self.graph_norms.append(BatchNorm(hidden_dim))\n",
        "      self.mlp_list.append(Seq(Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, out_dim)))\n",
        "\n",
        "  def forward(self, x_in, edge_index, batch):\n",
        "    x = note_features_to_one_hot(x_in)\n",
        "    for i in range(len(self.conv_list)):\n",
        "      x = self.conv_list[i](x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      if self.normalization:\n",
        "        x = self.graph_norms[i](x)\n",
        "      x = self.mlp_list[i](x)\n",
        "      #if i == 0:\n",
        "      #  x = self.dropout(x)\n",
        "\n",
        "    if self.single_pass_pooling:\n",
        "      x = global_mask_pool(x, x_in, batch)\n",
        "    else:\n",
        "      x = global_mean_pool(x, batch)\n",
        "    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.sigmoid(x) + EPSILON\n",
        "    return x.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTUh3uVOtEMO"
      },
      "outputs": [],
      "source": [
        "model = AttentionNet(single_pass_pooling=True)\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data = data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaQ7f2E4dCqd"
      },
      "source": [
        "#### TransformerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS3CzlgcdEAK"
      },
      "outputs": [],
      "source": [
        "#relu = nn.Relu() ## todo: changed to relu here\n",
        "from torch_geometric.nn import TransformerConv\n",
        "\n",
        "class TransformerNet(torch.nn.Module):\n",
        "  def __init__(self, hidden_dim=32, layer_num=6, dropout=0.1, normalization=False, single_pass_pooling = False):\n",
        "    super(TransformerNet, self).__init__()\n",
        "\n",
        "    self.mlp_list = nn.ModuleList()\n",
        "    self.conv_list = nn.ModuleList()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.graph_norms = nn.ModuleList()\n",
        "    self.dropout = dropout\n",
        "    self.normalization = normalization\n",
        "    assert(hidden_dim % 2 == 0)\n",
        "    input_channels = NUM_CLASSES\n",
        "    self.single_pass_pooling = single_pass_pooling\n",
        "\n",
        "\n",
        "    for i in range(layer_num):\n",
        "      in_dim = hidden_dim if i > 0 else input_channels \n",
        "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
        "      self.conv_list.append(TransformerConv(in_channels=in_dim, out_channels=hidden_dim // 2, heads=2))\n",
        "      self.graph_norms.append(BatchNorm(hidden_dim))\n",
        "      self.mlp_list.append(Seq(Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, out_dim)))\n",
        "\n",
        "  def forward(self, x_in, edge_index, batch):\n",
        "    x = note_features_to_one_hot(x_in)\n",
        "    for i in range(len(self.conv_list)):\n",
        "      x = self.conv_list[i](x, edge_index)\n",
        "      if self.normalization:\n",
        "        x = self.graph_norms[i](x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "      x = self.mlp_list[i](x)\n",
        "    \n",
        "    if self.single_pass_pooling:\n",
        "      x = global_mask_pool(x, x_in, batch)\n",
        "    else:\n",
        "      x = global_mean_pool(x, batch)\n",
        "\n",
        "    x = self.sigmoid(x) + EPSILON\n",
        "    return x.flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plkfTVUrXRO-"
      },
      "outputs": [],
      "source": [
        "model = TransformerNet()\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data = data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwghOZsDdEJz"
      },
      "source": [
        "#### Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEwIcO4w0VaR"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphUNet\n",
        "\n",
        "class Unet(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels=32, out_channels=1, depth=4, single_pass_pooling = False):\n",
        "    super(Unet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    in_channels = NUM_CLASSES\n",
        "    self.single_pass_pooling = single_pass_pooling\n",
        "    self.unet = GraphUNet(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, depth=depth)\n",
        "\n",
        "  def forward(self, x_in, edge_index, batch):\n",
        "    x = note_features_to_one_hot(x_in)\n",
        "    x = self.unet(x, edge_index)\n",
        "\n",
        "    if self.single_pass_pooling:\n",
        "      x = global_mask_pool(x, x_in, batch)\n",
        "    else:\n",
        "      x = global_mean_pool(x, batch)\n",
        "\n",
        "    x = self.sigmoid(x) + 0.0000001 # add eps to avoid div. by zero\n",
        "    return x.flatten()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBYSWAp9XOLz"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Unet()\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir4sFS6YS5UY"
      },
      "source": [
        "#### PNA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydDINWbaaSav"
      },
      "outputs": [],
      "source": [
        "def print_inputs(*args):\n",
        "    for arg in args:\n",
        "        print(arg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_8SApKdS3wo"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "#train_dataset = build_dataset()\n",
        "\n",
        "def dataset_to_degree_bin(train_dataset):\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "      d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "      max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
        "  for data in train_dataset:\n",
        "      d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "      deg += torch.bincount(d, minlength=deg.numel())\n",
        "  return deg\n",
        "\n",
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset_example, hidden_channels=32, depth=4, dropout=0.0, towers=2, single_pass_pooling=False, graph_transform=False, normalization=True, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = NUM_CLASSES\n",
        "    if graph_transform:\n",
        "      in_channels += NODE_FEATURE_EXTEND\n",
        "    deg = dataset_to_degree_bin(train_dataset_example)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "    self.single_pass_pooling = single_pass_pooling\n",
        "    self.graph_transform = graph_transform\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x_in, edge_index, batch):\n",
        "    x = note_features_to_one_hot(x_in)\n",
        "    if self.graph_transform:\n",
        "      x = extend_node_features(x, edge_index, batch)\n",
        "    x = self.pnanet(x, edge_index)\n",
        "\n",
        "    if self.single_pass_pooling:\n",
        "      x = global_mask_pool(x, x_in, batch)\n",
        "    else:\n",
        "      x = global_mean_pool(x, batch)\n",
        "\n",
        "    #x = x.sum(dim=1)\n",
        "    x = self.final_mlp(x)\n",
        "    x = self.sigmoid(x) + EPSILON # add eps to avoid div. by zero\n",
        "    return x.flatten()\n",
        "\n",
        "class PNAmulti(torch.nn.Module):\n",
        "  def __init__(self, train_dataset_example, hidden_channels=32, graph_transform=False, depth=4, dropout=0.0, towers=1, model_num=5, single_pass_pooling=False, normalization=True, pre_post_layers=1):\n",
        "    super(PNAmulti, self).__init__()\n",
        "    self.model_num = model_num\n",
        "    self.pna_list = nn.ModuleList()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    weights = torch.ones(model_num)\n",
        "    self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "    for i in range(model_num):\n",
        "      # towers is i+1\n",
        "      pna_model = PNAnet(train_dataset_example, hidden_channels=hidden_channels, graph_transform=graph_transform, depth=depth, dropout=dropout, towers=i+1, single_pass_pooling=single_pass_pooling, normalization=normalization, pre_post_layers=pre_post_layers)\n",
        "      self.pna_list.append(pna_model)\n",
        "\n",
        "  def get_normalized_weights(self):\n",
        "    w_sigmoid = self.sigmoid(self.weights)\n",
        "    w = w_sigmoid/w_sigmoid.sum()\n",
        "    return w\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    x_agg = None\n",
        "    i_not = random.choice(range(len(self.pna_list))) if self.training  and len(self.pna_list) > 1 else -1 # skip 1 during training\n",
        "    w = self.get_normalized_weights()\n",
        "    for i, pna_model in enumerate(self.pna_list):\n",
        "      if i == i_not:\n",
        "        continue\n",
        "      if x_agg is None:\n",
        "        x_agg = pna_model(x, edge_index, batch) * w[i]\n",
        "      else:\n",
        "        x_agg = x_agg + pna_model( x, edge_index, batch) * w[i]\n",
        "    return x_agg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTvXwuajPPOJ"
      },
      "outputs": [],
      "source": [
        "model = PNAmulti(build_dataset()[0])\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSyYUfY1X6Os"
      },
      "outputs": [],
      "source": [
        "model = PNAnet(build_dataset()[0])\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB6WbKuwVdix"
      },
      "source": [
        "#### PNA2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mvgyb_hVgpv"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNAConv\n",
        "\n",
        "class PNA2net(torch.nn.Module):\n",
        "  def __init__(self, train_dataset_example, hidden_dim=32, layer_num=4, dropout=0.0, normalization=False, tower_num=5, single_pass_pooling=False):\n",
        "    super().__init__()\n",
        "\n",
        "    aggregators = ['mean', 'min', 'max', 'std', 'var', 'sum']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    deg = dataset_to_degree_bin(train_dataset_example)\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_dim = tower_num * ((hidden_dim // tower_num) + 1)\n",
        "\n",
        "    self.conv_list = nn.ModuleList()\n",
        "    self.batchnorm_list = nn.ModuleList()\n",
        "    self.mlp_list = nn.ModuleList()\n",
        "\n",
        "    self.dropout = dropout\n",
        "    self.normalization = normalization\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.single_pass_pooling = single_pass_pooling\n",
        "\n",
        "    input_channels = NUM_CLASSES\n",
        "  \n",
        "\n",
        "    for i in range(layer_num):\n",
        "      in_dim = hidden_dim if i > 0 else input_channels \n",
        "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
        "      self.conv_list.append(PNAConv(in_channels=in_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=tower_num, pre_layers=1, post_layers=1, divide_input=False))\n",
        "      self.batchnorm_list.append(BatchNorm(hidden_dim))\n",
        "      self.mlp_list.append(Seq(nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, hidden_dim+NUM_CLASSES+in_dim), nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, hidden_dim+NUM_CLASSES+in_dim), nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, out_dim)))\n",
        "          \n",
        "  def forward(self, x_in, edge_index, batch):\n",
        "    x_onehot = note_features_to_one_hot(x_in)\n",
        "    x = x_onehot.clone()\n",
        "    for i in range(len(self.conv_list)):\n",
        "      x_in = x.clone()\n",
        "      x = self.conv_list[i](x, edge_index)\n",
        "      if self.normalization:\n",
        "        x = self.batchnorm_list[i](x)\n",
        "      x_concat = torch.concat([x, x_onehot, x_in], dim=1)\n",
        "      x = self.mlp_list[i](x_concat)\n",
        "      if i < len(self.conv_list)-1:\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "    if self.single_pass_pooling:\n",
        "      x = global_mask_pool(x, x_in, batch)\n",
        "    else:\n",
        "      x = global_mean_pool(x, batch)\n",
        "      \n",
        "    x = self.sigmoid(x) + EPSILON\n",
        "    return x.flatten()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgjqgcfhY1gC"
      },
      "outputs": [],
      "source": [
        "model = PNA2net(build_dataset()[0])\n",
        "model.to(DEVICE)\n",
        "print(model) \n",
        "for step, data in enumerate(train_loader):\n",
        "  data.to(DEVICE)\n",
        "  print(model(data.x, data.edge_index, data.batch))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tph0rjhNS6iS"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K0Yuz5FNVdy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDKYLoMV6z6J"
      },
      "outputs": [],
      "source": [
        "def build_model(config, dataset):\n",
        "  hidden_dim = config.hidden_dim\n",
        "  single_pass_pooling = config.single_pass\n",
        "  if config.model == \"unet\":\n",
        "    model =  Unet(hidden_channels = hidden_dim,  out_channels = 1, depth=config.hidden_layer, single_pass_pooling=single_pass_pooling)\n",
        "  elif config.model == \"attention\":\n",
        "     model = AttentionNet(hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
        "  elif config.model == \"transformer\":\n",
        "    model = TransformerNet(hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
        "  elif config.model == \"pna\":\n",
        "    model = PNAnet(dataset, hidden_channels=hidden_dim, depth=config.hidden_layer, dropout=config.dropout, towers=config.towers, single_pass_pooling=single_pass_pooling, graph_transform=config.graph_transform, normalization=config.normalization, pre_post_layers=config.pre_post_layers)\n",
        "  elif config.model == \"pnamulti\":\n",
        "    model = PNAmulti(dataset, hidden_channels=hidden_dim, depth=config.hidden_layer, dropout=config.dropout, towers=config.towers, single_pass_pooling=single_pass_pooling, graph_transform=config.graph_transform, normalization=config.normalization, pre_post_layers=config.pre_post_layers)\n",
        "  elif config.model == \"pna2\":\n",
        "    model = PNA2net(dataset, hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
        "  else:\n",
        "    ValueError(\"illegal net\")\n",
        "  return model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GDOIysiiIGT"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsofkLkkP3GK"
      },
      "outputs": [],
      "source": [
        "def find_best_choice(model, g, test_choices, x, x_shape):\n",
        "  p_0_list = list()\n",
        "  p_1_list = list()\n",
        "  x_noedge_list = list()\n",
        "  x_edge_list = list()\n",
        "  mask_indicator_list = list()\n",
        "\n",
        "  for i, mask_indicator in enumerate(test_choices):\n",
        "    x_noedge, x_edge = x.clone(), x.clone()\n",
        "    x_noedge[mask_indicator] = NO_EDGE_INDICATOR\n",
        "    x_edge[mask_indicator] = EDGE_INDICATOR\n",
        "    batch = torch.zeros(x_shape[0], dtype=torch.long).to(DEVICE)\n",
        "    p_0 = model(x_noedge.view(x_shape), g.edge_index, batch=batch).item()\n",
        "    p_1 = model(x_edge.view(x_shape), g.edge_index, batch=batch).item()\n",
        "\n",
        "    p_0_list.append(p_0/(p_0+p_1))\n",
        "    p_1_list.append(p_1/(p_0+p_1))\n",
        "    x_noedge_list.append(x_noedge)\n",
        "    x_edge_list.append(x_edge)\n",
        "    mask_indicator_list.append(mask_indicator)\n",
        "\n",
        "  best_i = np.argmax(p_1_list)\n",
        "  return p_0_list[best_i], p_1_list[best_i], x_noedge_list[best_i],  x_edge_list[best_i], mask_indicator_list[best_i]\n",
        "\n",
        "\n",
        "def generate_graph_multi(model, g, choice_num=5):\n",
        "  x = g.x\n",
        "  x_shape = x.shape\n",
        "  x = x.flatten()\n",
        "  indices_of_edges = (x != DUMMY).nonzero(as_tuple=False).flatten()\n",
        "  num_edges = indices_of_edges.numel()\n",
        "  x[indices_of_edges] = MASK\n",
        "  indices_of_edges = indices_of_edges.flatten().tolist()\n",
        "\n",
        "  for _ in range(num_edges):\n",
        "    random.shuffle(indices_of_edges)\n",
        "    choice_num_i = min(choice_num, len(indices_of_edges)) \n",
        "    test_choices = indices_of_edges[0:choice_num_i]\n",
        "    p_0, p_1, x_noedge, x_edge, mask_indicator = find_best_choice(model, g, test_choices, x, x_shape)\n",
        "    random_selection_index = int(np.random.choice([0, 1], size=1, p=[p_0, p_1]))\n",
        "    x = (x_noedge, x_edge)[random_selection_index]\n",
        "    indices_of_edges.remove(mask_indicator)\n",
        "\n",
        "  g.x = x.reshape(x_shape)\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQay_Zy35bHr"
      },
      "outputs": [],
      "source": [
        "def generate_graph_singlepass(model, g):\n",
        "  # does not have multi\n",
        "  model.eval()\n",
        "  x = g.x\n",
        "  x_shape = x.shape\n",
        "  x = x.flatten()\n",
        "  indices_of_edges = (x != DUMMY).nonzero(as_tuple=False).flatten()\n",
        "  num_edges = indices_of_edges.numel()\n",
        "  x[indices_of_edges] = MASK\n",
        "  indices_of_edges = indices_of_edges.flatten().tolist()\n",
        "  random.shuffle(indices_of_edges)\n",
        "  batch = torch.zeros(x_shape[0], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "  for edge_index in indices_of_edges:\n",
        "    x[edge_index] = -1.0\n",
        "    edge_prob = model(x.view(x_shape), g.edge_index, batch=batch).item()\n",
        "    assert(edge_prob > 0.0 and edge_prob < 1.0)\n",
        "    if random.random() < edge_prob:\n",
        "      x[edge_index] = EDGE_INDICATOR\n",
        "    else:\n",
        "      x[edge_index] = NO_EDGE_INDICATOR\n",
        "\n",
        "  g.x = x.reshape(x_shape)\n",
        "  return g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cN5gKKUiJTR"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_graphs(num, epoch_i, model, g_like, config):\n",
        "  mean_degree_list = list()\n",
        "  var_degree_list = list()\n",
        "  nx_graph_list = list()\n",
        "  for j in range(num):\n",
        "    if config.single_pass:\n",
        "      g = generate_graph_singlepass(model, g_like)\n",
        "    else:\n",
        "      g = generate_graph_multi(model, g_like, choice_num = config.candidate_selection_radius)\n",
        "    nx_orig_graph = draw_pyg(g, filename=f\"generated_graph_epoch{str(epoch_i).zfill(5)}_sample{str(j).zfill(5)}.jpg\")\n",
        "    degree_list = [nx_orig_graph.degree(i) for i in nx_orig_graph.nodes()]\n",
        "    mean_degree_list.append(np.mean(degree_list))\n",
        "    var_degree_list.append(np.var(degree_list))\n",
        "    nx_graph_list.append(nx_orig_graph)\n",
        "  return np.mean(mean_degree_list), np.mean(var_degree_list), nx_graph_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKxurLjrXnCo"
      },
      "source": [
        "## Generat Samples for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nALlwSTtXmhx"
      },
      "outputs": [],
      "source": [
        "def gen_random_training_sample_unused(x):\n",
        "  x_masked = x.clone().reshape(-1)\n",
        "  indices_of_edges = (x_masked != DUMMY).nonzero(as_tuple=False).flatten()\n",
        "  num_edges = indices_of_edges.numel()\n",
        "  num_masked = torch.randint(1, num_edges+1, size=(1,), device=DEVICE)\n",
        "  #num_masked = sample_num_masked_geometric(1, num_edges+1)\n",
        "\n",
        "  shuffled_indices_of_edges = indices_of_edges[torch.randperm(indices_of_edges.numel())]\n",
        "  indices_of_edges_to_mask = shuffled_indices_of_edges[:num_masked]\n",
        "  \n",
        "  gt_value = x_masked[indices_of_edges_to_mask[0]].item() # we will change the first value back\n",
        "  x_masked[indices_of_edges_to_mask] = MASK\n",
        "\n",
        "\n",
        "  x_masked_0 = x_masked.clone()\n",
        "  x_masked_0[indices_of_edges_to_mask[0]] = NO_EDGE_INDICATOR\n",
        "  x_masked_0 = x_masked_0.reshape(x.shape)\n",
        "\n",
        "  x_masked_1 = x_masked.clone()\n",
        "  x_masked_1[indices_of_edges_to_mask[0]] = EDGE_INDICATOR\n",
        "  x_masked_1 = x_masked_1.reshape(x.shape)\n",
        "\n",
        "  if gt_value == NO_EDGE_INDICATOR:\n",
        "    return x_masked_0, x_masked_1\n",
        "  elif gt_value == EDGE_INDICATOR:\n",
        "    return x_masked_1, x_masked_0 \n",
        "  assert(False)\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-gIs8GVYm5l"
      },
      "outputs": [],
      "source": [
        "def sample_mask_and_flip(batch, x):\n",
        "  num_graphs = batch.view(-1)[-1]\n",
        "  x_id_mask = list()\n",
        "  x_id_flip = list()\n",
        "\n",
        "  for graph_id in range(num_graphs+1):\n",
        "    #num_nodes = (batch == graph_id).sum()\n",
        "    #indices_of_edges = (x != DUMMY and batch == graph_id).nonzero(as_tuple=False)\n",
        "    edge_indicator = x.view(-1) != DUMMY\n",
        "    batch_indicator = batch.view(-1) == graph_id \n",
        "    indices_of_edges = torch.nonzero(batch_indicator & edge_indicator)\n",
        "    indices_of_edges = indices_of_edges.flatten().tolist()\n",
        "    random.shuffle(indices_of_edges)\n",
        "    x_id_flip.append(indices_of_edges[-1])\n",
        "    num_masked = random.choice(range(len(indices_of_edges)))\n",
        "    x_id_mask = x_id_mask + indices_of_edges[:num_masked]\n",
        "\n",
        "  mask_and_flip_indicator = torch.zeros(batch.numel(), dtype=torch.long).to(DEVICE)\n",
        "  mask_and_flip_indicator[x_id_mask] = 1\n",
        "  mask_and_flip_indicator[x_id_flip] = 2\n",
        "\n",
        "  return mask_and_flip_indicator\n",
        "\n",
        "def mask_x_based_on_indicator(x, mask_and_flip_indicator):\n",
        "  x_new = x.clone()\n",
        "  to_mask = mask_and_flip_indicator == 1\n",
        "  x_new.view(-1)[to_mask] = MASK\n",
        "  return x_new\n",
        "\n",
        "\n",
        "# this is done inplace!\n",
        "def flip_x_based_on_indicator(x, mask_and_flip_indicator):\n",
        "  to_flip = (mask_and_flip_indicator == 2)#.to(DEVICE)\n",
        "  x = x.clone()  #sadly in-place operation not allowed here\n",
        "\n",
        "  #x[to_flip] = -x[to_flip]\n",
        "\n",
        "  x[to_flip] = torch.where(x[to_flip] == EDGE_INDICATOR, torch.tensor(NO_EDGE_INDICATOR).to(DEVICE), torch.tensor(EDGE_INDICATOR).to(DEVICE))\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1La4pbjWakyz"
      },
      "outputs": [],
      "source": [
        "for step, data in enumerate(train_loader):\n",
        "  data.to(DEVICE)\n",
        "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x)\n",
        "  break\n",
        "\n",
        "\n",
        "x = (mask_x_based_on_indicator(data.x, mask_and_flip_indicator))\n",
        "xz = x.clone()\n",
        "xx = (flip_x_based_on_indicator(x, mask_and_flip_indicator))\n",
        "\n",
        "(xz == xx).flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TulPGAH8R0Eo"
      },
      "source": [
        "## Compute Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flDsYTRKR1ZB"
      },
      "outputs": [],
      "source": [
        "bce_loss = nn.BCELoss()\n",
        "def reduce_loss(loss_for_graph, config):\n",
        "  # loss_for_graph should be > 0\n",
        "  if config.loss == \"l1\":\n",
        "    return loss_for_graph.sum() \n",
        "  elif config.loss == \"l2\":\n",
        "    return F.mse_loss(loss_for_graph, torch.zeros_like(loss_for_graph))\n",
        "  elif config.loss == \"bce\":\n",
        "    return bce_loss(loss_for_graph, torch.zeros_like(loss_for_graph))\n",
        "  else:\n",
        "    raise ValueError(\"Illegal loss value\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVx9HGc89VFe"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4M0XZ5Vpk5Z"
      },
      "outputs": [],
      "source": [
        "def prepare_x_for_single_pass(mask_and_flip_indicator, x):\n",
        "  x_new = x.clone()\n",
        "  to_mask = mask_and_flip_indicator == 1\n",
        "  x_new.view(-1)[to_mask] = MASK\n",
        "  choice_indicator = mask_and_flip_indicator == 2\n",
        "  x_new.view(-1)[choice_indicator] = -1 # this will be set to zero in the one hot encoding step\n",
        "  gt = x.clone().flatten()[choice_indicator]\n",
        "  gt = torch.where(gt == EDGE_INDICATOR, torch.tensor(1.0, device = DEVICE), torch.zeros_like(gt))\n",
        "  return x_new, gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj5td9zue7HT"
      },
      "outputs": [],
      "source": [
        "def single_pass_prediction(data, model, config):\n",
        "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x)\n",
        "  x_in, out_gt = prepare_x_for_single_pass(mask_and_flip_indicator, data.x)\n",
        "\n",
        "  if model.training: x_in = flip_edges_randomly(x_in, config.noise_probability)\n",
        "\n",
        "  out_prediction = model(x_in, data.edge_index, data.batch) # between 0 ,1 \n",
        "\n",
        "  #print(\"out gt: \", out_gt.flatten(), \"   out pred:   \", out_prediction.flatten())\n",
        "\n",
        "  loss_for_graph =  torch.abs(out_prediction.flatten() - out_gt.flatten()) # between 0 and 1\n",
        "  return loss_for_graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUfUS3OYfNxu"
      },
      "outputs": [],
      "source": [
        "def multi_pass_prediction(data, model, config):\n",
        "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x) # this then should also return gt \n",
        "  x = mask_x_based_on_indicator(data.x, mask_and_flip_indicator)\n",
        "\n",
        "  if model.training: x = flip_edges_randomly(x, config.noise_probability)\n",
        "\n",
        "  out_correct = model(x, data.edge_index, data.batch)  # is supposed to be 1 everywhere\n",
        "  x = flip_x_based_on_indicator(x, mask_and_flip_indicator)  # important to use x here and not data.x\n",
        "  out_incorrect = model(x, data.edge_index, data.batch) # is supposed to be 0 everywhere\n",
        "  loss_for_graph =  out_incorrect - out_correct # between -1 and 1\n",
        "  loss_for_graph = (loss_for_graph + 1.0) / 2.0   # between 0 and 1,\n",
        "  return loss_for_graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_84ieMIoYh5"
      },
      "outputs": [],
      "source": [
        "def feed_data_to_model(data, model, config):\n",
        "  if config.single_pass == True:\n",
        "    return single_pass_prediction(data, model, config)\n",
        "  return multi_pass_prediction(data, model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8RF9E-wfDXW"
      },
      "source": [
        "### Start Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDBAh9XPiO-a"
      },
      "outputs": [],
      "source": [
        "def start_agent(config):\n",
        "  graph_dataset_train, graph_dataset_test = load_dataset_planar()# build_dataset()\n",
        "  #graphs_ref_nx = build_test_set_nx(seed=42)\n",
        "  degree_loss, cluster_loss = compute_expected_optimal_loss(graph_dataset_train, graph_dataset_test)\n",
        "  print('Reference distance is: ', degree_loss, cluster_loss)\n",
        "  wandb.log({\"stats/degree_train\": degree_loss,  \"stats/cluster_train\": cluster_loss})\n",
        "\n",
        "  model = build_model(config, graph_dataset)\n",
        "  wandb.log({\"num_parameters\": sum(p.numel() for p in model.parameters())})\n",
        "\n",
        "  optimizer = Adam(model.parameters(), lr = config.learning_rate) \n",
        "  train_loader = DataLoader(graph_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "  for epoch_i in range(NUM_EPOCHS):\n",
        "    loss_list = list()\n",
        "    for step, data in enumerate(train_loader):\n",
        "      data.to(DEVICE)\n",
        "      model.train()\n",
        "      model.zero_grad()\n",
        "\n",
        "      loss_for_graph = feed_data_to_model(data, model, config) # between 0 and 1, probabilty of the incorrect choice\n",
        "      loss = reduce_loss(loss_for_graph, config)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss =  ((loss_for_graph.mean()).item()  -0.5 ) * 2.0 # between - 0.5 and 0.5 -> between - 1 and 1\n",
        "      loss_list.append(loss)\n",
        "      #wandb.log({\"epoch\": epoch_i+1, \"loss\": np.mean(loss_list)})\n",
        "\n",
        "    print('epoch: {:06}  loss: {:.5f}'.format(epoch_i+1 ,np.mean(loss_list)))\n",
        "    # early stopping\n",
        "    if np.mean(loss_list) > -0.000001 and (epoch_i > 199 and epoch_i % 100 == 0):\n",
        "      return\n",
        "\n",
        "    if epoch_i % 20 == 0 or epoch_i == NUM_EPOCHS-1:\n",
        "      dummy_graph = graph_dataset[0].clone().to(DEVICE)\n",
        "      #num = NUM_GRAPHS_GENERATE if epoch_i != NUM_EPOCHS-1 else NUM_GRAPHS_GENERATE*5\n",
        "      deg_mean, deg_var, graphs_pred_nx = generate_graphs(NUM_GRAPHS_GENERATE, epoch_i, model, dummy_graph, config)\n",
        "      generator_loss = (DEGREE-deg_mean)**2 + deg_var\n",
        "      wandb.log({\"graph-unmasking/gen-loss\": generator_loss, \"mean degree\": deg_mean, \"mean degree var\":deg_var, \"graph-unmasking/loss\": np.mean(loss_list)})\n",
        "      degree_loss, cluster_loss = compute_mmd_stats(graphs_pred_nx, graph_dataset_test)\n",
        "      wandb.log({\"stats/degree\": degree_loss,  \"stats/cluster\": cluster_loss})\n",
        "      print(deg_mean, deg_var, degree_loss, cluster_loss)\n",
        "\n",
        "  torch.save(model.state_dict(), \"model.weights\")\n",
        "  wandb.log_artifact(\"model.weights\", name=f'nn_weights_{SWEEP_ID}', type='weights') \n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPcPralV7IF0"
      },
      "outputs": [],
      "source": [
        "def start_agent_envelope():\n",
        "  try:\n",
        "    with wandb.init():\n",
        "      config = wandb.config\n",
        "      set_seeds(SWEEP_ID)\n",
        "      print(SWEEP_ID, config)\n",
        "      if not IN_COLAB:\n",
        "        for python_file in sorted(glob.glob('*.ipynb')):\n",
        "          wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}\", type=\"my_dataset\")\n",
        "        for python_file in sorted(glob.glob('*.py')):\n",
        "          wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}\", type=\"my_dataset\")\n",
        "      return start_agent(config)\n",
        "  except Exception:\n",
        "    print(traceback.format_exc())\n",
        "    wandb.log({\"graph-unmasking/gen-loss\": -1,  \"epoch\": -1, \"graph-unmasking/loss\": -1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHuZY5f_4NrN"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_MODE\"] = WANDB_MODE #\"dryrun\"\n",
        "\n",
        "SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
        "wandb.agent(SWEEP_ID, project=PROJECT_NAME, function=start_agent_envelope, count=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHcAvy-w6Zlu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg-wy3DnJRA9"
      },
      "source": [
        "# Load graph datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWlIj_iUJUtp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.system(\"git clone https://github.com/KarolisMart/SPECTRE.git\")\n",
        "\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "#import pytorch_lightning as pl\n",
        "import networkx as nx\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from scipy.spatial import Delaunay\n",
        "#from torch_geometric.datasets import QM9\n",
        "#from rdkit import Chem\n",
        "#from torch_geometric.utils import dense_to_sparse, to_dense_adj, to_networkx\n",
        "\n",
        "#from util.eval_helper import degree_stats, orbit_stats_all, clustering_stats, spectral_stats, eigval_stats, compute_list_eigh, spectral_filter_stats\n",
        "#from util.molecular_eval import BasicMolecularMetrics\n",
        "\n",
        "\n",
        "class SpectreGraphDataset(Dataset):\n",
        "    def __init__(self, data_file):\n",
        "        \"\"\" This class can be used to load the comm20, sbm and planar datasets. \"\"\"\n",
        "        base_path = 'SPECTRE/data/'\n",
        "        filename = os.path.join(base_path, data_file)\n",
        "        self.adjs, self.eigvals, self.eigvecs, self.n_nodes, self.max_eigval, self.min_eigval, self.same_sample, self.n_max = torch.load(\n",
        "            filename)\n",
        "        print(f'Dataset {filename} loaded from file')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.adjs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        adj = self.adjs[idx]\n",
        "        n = adj.shape[-1]\n",
        "        X = torch.ones(n, 1, dtype=torch.float)\n",
        "        y = torch.zeros([1, 0]).float()\n",
        "        edge_index, _ = torch_geometric.utils.dense_to_sparse(adj)\n",
        "        edge_attr = torch.zeros(edge_index.shape[-1], 2, dtype=torch.float)\n",
        "        edge_attr[:, 1] = 1\n",
        "        num_nodes = n * torch.ones(1, dtype=torch.long)\n",
        "        data = torch_geometric.data.Data(x=X, edge_index=edge_index, edge_attr=edge_attr,\n",
        "                                         y=y, idx=idx, n_nodes=num_nodes)\n",
        "        return data\n",
        "\n",
        "\n",
        "class PlanarDataset(SpectreGraphDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__('planar_64_200.pt')\n",
        "\n",
        "\n",
        "PlanarDataset()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9QJ8aL44va_"
      },
      "outputs": [],
      "source": [
        "\n",
        "adjs, eigvals, eigvecs, n_nodes, max_eigval, min_eigval, same_sample, n_max = torch.load('SPECTRE/data/planar_64_200.pt')\n",
        "adjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rZedjy0wJxy"
      },
      "outputs": [],
      "source": [
        "graphs = adjs\n",
        "test_len = int(round(len(graphs) * 0.2))\n",
        "train_len = int(round((len(graphs) - test_len) * 0.8))\n",
        "val_len = len(graphs) - train_len - test_len\n",
        "print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
        "splits = random_split(graphs, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2hS79yrqST"
      },
      "source": [
        "### next try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_GqY9tyuKpA"
      },
      "outputs": [],
      "source": [
        "os.system(\"pip install omegaconf\")\n",
        "os.system(\"pip install pytorch_lightning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJJ8ICD9vgCQ"
      },
      "outputs": [],
      "source": [
        "os.system(\"pip install overrides\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGc8eQCbrrd0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.system(\"git clone https://github.com/cvignac/DiGress.git\")\n",
        "os.system(\"cp -r DiGress/src src/\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import random_split, Dataset\n",
        "import torch_geometric.utils\n",
        "\n",
        "from src.datasets.abstract_dataset import AbstractDataModule, AbstractDatasetInfos\n",
        "\n",
        "\n",
        "class SpectreGraphDataset(Dataset):\n",
        "    def __init__(self, data_file):\n",
        "        \"\"\" This class can be used to load the comm20, sbm and planar datasets. \"\"\"\n",
        "        base_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, 'data')\n",
        "        filename = os.path.join(base_path, data_file)\n",
        "        self.adjs, self.eigvals, self.eigvecs, self.n_nodes, self.max_eigval, self.min_eigval, self.same_sample, self.n_max = torch.load(\n",
        "            filename)\n",
        "        print(f'Dataset {filename} loaded from file')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.adjs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        adj = self.adjs[idx]\n",
        "        n = adj.shape[-1]\n",
        "        X = torch.ones(n, 1, dtype=torch.float)\n",
        "        y = torch.zeros([1, 0]).float()\n",
        "        edge_index, _ = torch_geometric.utils.dense_to_sparse(adj)\n",
        "        edge_attr = torch.zeros(edge_index.shape[-1], 2, dtype=torch.float)\n",
        "        edge_attr[:, 1] = 1\n",
        "        num_nodes = n * torch.ones(1, dtype=torch.long)\n",
        "        data = torch_geometric.data.Data(x=X, edge_index=edge_index, edge_attr=edge_attr,\n",
        "                                         y=y, idx=idx, n_nodes=num_nodes)\n",
        "        return data\n",
        "\n",
        "class SpectreGraphDataModule(AbstractDataModule):\n",
        "    def __init__(self, cfg, n_graphs=200):\n",
        "        super().__init__(cfg)\n",
        "        self.n_graphs = n_graphs\n",
        "        self.prepare_data()\n",
        "        self.inner = self.train_dataloader()\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.inner[item]\n",
        "\n",
        "    def prepare_data(self, graphs):\n",
        "        test_len = int(round(len(graphs) * 0.2))\n",
        "        train_len = int(round((len(graphs) - test_len) * 0.8))\n",
        "        val_len = len(graphs) - train_len - test_len\n",
        "        print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
        "        splits = random_split(graphs, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))\n",
        "\n",
        "        datasets = {'train': splits[0], 'val': splits[1], 'test': splits[2]}\n",
        "        super().prepare_data(datasets)\n",
        "\n",
        "class Comm20Dataset(SpectreGraphDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__('community_12_21_100.pt')\n",
        "\n",
        "\n",
        "class SBMDataset(SpectreGraphDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__('sbm_200.pt')\n",
        "\n",
        "\n",
        "class PlanarDataset(SpectreGraphDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__('planar_64_200.pt')\n",
        "\n",
        "dataset = PlanarDataset()\n",
        "dataset.prepare_data\n",
        "dataset.datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbPFtDS8x810"
      },
      "source": [
        "# Quality Measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oSqDFX4x_7C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCDeFcFFyVRQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.system(\"pip install pyemd\")\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s_ZCqiiyVfj"
      },
      "outputs": [],
      "source": [
        "import pyemd\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "from scipy.linalg import toeplitz\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "PRINT_TIME = False\n",
        "\n",
        "def degree_worker(G):\n",
        "    return np.array(nx.degree_histogram(G))\n",
        "\n",
        "def degree_stats(graph_ref_list, graph_pred_list, is_parallel=True, compute_emd=False):\n",
        "    ''' Compute the distance between the degree distributions of two unordered sets of graphs.\n",
        "        Args:\n",
        "            graph_ref_list, graph_target_list: two lists of networkx graphs to be evaluated\n",
        "        '''\n",
        "    sample_ref = []\n",
        "    sample_pred = []\n",
        "    # in case an empty graph is generated\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    prev = datetime.now()\n",
        "    if is_parallel:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for deg_hist in executor.map(degree_worker, graph_ref_list):\n",
        "                sample_ref.append(deg_hist)\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for deg_hist in executor.map(degree_worker, graph_pred_list_remove_empty):\n",
        "                sample_pred.append(deg_hist)\n",
        "    else:\n",
        "        for i in range(len(graph_ref_list)):\n",
        "            degree_temp = np.array(nx.degree_histogram(graph_ref_list[i]))\n",
        "            sample_ref.append(degree_temp)\n",
        "        for i in range(len(graph_pred_list_remove_empty)):\n",
        "            degree_temp = np.array(\n",
        "                nx.degree_histogram(graph_pred_list_remove_empty[i]))\n",
        "            sample_pred.append(degree_temp)\n",
        "\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
        "    if compute_emd:\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv)\n",
        "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian)\n",
        "\n",
        "    elapsed = datetime.now() - prev\n",
        "    if PRINT_TIME:\n",
        "        print('Time computing degree mmd: ', elapsed)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "def clustering_worker(param):\n",
        "    G, bins = param\n",
        "    clustering_coeffs_list = list(nx.clustering(G).values())\n",
        "    hist, _ = np.histogram(\n",
        "        clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "    return hist\n",
        "\n",
        "\n",
        "def clustering_stats(graph_ref_list,\n",
        "                     graph_pred_list,\n",
        "                     bins=100,\n",
        "                     is_parallel=True, compute_emd=False):\n",
        "    sample_ref = []\n",
        "    sample_pred = []\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    prev = datetime.now()\n",
        "    if is_parallel:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for clustering_hist in executor.map(clustering_worker,\n",
        "                                                [(G, bins) for G in graph_ref_list]):\n",
        "                sample_ref.append(clustering_hist)\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for clustering_hist in executor.map(\n",
        "                    clustering_worker, [(G, bins) for G in graph_pred_list_remove_empty]):\n",
        "                sample_pred.append(clustering_hist)\n",
        "\n",
        "        # check non-zero elements in hist\n",
        "        # total = 0\n",
        "        # for i in range(len(sample_pred)):\n",
        "        #    nz = np.nonzero(sample_pred[i])[0].shape[0]\n",
        "        #    total += nz\n",
        "        # print(total)\n",
        "    else:\n",
        "        for i in range(len(graph_ref_list)):\n",
        "            clustering_coeffs_list = list(nx.clustering(graph_ref_list[i]).values())\n",
        "            hist, _ = np.histogram(\n",
        "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "            sample_ref.append(hist)\n",
        "\n",
        "        for i in range(len(graph_pred_list_remove_empty)):\n",
        "            clustering_coeffs_list = list(\n",
        "                nx.clustering(graph_pred_list_remove_empty[i]).values())\n",
        "            hist, _ = np.histogram(\n",
        "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
        "            sample_pred.append(hist)\n",
        "\n",
        "    if compute_emd:\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd, sigma=1.0 / 10)\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd, sigma=1.0 / 10, distance_scaling=bins)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv, sigma=1.0 / 10)\n",
        "\n",
        "    elapsed = datetime.now() - prev\n",
        "    if PRINT_TIME:\n",
        "        print('Time computing clustering mmd: ', elapsed)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "# maps motif/orbit name string to its corresponding list of indices from orca output\n",
        "motif_to_indices = {\n",
        "    '3path': [1, 2],\n",
        "    '4cycle': [8],\n",
        "}\n",
        "COUNT_START_STR = 'orbit counts:'\n",
        "\n",
        "\n",
        "def edge_list_reindexed(G):\n",
        "    idx = 0\n",
        "    id2idx = dict()\n",
        "    for u in G.nodes():\n",
        "        id2idx[str(u)] = idx\n",
        "        idx += 1\n",
        "\n",
        "    edges = []\n",
        "    for (u, v) in G.edges():\n",
        "        edges.append((id2idx[str(u)], id2idx[str(v)]))\n",
        "    return edges\n",
        "\n",
        "def orca(graph):\n",
        "    # tmp_fname = f'analysis/orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
        "    tmp_fname = f'orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
        "    tmp_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)), tmp_fname)\n",
        "    # print(tmp_fname, flush=True)\n",
        "    f = open(tmp_fname, 'w')\n",
        "    f.write(\n",
        "        str(graph.number_of_nodes()) + ' ' + str(graph.number_of_edges()) + '\\n')\n",
        "    for (u, v) in edge_list_reindexed(graph):\n",
        "        f.write(str(u) + ' ' + str(v) + '\\n')\n",
        "    f.close()\n",
        "    output = sp.check_output(\n",
        "        [str(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'orca/orca')), 'node', '4', tmp_fname, 'std'])\n",
        "    output = output.decode('utf8').strip()\n",
        "    idx = output.find(COUNT_START_STR) + len(COUNT_START_STR) + 2\n",
        "    output = output[idx:]\n",
        "    node_orbit_counts = np.array([\n",
        "        list(map(int,\n",
        "                 node_cnts.strip().split(' ')))\n",
        "        for node_cnts in output.strip('\\n').split('\\n')\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        os.remove(tmp_fname)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    return node_orbit_counts\n",
        "\n",
        "\n",
        "def orbit_stats_all(graph_ref_list, graph_pred_list, compute_emd=False):\n",
        "    total_counts_ref = []\n",
        "    total_counts_pred = []\n",
        "\n",
        "    graph_pred_list_remove_empty = [\n",
        "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
        "    ]\n",
        "\n",
        "    for G in graph_ref_list:\n",
        "        orbit_counts = orca(G)\n",
        "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
        "        total_counts_ref.append(orbit_counts_graph)\n",
        "\n",
        "    for G in graph_pred_list:\n",
        "        orbit_counts = orca(G)\n",
        "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
        "        total_counts_pred.append(orbit_counts_graph)\n",
        "\n",
        "    total_counts_ref = np.array(total_counts_ref)\n",
        "    total_counts_pred = np.array(total_counts_pred)\n",
        "\n",
        "    # mmd_dist = compute_mmd(\n",
        "    #     total_counts_ref,\n",
        "    #     total_counts_pred,\n",
        "    #     kernel=gaussian,\n",
        "    #     is_hist=False,\n",
        "    #     sigma=30.0)\n",
        "\n",
        "    # mmd_dist = compute_mmd(\n",
        "    #         total_counts_ref,\n",
        "    #         total_counts_pred,\n",
        "    #         kernel=gaussian_tv,\n",
        "    #         is_hist=False,\n",
        "    #         sigma=30.0)  \n",
        "\n",
        "    if compute_emd:\n",
        "        # mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=emd, sigma=30.0)\n",
        "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
        "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian, is_hist=False, sigma=30.0)\n",
        "    else:\n",
        "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian_tv, is_hist=False, sigma=30.0)\n",
        "    return mmd_dist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gaussian(x, y, sigma=1.0):\n",
        "    support_size = max(len(x), len(y))\n",
        "    # convert histogram values x and y to float, and make them equal len\n",
        "    x = x.astype(float)\n",
        "    y = y.astype(float)\n",
        "    if len(x) < len(y):\n",
        "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
        "    elif len(y) < len(x):\n",
        "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
        "\n",
        "    dist = np.linalg.norm(x - y, 2)\n",
        "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
        "\n",
        "\n",
        "def gaussian_tv(x, y, sigma=1.0):  \n",
        "    support_size = max(len(x), len(y))\n",
        "    # convert histogram values x and y to float, and make them equal len\n",
        "    x = x.astype(float)\n",
        "    y = y.astype(float)\n",
        "    if len(x) < len(y):\n",
        "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
        "    elif len(y) < len(x):\n",
        "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
        "\n",
        "    dist = np.abs(x - y).sum() / 2.0\n",
        "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
        "\n",
        "\n",
        "def kernel_parallel_unpacked(x, samples2, kernel):\n",
        "    d = 0\n",
        "    for s2 in samples2:\n",
        "        d += kernel(x, s2)\n",
        "    return d\n",
        "\n",
        "\n",
        "def kernel_parallel_worker(t):\n",
        "    return kernel_parallel_unpacked(*t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def disc(samples1, samples2, kernel, is_parallel=True, *args, **kwargs):\n",
        "    ''' Discrepancy between 2 samples '''\n",
        "    d = 0\n",
        "\n",
        "    if not is_parallel:\n",
        "        for s1 in samples1:\n",
        "            for s2 in samples2:\n",
        "                d += kernel(s1, s2, *args, **kwargs)\n",
        "    else:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for dist in executor.map(kernel_parallel_worker, [\n",
        "                    (s1, samples2, partial(kernel, *args, **kwargs)) for s1 in samples1\n",
        "            ]):\n",
        "                d += dist\n",
        "    if len(samples1) * len(samples2) > 0:\n",
        "        d /= len(samples1) * len(samples2)\n",
        "    else:\n",
        "        d = 1e+6\n",
        "    #print('error')\n",
        "    return d\n",
        "\n",
        "\n",
        "def compute_mmd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
        "    ''' MMD between two samples '''\n",
        "    # normalize histograms into pmf\n",
        "    if is_hist:\n",
        "        samples1 = [s1 / (np.sum(s1) + 1e-6) for s1 in samples1]\n",
        "        samples2 = [s2 / (np.sum(s2) + 1e-6) for s2 in samples2]\n",
        "    print(samples1)\n",
        "    return disc(samples1, samples1, kernel, *args, **kwargs) + disc(samples2, samples2, kernel, *args, **kwargs) - \\\n",
        "                2 * disc(samples1, samples2, kernel, *args, **kwargs)\n",
        "\n",
        "\n",
        "def compute_emd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
        "    ''' EMD between average of two samples '''\n",
        "    # normalize histograms into pmf\n",
        "    if is_hist:\n",
        "        samples1 = [np.mean(samples1)]\n",
        "        samples2 = [np.mean(samples2)]\n",
        "    return disc(samples1, samples2, kernel, *args,\n",
        "                            **kwargs), [samples1[0], samples2[0]]\n",
        "\n",
        "\n",
        "\n",
        "def compute_mmd_stats(graphs_ref_nx, graphs_pred_nx):\n",
        "    return degree_stats(graphs_ref_nx, graphs_pred_nx), clustering_stats(graphs_ref_nx, graphs_pred_nx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pscy53ZLz9Se"
      },
      "outputs": [],
      "source": [
        "samples1 = [np.array([1,2,3.0]),  np.array([1,20,4.0])]\n",
        "samples2 = [np.array([1,2,3.5]),  np.array([1,20,4.5])]\n",
        "\n",
        "compute_mmd(samples1, samples2, kernel=gaussian_tv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTAYm3qa2rr_"
      },
      "outputs": [],
      "source": [
        "samples1 = [np.array([1,200,3.0]),  np.array([1,20,4.0])]\n",
        "samples2 = [np.array([1,2,3.01]),  np.array([1,20,4.01])]\n",
        "\n",
        "compute_mmd(samples1, samples2, kernel=gaussian_tv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpfa0Ldw0Pkm"
      },
      "outputs": [],
      "source": [
        "graph_ref_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
        "graph_pred_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
        "\n",
        "degree_stats(graph_ref_list, graph_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVv3mnMY-fal"
      },
      "outputs": [],
      "source": [
        "graph_ref_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
        "graph_pred_list = [nx.erdos_renyi_graph(15, 0.9) for _ in range(15) ]\n",
        "\n",
        "degree_stats(graph_ref_list, graph_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ScNBY6Z-vqM"
      },
      "outputs": [],
      "source": [
        "graph_ref_list = [nx.erdos_renyi_graph(15, 0.1) for _ in range(15) ]\n",
        "graph_pred_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
        "\n",
        "clustering_stats(graph_ref_list, graph_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O9ZYYVk4rWL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkpkT/AN4pXkyPuSSt70hr",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}