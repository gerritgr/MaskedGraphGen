{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gerritgr/MaskedGraphGen/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulnOzsCCmA8g"
   },
   "source": [
    "\n",
    "# <img src=\"https://mario.wiki.gallery/images/thumb/8/89/MPS_Toad_Artwork.png/170px-MPS_Toad_Artwork.png\" alt=\"Dataset Summary\" width=\"3%\">  Batched Toad  <img src=\"https://mario.wiki.gallery/images/thumb/8/89/MPS_Toad_Artwork.png/170px-MPS_Toad_Artwork.png\" alt=\"Dataset Summary\" width=\"3%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCWLJT5DLH78"
   },
   "source": [
    "Todos:\n",
    "- weightes sampling\n",
    "- multi-channel\n",
    "- uncertainty\n",
    "- better ensable like skip one during training or learn weights\n",
    "- to make network better: virutal node, degree input, spectral input, edge \n",
    "features\n",
    "- fix order, e.g. based on degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-k6-4CDmEwH"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Csn1_ff8mJCE"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "  os.system(\"pip install wandb -Uq\")\n",
    "  #!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "  #os.system(\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\")\n",
    "  os.system(\"pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\")\n",
    "  os.system(\"pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\")\n",
    "  os.system(\"pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\")\n",
    "  os.system(\"pip install pyemd\")\n",
    "\n",
    "\n",
    "PATH_TO_NOTEBOOK = \"Batched Toad 3.ipynb\"\n",
    "PROJECT_NAME = \"toad_retry\"\n",
    "KEY = \"e306abc657482e172a5ccc8d7c0b81bc4d297dd4\"\n",
    "WANDB_MODE = \"online\"   # \"online\" or \"dryrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cN9PW__MvfFk"
   },
   "outputs": [],
   "source": [
    "def get_wand_api_key():\n",
    "  api_key = KEY\n",
    "  # use   echo \"your_api_key_here\" > ~/api_key.txt   to write the api key to the home file dir    \n",
    "  if api_key != \"\":\n",
    "    return api_key\n",
    "  home_dir = os.path.expanduser('~')\n",
    "  file_path = os.path.join(home_dir, 'api_key.txt')\n",
    "  with open(file_path, 'r') as file:\n",
    "      api_key = file.read().strip()\n",
    "  return api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUcQ44Bj42GY"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nKbKpbiSmMfW"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100 # Set this to 300 to get better image quality\n",
    "from PIL import Image # We use PIL to load images\n",
    "import seaborn as sns\n",
    "import imageio # to generate .gifs\n",
    "\n",
    "# always good to have\n",
    "import glob, random, os, traceback\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "# the typical pytorch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear as Lin\n",
    "from torch.nn import Sequential as Seq\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyG\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "#Video\n",
    "from PIL import Image, ImageDraw\n",
    "#import cv2\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAyrOuvomPAE"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaElc28pmSzf",
    "outputId": "93b40587-aa74-40ea-b2a5-7e8b889f47a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_NODES = 12 #12   \n",
    "DEGREE = 3 #3\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "NUM_SAMPLES = 2000\n",
    "NUM_GRAPHS_GENERATE = 10 # 40 in compare set, but they use 10 in spectre\n",
    "\n",
    "GENERATE_X_EPOCHS = 100\n",
    "\n",
    "NO_EDGE_INDICATOR = 3.0\n",
    "EDGE_INDICATOR = 1.0 \n",
    "DUMMY = 0.0 # this node is an actual node not a placeholder for an edge\n",
    "MASK = 2.0\n",
    "NUM_CLASSES = 4  #todo change to 5 an\n",
    "\n",
    "EPSILON = 0.000000001\n",
    "\n",
    "NODE_FEATURE_EXTEND = 3\n",
    "\n",
    "# do i need this\n",
    "#BATCH_SIZE_TEST = 10\n",
    "\n",
    "#TIME_EMBEDDING_DIM = 1\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#DEVICE = torch.device('cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWpdimGn3fDH"
   },
   "source": [
    "### Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eCtxrGI4mVz2"
   },
   "outputs": [],
   "source": [
    "# from stack overflow\n",
    "def str_to_float(s, encoding=\"utf-8\"):\n",
    "  from zlib import crc32\n",
    "  def bytes_to_float(b):\n",
    "    return float(crc32(b) & 0xffffffff) / 2**32\n",
    "  return bytes_to_float(s.encode(encoding))\n",
    "\n",
    "    \n",
    "def set_seeds(seed=42):\n",
    "  if not \"int\" in str(type(seed)):\n",
    "    seed = int(str_to_float(str(seed))*1000 % 100000000)\n",
    "  np.random.seed(seed)\n",
    "  torch.random.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32hyXlJG8360"
   },
   "source": [
    "## Weights and Biases Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_pixfTW84rq",
    "outputId": "2d61a2f9-8b76-4999-b2a9-22f371c013d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerritgr\u001b[0m (\u001b[33mnextaid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/gerritgrossmann/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'graph-unmasking/gen-loss'},\n",
      " 'name': 'graph-unmasking',\n",
      " 'parameters': {'batch_size': {'values': [500]},\n",
      "                'candidate_selection_radius': {'values': [1, 5]},\n",
      "                'double_inference': {'values': [True, False]},\n",
      "                'dropout': {'values': [0.0]},\n",
      "                'graph_transform': {'values': [False]},\n",
      "                'hidden_dim': {'values': [16, 32]},\n",
      "                'hidden_layer': {'values': [5, 6]},\n",
      "                'learning_rate': {'values': [1e-05, 0.0001]},\n",
      "                'loss': {'values': ['bce']},\n",
      "                'model': {'values': ['pna']},\n",
      "                'noise_probability': {'values': [0.0, 0.05]},\n",
      "                'normalization': {'values': [False]},\n",
      "                'pre_post_layers': {'values': [1]},\n",
      "                'single_pass': {'values': [True]},\n",
      "                'towers': {'values': [1, 2]}}}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=get_wand_api_key())\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"graph-unmasking\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"graph-unmasking/gen-loss\",\n",
    "        \"goal\": \"minimize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [500]},  # write only\n",
    "        \"hidden_dim\": {\"values\": [16,32]},  \n",
    "        \"hidden_layer\": {\"values\": [5,6]},\n",
    "        \"learning_rate\": {\"values\": [0.00001,0.0001]},\n",
    "        \"dropout\": {\"values\": [0.0]},\n",
    "        \"normalization\": {\"values\": [False]},\n",
    "        \"model\": {\"values\": [\"pna\"]},  # [ \"pnamulti\", \"pna\",\"pna2\", \"attention\",\"transformer\", \"unet\"]}, \n",
    "        \"candidate_selection_radius\": {\"values\": [1, 5]},\n",
    "        \"loss\": {\"values\": [\"bce\"]},   # \"loss\": {\"values\": [\"l2\",\"l1\", \"bce\"]}, \n",
    "        \"single_pass\": {\"values\": [True]},  \n",
    "        \"towers\": {\"values\": [1, 2]},   \n",
    "        \"noise_probability\": {\"values\": [0.0, 0.05]},\n",
    "        \"graph_transform\": {\"values\": [False]},    \n",
    "        \"pre_post_layers\": {\"values\": [1]},   \n",
    "        \"double_inference\": {\"values\": [True, False]}, \n",
    "    },\n",
    "}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQyrEbzkmMCL"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9_zto3HQmp5v"
   },
   "outputs": [],
   "source": [
    "def lift_nx_to_pyg(g_nx):\n",
    "  import torch_geometric.transforms as T\n",
    "  transform = T.Compose([T.ToUndirected()])\n",
    "\n",
    "  # g_nx has to have node_labels 0 ... number_of_nodes-1\n",
    "  num_nodes = g_nx.number_of_nodes()\n",
    "  g_complete_nx = nx.complete_graph(num_nodes)\n",
    "\n",
    "  edges_new = list()\n",
    "  node_value_dict = dict()\n",
    "  for v1, v2 in g_complete_nx.edges():\n",
    "    v3 = (v1+1)*10000+(v2+1)*100000000000\n",
    "    edges_new.append((v1, v3))\n",
    "    edges_new.append((v2, v3))\n",
    "    node_value_dict[v1] = DUMMY\n",
    "    node_value_dict[v2] = DUMMY\n",
    "    node_value_dict[v3] = EDGE_INDICATOR if g_nx.has_edge(v1,v2) else NO_EDGE_INDICATOR\n",
    "\n",
    "  graph_new = nx.from_edgelist(edges_new)\n",
    "  for node, x_value in node_value_dict.items():\n",
    "    graph_new.nodes[node]['x'] = x_value\n",
    "\n",
    "  graph_new = nx.convert_node_labels_to_integers(graph_new, ordering='sorted')\n",
    "\n",
    "  g = from_networkx(graph_new, group_node_attrs=['x'])\n",
    "  g = transform(g)  # probably unnecs.\n",
    "  return g\n",
    "\n",
    "\n",
    "def reduce_nx_graph(g_old):\n",
    "  g_new = nx.Graph()\n",
    "  for v_i in g_old.nodes():\n",
    "    if g_old.nodes[v_i]['x'] == DUMMY:\n",
    "      g_new.add_node(v_i)\n",
    "      g_new.nodes[v_i]['x'] = DUMMY\n",
    "\n",
    "  for v_i in g_old.nodes():\n",
    "    if g_old.nodes[v_i]['x'] == EDGE_INDICATOR:\n",
    "      neigh_list = list(g_old.neighbors(v_i))\n",
    "      assert(len(neigh_list) == 2)\n",
    "      g_new.add_edge(neigh_list[0], neigh_list[1])\n",
    "\n",
    "  return g_new\n",
    "\n",
    "def pyg_graph_to_nx(g_pyg):\n",
    "  g_nx = to_networkx(g_pyg, node_attrs=['x'], to_undirected=True,)\n",
    "  g_nx = reduce_nx_graph(g_nx)\n",
    "  return g_nx\n",
    "\n",
    "def draw_pyg(g_pyg, ax = None, filename = None):\n",
    "  if ax is None:\n",
    "    plt.clf()\n",
    "\n",
    "  g_nx = pyg_graph_to_nx(g_pyg)\n",
    "\n",
    "  node_labels = {i: g_nx.nodes[i]['x'] for i in g_nx.nodes}\n",
    "  pos = nx.spring_layout(g_nx, seed=1234)\n",
    "  try:\n",
    "    nx.draw(g_nx, with_labels = True, labels=node_labels, ax=ax, pos=pos)\n",
    "  except:\n",
    "    pass\n",
    "  if filename is not None:\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "  return g_nx\n",
    "\n",
    "\n",
    "def shuffle_tensor(x):\n",
    "  x = x.clone()\n",
    "  x_shape = x.shape\n",
    "  x = x.flatten()\n",
    "  x = x[torch.randperm(x.numel())]\n",
    "  return x.reshape(x.shape)\n",
    "\n",
    "\n",
    "def note_features_to_one_hot(x):\n",
    "  # create indicator vector for NUM_CLASSES classes, values outside are set to all zeros (first to special class that then gets truncated)\n",
    "  x_cut = torch.where((x < -0.1) | (x > NUM_CLASSES-0.5), torch.tensor(NUM_CLASSES, device = DEVICE, dtype=torch.float), x)\n",
    "  x_one_hot = torch.zeros(x.shape[0], NUM_CLASSES+1, device = DEVICE)\n",
    "  x_one_hot.scatter_(1, x_cut.round().long().view(-1).unsqueeze(1), 1)\n",
    "  x_one_hot = x_one_hot[:,0:NUM_CLASSES]\n",
    "  return x_one_hot\n",
    "\n",
    "def global_mask_pool(x, x_in, batch):\n",
    "  mask_indicator = torch.lt(torch.sum(x_in, dim=1), 0.5).view(-1, 1)\n",
    "  x = x * mask_indicator\n",
    "  x = global_add_pool(x, batch)\n",
    "  return x \n",
    "\n",
    "\n",
    "def flip_edges_randomly(tensor, probability):\n",
    "  if probability < EPSILON:\n",
    "    return tensor\n",
    "\n",
    "  # Find the positions where the tensor has values 1.0 or 3.0\n",
    "  ones_positions = (tensor == EDGE_INDICATOR).to(DEVICE)\n",
    "  threes_positions = (tensor == NO_EDGE_INDICATOR).to(DEVICE)\n",
    "\n",
    "  # Generate a random binary mask for switching values\n",
    "  mask = torch.rand(tensor.shape, device=DEVICE) < probability\n",
    "\n",
    "  # Switch 1.0s to 3.0s using the random binary mask\n",
    "  tensor = torch.where(ones_positions & mask, torch.tensor(NO_EDGE_INDICATOR, device=DEVICE), tensor)\n",
    "\n",
    "  # Switch 3.0s to 1.0s using the inverted random binary mask\n",
    "  tensor = torch.where(threes_positions & mask, torch.tensor(EDGE_INDICATOR, device=DEVICE), tensor)\n",
    "\n",
    "  return tensor\n",
    "\n",
    "def extend_node_features(x, edge_index, batch):\n",
    "  assert(NODE_FEATURE_EXTEND == 3)\n",
    "\n",
    "  num_graphs = batch.max()+1\n",
    "  num_nodes = x.shape[0]\n",
    "  degree_feature = torch.zeros([num_nodes, NODE_FEATURE_EXTEND], device=DEVICE)\n",
    "  for i in range(edge_index.shape[1]):\n",
    "    src_node = edge_index[0,i]\n",
    "    target_node = edge_index[1,i]\n",
    "    # if src is real node and connection is edge indicator \n",
    "    if x[src_node, round(DUMMY)].item() > 0.5 and x[target_node, round(EDGE_INDICATOR)].item() > 0.5:\n",
    "      degree_feature[src_node,0] += 1.0\n",
    "    if x[src_node, round(DUMMY)].item() > 0.5 and x[target_node, round(MASK)].item() > 0.5:\n",
    "      degree_feature[src_node,1] += 1.0\n",
    "  for i in range(num_graphs):\n",
    "    num_mask_in_this_graph = torch.sum(x[batch == i,round(MASK)])\n",
    "    degree_feature[batch == i, 2] = num_mask_in_this_graph\n",
    "  x = torch.cat([x, degree_feature], dim=1)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEusLk_fd7hC",
    "outputId": "a378c5cb-eddf-450c-e156-4dc5ff9d4c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4, 4], edge_index=[2, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.ToUndirected()])\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 2, 3, 1, 2, 3],\n",
    "                           [1, 2, 3, 1, 0, 1, 2]], dtype=torch.long).to(DEVICE)\n",
    "x = torch.tensor([[0,1,0,0], [1,0,0,1], [0,0,1,1], [0,1,0,0]], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data = transform(data)\n",
    "data\n",
    "#extend_node_features(data.x, data.edge_index, batch=torch.zeros(data.x.shape[0], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "bJ3hIT0pmBXp",
    "outputId": "e560fd6e-07fc-48f4-f758-8b7aef4850c7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 2, 3, 1, 2, 3, 1],\n",
    "                           [1, 2, 3, 1, 0, 1, 2, 3]], dtype=torch.long).to(DEVICE)\n",
    "x = torch.tensor([[-1], [0], [1], [1]], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "g = torch_geometric.utils.to_networkx(data, to_undirected=True, node_attrs=['x'])\n",
    "try:\n",
    "  nx.draw(g, pos=nx.spring_layout(g),  with_labels = True)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiLapF9HnilF",
    "outputId": "839b03a1-c60c-471e-aab3-6a510e9bd49f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_index=[2, 24], x=[10, 1]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 6, 6, 7, 7, 8, 8, 9, 9],\n",
       "         [1, 3, 5, 0, 2, 1, 7, 8, 0, 4, 3, 7, 9, 0, 6, 5, 8, 9, 2, 4, 2, 6, 4, 6]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = lift_nx_to_pyg(g)\n",
    "data, data.x, data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "cgkOkx4amnfY",
    "outputId": "849946a8-af1e-4e85-95f4-920d9a86be26"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_pyg(data, filename='examplegraph.png')\n",
    "if IN_COLAB:\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaElQYAbWu8M"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwdGZitSyjKi"
   },
   "source": [
    "### d-Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mCpXui1mW9mp"
   },
   "outputs": [],
   "source": [
    "class ShuffleList:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.data)\n",
    "        self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index < len(self.data):\n",
    "            value = self.data[self.index]\n",
    "            self.index += 1\n",
    "            return value\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IgbW4Z0tWuN1"
   },
   "outputs": [],
   "source": [
    "def build_dataset(num_nodes=NUM_NODES, num_samples=NUM_SAMPLES, degree=DEGREE, seed=1234):\n",
    "  graph_dataset = list() \n",
    "\n",
    "  for _ in range(num_samples):\n",
    "    while True:\n",
    "      seed += 1\n",
    "      graph = nx.random_regular_graph(d=degree, n=num_nodes, seed=seed)\n",
    "      if nx.is_connected(graph):\n",
    "        graph.x = torch.ones(num_nodes).t()\n",
    "        graph_dataset.append(lift_nx_to_pyg(graph))\n",
    "        break\n",
    "\n",
    "  graph_dataset_train = graph_dataset[:int(len(graph_dataset)*0.8)]\n",
    "  graph_dataset_test = graph_dataset[int(len(graph_dataset)*0.8):]\n",
    "  return ShuffleList(graph_dataset_train), ShuffleList(graph_dataset_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3QWWQlO36fzH"
   },
   "outputs": [],
   "source": [
    "#def build_test_set_nx(seed):\n",
    "#  graph_dataset = build_dataset(num_samples=10, seed=seed)\n",
    "#  graph_dataset_nx = list()\n",
    "#  for i, g in enumerate(graph_dataset):\n",
    "#    nx_orig_graph = draw_pyg(g, filename=f\"test_graph_sample{str(i).zfill(5)}.jpg\")\n",
    "#    graph_dataset_nx.append(nx_orig_graph)\n",
    "#  return graph_dataset_nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "xKqFT6D7XHE7",
    "outputId": "394e07f2-265b-452e-e6ee-3e0356e65e6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2b27926a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset, _  = build_dataset()\n",
    "data = dataset[0]\n",
    "draw_pyg(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 264], x=[78, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2FUnVjG4Cwh",
    "outputId": "21044bdc-2c5c-4816-9804-267531ed3c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 10\n",
      "DataBatch(edge_index=[2, 2640], x=[780, 1], batch=[780], ptr=[11])\n",
      "\n",
      "tensor([0., 1., 0., 1., 0., 3., 0., 1., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3.,\n",
      "        0., 3., 0., 3., 0., 1., 3., 3., 3., 3., 3., 3., 3., 3., 1., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 3., 1., 1., 3., 1., 3., 3., 1.,\n",
      "        3., 3., 3., 3., 3., 1., 3., 3., 3., 1., 3., 1., 3., 1., 1., 3., 3., 1.,\n",
      "        3., 3., 3., 1., 3., 3., 0., 3., 0., 3., 0., 3., 0., 3., 0., 1., 0., 3.,\n",
      "        0., 3., 0., 3., 0., 3., 0., 1., 0., 1., 0., 3., 3., 1., 3., 3., 3., 1.,\n",
      "        3., 3., 1., 3., 3., 3., 3., 1., 1., 3., 1., 3., 1., 1., 1., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 3., 1., 3., 1.,\n",
      "        3., 3., 3., 1., 3., 3., 3., 3., 3., 1., 3., 3., 0., 3., 0., 3., 0., 3.,\n",
      "        0., 3., 0., 1., 0., 3., 0., 1., 0., 3., 0., 3., 0., 1., 0., 3., 0., 1.,\n",
      "        1., 3., 3., 1., 3., 3., 3., 3., 3., 1., 3., 3., 3., 3., 3., 1., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 1., 3., 1., 3., 1., 3.,\n",
      "        3., 1., 3., 3., 1., 3., 3., 3., 1., 3., 3., 3., 3., 3., 1., 3., 1., 3.,\n",
      "        0., 3., 0., 1., 0., 1., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3.,\n",
      "        0., 1., 0., 3., 0., 3., 3., 3., 1., 3., 1., 1., 3., 3., 3., 3., 1., 3.,\n",
      "        3., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 3., 3., 3.,\n",
      "        3., 3., 1., 3., 3., 3., 1., 3., 3., 3., 1., 1., 3., 1., 3., 1., 3., 3.,\n",
      "        3., 1., 3., 3., 3., 3., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3.,\n",
      "        0., 1., 0., 1., 0., 3., 0., 1., 0., 3., 0., 3., 3., 3., 1., 3., 3., 3.,\n",
      "        3., 1., 1., 3., 1., 3., 3., 3., 1., 1., 3., 3., 1., 3., 1., 1., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 1., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        1., 3., 3., 3., 3., 3., 3., 3., 1., 3., 1., 3., 0., 3., 0., 1., 0., 3.,\n",
      "        0., 3., 0., 3., 0., 3., 0., 1., 0., 3., 0., 1., 0., 3., 0., 3., 0., 3.,\n",
      "        1., 3., 1., 3., 3., 3., 3., 3., 1., 3., 3., 3., 3., 1., 3., 3., 1., 3.,\n",
      "        3., 1., 3., 3., 3., 3., 1., 3., 3., 1., 3., 1., 3., 3., 1., 1., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 1., 3., 1., 3., 3., 3.,\n",
      "        0., 3., 0., 3., 0., 3., 0., 3., 0., 3., 0., 1., 0., 3., 0., 1., 0., 3.,\n",
      "        0., 1., 0., 3., 0., 1., 3., 3., 3., 3., 3., 1., 3., 1., 3., 3., 3., 3.,\n",
      "        3., 1., 3., 3., 3., 1., 1., 1., 3., 3., 3., 1., 3., 3., 3., 1., 3., 3.,\n",
      "        3., 3., 1., 3., 1., 3., 3., 3., 1., 3., 1., 3., 3., 3., 3., 1., 3., 3.,\n",
      "        3., 3., 3., 1., 3., 3., 0., 1., 0., 3., 0., 3., 0., 3., 0., 3., 0., 3.,\n",
      "        0., 1., 0., 3., 0., 3., 0., 3., 0., 1., 0., 3., 1., 3., 3., 3., 3., 3.,\n",
      "        1., 3., 3., 3., 3., 3., 1., 3., 1., 3., 1., 3., 3., 3., 3., 3., 1., 3.,\n",
      "        1., 3., 1., 3., 1., 3., 1., 3., 3., 1., 3., 3., 1., 3., 3., 3., 3., 3.,\n",
      "        3., 1., 3., 3., 1., 3., 3., 3., 1., 3., 3., 3., 0., 1., 0., 3., 0., 3.,\n",
      "        0., 1., 0., 3., 0., 3., 0., 3., 0., 3., 0., 1., 0., 3., 0., 3., 0., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 1.,\n",
      "        1., 3., 3., 3., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 1., 3., 1.,\n",
      "        1., 3., 3., 1., 3., 1., 3., 3., 3., 3., 1., 1., 3., 1., 3., 3., 3., 3.,\n",
      "        0., 1., 0., 3., 0., 3., 0., 1., 0., 3., 0., 3., 0., 3., 0., 3., 0., 1.,\n",
      "        0., 3., 0., 3., 0., 3., 1., 3., 1., 3., 3., 3., 3., 3., 3., 3., 1., 1.,\n",
      "        3., 3., 3., 1., 3., 3., 3., 3., 3., 3., 1., 3., 3., 1., 3., 3., 3., 3.,\n",
      "        3., 1., 3., 1., 3., 3., 3., 3., 3., 1., 1., 3., 3., 3., 1., 3., 3., 1.,\n",
      "        3., 3., 3., 1., 3., 1.]) tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   2,\n",
      "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   3,   3,   4,   4,\n",
      "           4,   4,   4,   4,   4,   4,   4,   4,   4,   5,   5,   6,   6,   6,\n",
      "           6,   6,   6,   6,   6,   6,   6,   6,   7,   7,   8,   8,   8,   8,\n",
      "           8,   8,   8,   8,   8,   8,   8,   9,   9,  10,  10,  10,  10,  10,\n",
      "          10,  10,  10,  10,  10,  10,  11,  11,  12,  12,  12,  12,  12,  12,\n",
      "          12,  12,  12,  12,  12,  13,  13,  14,  14,  14,  14,  14,  14,  14,\n",
      "          14,  14,  14,  14,  15,  15,  16,  16,  16,  16,  16,  16,  16,  16,\n",
      "          16,  16,  16,  17,  17,  18,  18,  18,  18,  18,  18,  18,  18,  18,\n",
      "          18,  18,  19,  19,  20,  20,  20,  20,  20,  20,  20,  20,  20,  20,\n",
      "          20,  21,  21,  22,  22,  22,  22,  22,  22,  22,  22,  22,  22,  22,\n",
      "          23,  23,  24,  24,  25,  25,  26,  26,  27,  27,  28,  28,  29,  29,\n",
      "          30,  30,  31,  31,  32,  32,  33,  33,  34,  34,  35,  35,  36,  36,\n",
      "          37,  37,  38,  38,  39,  39,  40,  40,  41,  41,  42,  42,  43,  43,\n",
      "          44,  44,  45,  45,  46,  46,  47,  47,  48,  48,  49,  49,  50,  50,\n",
      "          51,  51,  52,  52,  53,  53,  54,  54,  55,  55,  56,  56,  57,  57,\n",
      "          58,  58,  59,  59,  60,  60,  61,  61,  62,  62,  63,  63,  64,  64,\n",
      "          65,  65,  66,  66,  67,  67,  68,  68,  69,  69,  70,  70,  71,  71,\n",
      "          72,  72,  73,  73,  74,  74,  75,  75,  76,  76,  77,  77,  78,  78,\n",
      "          78,  78,  78,  78,  78,  78,  78,  78,  78,  79,  79,  80,  80,  80,\n",
      "          80,  80,  80,  80,  80,  80,  80,  80,  81,  81,  82,  82,  82,  82,\n",
      "          82,  82,  82,  82,  82,  82,  82,  83,  83,  84,  84,  84,  84,  84,\n",
      "          84,  84,  84,  84,  84,  84,  85,  85,  86,  86,  86,  86,  86,  86,\n",
      "          86,  86,  86,  86,  86,  87,  87,  88,  88,  88,  88,  88,  88,  88,\n",
      "          88,  88,  88,  88,  89,  89,  90,  90,  90,  90,  90,  90,  90,  90,\n",
      "          90,  90,  90,  91,  91,  92,  92,  92,  92,  92,  92,  92,  92,  92,\n",
      "          92,  92,  93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
      "          94,  95,  95,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,\n",
      "          97,  97,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  99,\n",
      "          99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101,\n",
      "         102, 102, 103, 103, 104, 104, 105, 105, 106, 106, 107, 107, 108, 108,\n",
      "         109, 109, 110, 110, 111, 111, 112, 112, 113, 113, 114, 114, 115, 115,\n",
      "         116, 116, 117, 117, 118, 118, 119, 119, 120, 120, 121, 121, 122, 122,\n",
      "         123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 129, 129,\n",
      "         130, 130, 131, 131, 132, 132, 133, 133, 134, 134, 135, 135, 136, 136,\n",
      "         137, 137, 138, 138, 139, 139, 140, 140, 141, 141, 142, 142, 143, 143,\n",
      "         144, 144, 145, 145, 146, 146, 147, 147, 148, 148, 149, 149, 150, 150,\n",
      "         151, 151, 152, 152, 153, 153, 154, 154, 155, 155, 156, 156, 156, 156,\n",
      "         156, 156, 156, 156, 156, 156, 156, 157, 157, 158, 158, 158, 158, 158,\n",
      "         158, 158, 158, 158, 158, 158, 159, 159, 160, 160, 160, 160, 160, 160,\n",
      "         160, 160, 160, 160, 160, 161, 161, 162, 162, 162, 162, 162, 162, 162,\n",
      "         162, 162, 162, 162, 163, 163, 164, 164, 164, 164, 164, 164, 164, 164,\n",
      "         164, 164, 164, 165, 165, 166, 166, 166, 166, 166, 166, 166, 166, 166,\n",
      "         166, 166, 167, 167, 168, 168, 168, 168, 168, 168, 168, 168, 168, 168,\n",
      "         168, 169, 169, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170, 170,\n",
      "         171, 171, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 172, 173,\n",
      "         173, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 175, 175,\n",
      "         176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 176, 177, 177, 178,\n",
      "         178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 179, 179, 180, 180,\n",
      "         181, 181, 182, 182, 183, 183, 184, 184, 185, 185, 186, 186, 187, 187,\n",
      "         188, 188, 189, 189, 190, 190, 191, 191, 192, 192, 193, 193, 194, 194,\n",
      "         195, 195, 196, 196, 197, 197, 198, 198, 199, 199, 200, 200, 201, 201,\n",
      "         202, 202, 203, 203, 204, 204, 205, 205, 206, 206, 207, 207, 208, 208,\n",
      "         209, 209, 210, 210, 211, 211, 212, 212, 213, 213, 214, 214, 215, 215,\n",
      "         216, 216, 217, 217, 218, 218, 219, 219, 220, 220, 221, 221, 222, 222,\n",
      "         223, 223, 224, 224, 225, 225, 226, 226, 227, 227, 228, 228, 229, 229,\n",
      "         230, 230, 231, 231, 232, 232, 233, 233, 234, 234, 234, 234, 234, 234,\n",
      "         234, 234, 234, 234, 234, 235, 235, 236, 236, 236, 236, 236, 236, 236,\n",
      "         236, 236, 236, 236, 237, 237, 238, 238, 238, 238, 238, 238, 238, 238,\n",
      "         238, 238, 238, 239, 239, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "         240, 240, 241, 241, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
      "         242, 243, 243, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "         245, 245, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 247,\n",
      "         247, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 249, 249,\n",
      "         250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 251, 251, 252,\n",
      "         252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 253, 253, 254, 254,\n",
      "         254, 254, 254, 254, 254, 254, 254, 254, 254, 255, 255, 256, 256, 256,\n",
      "         256, 256, 256, 256, 256, 256, 256, 256, 257, 257, 258, 258, 259, 259,\n",
      "         260, 260, 261, 261, 262, 262, 263, 263, 264, 264, 265, 265, 266, 266,\n",
      "         267, 267, 268, 268, 269, 269, 270, 270, 271, 271, 272, 272, 273, 273,\n",
      "         274, 274, 275, 275, 276, 276, 277, 277, 278, 278, 279, 279, 280, 280,\n",
      "         281, 281, 282, 282, 283, 283, 284, 284, 285, 285, 286, 286, 287, 287,\n",
      "         288, 288, 289, 289, 290, 290, 291, 291, 292, 292, 293, 293, 294, 294,\n",
      "         295, 295, 296, 296, 297, 297, 298, 298, 299, 299, 300, 300, 301, 301,\n",
      "         302, 302, 303, 303, 304, 304, 305, 305, 306, 306, 307, 307, 308, 308,\n",
      "         309, 309, 310, 310, 311, 311, 312, 312, 312, 312, 312, 312, 312, 312,\n",
      "         312, 312, 312, 313, 313, 314, 314, 314, 314, 314, 314, 314, 314, 314,\n",
      "         314, 314, 315, 315, 316, 316, 316, 316, 316, 316, 316, 316, 316, 316,\n",
      "         316, 317, 317, 318, 318, 318, 318, 318, 318, 318, 318, 318, 318, 318,\n",
      "         319, 319, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 321,\n",
      "         321, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 322, 323, 323,\n",
      "         324, 324, 324, 324, 324, 324, 324, 324, 324, 324, 324, 325, 325, 326,\n",
      "         326, 326, 326, 326, 326, 326, 326, 326, 326, 326, 327, 327, 328, 328,\n",
      "         328, 328, 328, 328, 328, 328, 328, 328, 328, 329, 329, 330, 330, 330,\n",
      "         330, 330, 330, 330, 330, 330, 330, 330, 331, 331, 332, 332, 332, 332,\n",
      "         332, 332, 332, 332, 332, 332, 332, 333, 333, 334, 334, 334, 334, 334,\n",
      "         334, 334, 334, 334, 334, 334, 335, 335, 336, 336, 337, 337, 338, 338,\n",
      "         339, 339, 340, 340, 341, 341, 342, 342, 343, 343, 344, 344, 345, 345,\n",
      "         346, 346, 347, 347, 348, 348, 349, 349, 350, 350, 351, 351, 352, 352,\n",
      "         353, 353, 354, 354, 355, 355, 356, 356, 357, 357, 358, 358, 359, 359,\n",
      "         360, 360, 361, 361, 362, 362, 363, 363, 364, 364, 365, 365, 366, 366,\n",
      "         367, 367, 368, 368, 369, 369, 370, 370, 371, 371, 372, 372, 373, 373,\n",
      "         374, 374, 375, 375, 376, 376, 377, 377, 378, 378, 379, 379, 380, 380,\n",
      "         381, 381, 382, 382, 383, 383, 384, 384, 385, 385, 386, 386, 387, 387,\n",
      "         388, 388, 389, 389, 390, 390, 390, 390, 390, 390, 390, 390, 390, 390,\n",
      "         390, 391, 391, 392, 392, 392, 392, 392, 392, 392, 392, 392, 392, 392,\n",
      "         393, 393, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 395,\n",
      "         395, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 396, 397, 397,\n",
      "         398, 398, 398, 398, 398, 398, 398, 398, 398, 398, 398, 399, 399, 400,\n",
      "         400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 401, 401, 402, 402,\n",
      "         402, 402, 402, 402, 402, 402, 402, 402, 402, 403, 403, 404, 404, 404,\n",
      "         404, 404, 404, 404, 404, 404, 404, 404, 405, 405, 406, 406, 406, 406,\n",
      "         406, 406, 406, 406, 406, 406, 406, 407, 407, 408, 408, 408, 408, 408,\n",
      "         408, 408, 408, 408, 408, 408, 409, 409, 410, 410, 410, 410, 410, 410,\n",
      "         410, 410, 410, 410, 410, 411, 411, 412, 412, 412, 412, 412, 412, 412,\n",
      "         412, 412, 412, 412, 413, 413, 414, 414, 415, 415, 416, 416, 417, 417,\n",
      "         418, 418, 419, 419, 420, 420, 421, 421, 422, 422, 423, 423, 424, 424,\n",
      "         425, 425, 426, 426, 427, 427, 428, 428, 429, 429, 430, 430, 431, 431,\n",
      "         432, 432, 433, 433, 434, 434, 435, 435, 436, 436, 437, 437, 438, 438,\n",
      "         439, 439, 440, 440, 441, 441, 442, 442, 443, 443, 444, 444, 445, 445,\n",
      "         446, 446, 447, 447, 448, 448, 449, 449, 450, 450, 451, 451, 452, 452,\n",
      "         453, 453, 454, 454, 455, 455, 456, 456, 457, 457, 458, 458, 459, 459,\n",
      "         460, 460, 461, 461, 462, 462, 463, 463, 464, 464, 465, 465, 466, 466,\n",
      "         467, 467, 468, 468, 468, 468, 468, 468, 468, 468, 468, 468, 468, 469,\n",
      "         469, 470, 470, 470, 470, 470, 470, 470, 470, 470, 470, 470, 471, 471,\n",
      "         472, 472, 472, 472, 472, 472, 472, 472, 472, 472, 472, 473, 473, 474,\n",
      "         474, 474, 474, 474, 474, 474, 474, 474, 474, 474, 475, 475, 476, 476,\n",
      "         476, 476, 476, 476, 476, 476, 476, 476, 476, 477, 477, 478, 478, 478,\n",
      "         478, 478, 478, 478, 478, 478, 478, 478, 479, 479, 480, 480, 480, 480,\n",
      "         480, 480, 480, 480, 480, 480, 480, 481, 481, 482, 482, 482, 482, 482,\n",
      "         482, 482, 482, 482, 482, 482, 483, 483, 484, 484, 484, 484, 484, 484,\n",
      "         484, 484, 484, 484, 484, 485, 485, 486, 486, 486, 486, 486, 486, 486,\n",
      "         486, 486, 486, 486, 487, 487, 488, 488, 488, 488, 488, 488, 488, 488,\n",
      "         488, 488, 488, 489, 489, 490, 490, 490, 490, 490, 490, 490, 490, 490,\n",
      "         490, 490, 491, 491, 492, 492, 493, 493, 494, 494, 495, 495, 496, 496,\n",
      "         497, 497, 498, 498, 499, 499, 500, 500, 501, 501, 502, 502, 503, 503,\n",
      "         504, 504, 505, 505, 506, 506, 507, 507, 508, 508, 509, 509, 510, 510,\n",
      "         511, 511, 512, 512, 513, 513, 514, 514, 515, 515, 516, 516, 517, 517,\n",
      "         518, 518, 519, 519, 520, 520, 521, 521, 522, 522, 523, 523, 524, 524,\n",
      "         525, 525, 526, 526, 527, 527, 528, 528, 529, 529, 530, 530, 531, 531,\n",
      "         532, 532, 533, 533, 534, 534, 535, 535, 536, 536, 537, 537, 538, 538,\n",
      "         539, 539, 540, 540, 541, 541, 542, 542, 543, 543, 544, 544, 545, 545,\n",
      "         546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 547, 547, 548,\n",
      "         548, 548, 548, 548, 548, 548, 548, 548, 548, 548, 549, 549, 550, 550,\n",
      "         550, 550, 550, 550, 550, 550, 550, 550, 550, 551, 551, 552, 552, 552,\n",
      "         552, 552, 552, 552, 552, 552, 552, 552, 553, 553, 554, 554, 554, 554,\n",
      "         554, 554, 554, 554, 554, 554, 554, 555, 555, 556, 556, 556, 556, 556,\n",
      "         556, 556, 556, 556, 556, 556, 557, 557, 558, 558, 558, 558, 558, 558,\n",
      "         558, 558, 558, 558, 558, 559, 559, 560, 560, 560, 560, 560, 560, 560,\n",
      "         560, 560, 560, 560, 561, 561, 562, 562, 562, 562, 562, 562, 562, 562,\n",
      "         562, 562, 562, 563, 563, 564, 564, 564, 564, 564, 564, 564, 564, 564,\n",
      "         564, 564, 565, 565, 566, 566, 566, 566, 566, 566, 566, 566, 566, 566,\n",
      "         566, 567, 567, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568,\n",
      "         569, 569, 570, 570, 571, 571, 572, 572, 573, 573, 574, 574, 575, 575,\n",
      "         576, 576, 577, 577, 578, 578, 579, 579, 580, 580, 581, 581, 582, 582,\n",
      "         583, 583, 584, 584, 585, 585, 586, 586, 587, 587, 588, 588, 589, 589,\n",
      "         590, 590, 591, 591, 592, 592, 593, 593, 594, 594, 595, 595, 596, 596,\n",
      "         597, 597, 598, 598, 599, 599, 600, 600, 601, 601, 602, 602, 603, 603,\n",
      "         604, 604, 605, 605, 606, 606, 607, 607, 608, 608, 609, 609, 610, 610,\n",
      "         611, 611, 612, 612, 613, 613, 614, 614, 615, 615, 616, 616, 617, 617,\n",
      "         618, 618, 619, 619, 620, 620, 621, 621, 622, 622, 623, 623, 624, 624,\n",
      "         624, 624, 624, 624, 624, 624, 624, 624, 624, 625, 625, 626, 626, 626,\n",
      "         626, 626, 626, 626, 626, 626, 626, 626, 627, 627, 628, 628, 628, 628,\n",
      "         628, 628, 628, 628, 628, 628, 628, 629, 629, 630, 630, 630, 630, 630,\n",
      "         630, 630, 630, 630, 630, 630, 631, 631, 632, 632, 632, 632, 632, 632,\n",
      "         632, 632, 632, 632, 632, 633, 633, 634, 634, 634, 634, 634, 634, 634,\n",
      "         634, 634, 634, 634, 635, 635, 636, 636, 636, 636, 636, 636, 636, 636,\n",
      "         636, 636, 636, 637, 637, 638, 638, 638, 638, 638, 638, 638, 638, 638,\n",
      "         638, 638, 639, 639, 640, 640, 640, 640, 640, 640, 640, 640, 640, 640,\n",
      "         640, 641, 641, 642, 642, 642, 642, 642, 642, 642, 642, 642, 642, 642,\n",
      "         643, 643, 644, 644, 644, 644, 644, 644, 644, 644, 644, 644, 644, 645,\n",
      "         645, 646, 646, 646, 646, 646, 646, 646, 646, 646, 646, 646, 647, 647,\n",
      "         648, 648, 649, 649, 650, 650, 651, 651, 652, 652, 653, 653, 654, 654,\n",
      "         655, 655, 656, 656, 657, 657, 658, 658, 659, 659, 660, 660, 661, 661,\n",
      "         662, 662, 663, 663, 664, 664, 665, 665, 666, 666, 667, 667, 668, 668,\n",
      "         669, 669, 670, 670, 671, 671, 672, 672, 673, 673, 674, 674, 675, 675,\n",
      "         676, 676, 677, 677, 678, 678, 679, 679, 680, 680, 681, 681, 682, 682,\n",
      "         683, 683, 684, 684, 685, 685, 686, 686, 687, 687, 688, 688, 689, 689,\n",
      "         690, 690, 691, 691, 692, 692, 693, 693, 694, 694, 695, 695, 696, 696,\n",
      "         697, 697, 698, 698, 699, 699, 700, 700, 701, 701, 702, 702, 702, 702,\n",
      "         702, 702, 702, 702, 702, 702, 702, 703, 703, 704, 704, 704, 704, 704,\n",
      "         704, 704, 704, 704, 704, 704, 705, 705, 706, 706, 706, 706, 706, 706,\n",
      "         706, 706, 706, 706, 706, 707, 707, 708, 708, 708, 708, 708, 708, 708,\n",
      "         708, 708, 708, 708, 709, 709, 710, 710, 710, 710, 710, 710, 710, 710,\n",
      "         710, 710, 710, 711, 711, 712, 712, 712, 712, 712, 712, 712, 712, 712,\n",
      "         712, 712, 713, 713, 714, 714, 714, 714, 714, 714, 714, 714, 714, 714,\n",
      "         714, 715, 715, 716, 716, 716, 716, 716, 716, 716, 716, 716, 716, 716,\n",
      "         717, 717, 718, 718, 718, 718, 718, 718, 718, 718, 718, 718, 718, 719,\n",
      "         719, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 721, 721,\n",
      "         722, 722, 722, 722, 722, 722, 722, 722, 722, 722, 722, 723, 723, 724,\n",
      "         724, 724, 724, 724, 724, 724, 724, 724, 724, 724, 725, 725, 726, 726,\n",
      "         727, 727, 728, 728, 729, 729, 730, 730, 731, 731, 732, 732, 733, 733,\n",
      "         734, 734, 735, 735, 736, 736, 737, 737, 738, 738, 739, 739, 740, 740,\n",
      "         741, 741, 742, 742, 743, 743, 744, 744, 745, 745, 746, 746, 747, 747,\n",
      "         748, 748, 749, 749, 750, 750, 751, 751, 752, 752, 753, 753, 754, 754,\n",
      "         755, 755, 756, 756, 757, 757, 758, 758, 759, 759, 760, 760, 761, 761,\n",
      "         762, 762, 763, 763, 764, 764, 765, 765, 766, 766, 767, 767, 768, 768,\n",
      "         769, 769, 770, 770, 771, 771, 772, 772, 773, 773, 774, 774, 775, 775,\n",
      "         776, 776, 777, 777, 778, 778, 779, 779],\n",
      "        [  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,   0,   2,   1,\n",
      "          23,  24,  25,  26,  27,  28,  29,  30,  31,  32,   0,   4,   3,  23,\n",
      "          33,  34,  35,  36,  37,  38,  39,  40,  41,   0,   6,   5,  24,  33,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,   0,   8,   7,  25,  34,  42,\n",
      "          50,  51,  52,  53,  54,  55,  56,   0,  10,   9,  26,  35,  43,  50,\n",
      "          57,  58,  59,  60,  61,  62,   0,  12,  11,  27,  36,  44,  51,  57,\n",
      "          63,  64,  65,  66,  67,   0,  14,  13,  28,  37,  45,  52,  58,  63,\n",
      "          68,  69,  70,  71,   0,  16,  15,  29,  38,  46,  53,  59,  64,  68,\n",
      "          72,  73,  74,   0,  18,  17,  30,  39,  47,  54,  60,  65,  69,  72,\n",
      "          75,  76,   0,  20,  19,  31,  40,  48,  55,  61,  66,  70,  73,  75,\n",
      "          77,   0,  22,  21,  32,  41,  49,  56,  62,  67,  71,  74,  76,  77,\n",
      "           2,   4,   2,   6,   2,   8,   2,  10,   2,  12,   2,  14,   2,  16,\n",
      "           2,  18,   2,  20,   2,  22,   4,   6,   4,   8,   4,  10,   4,  12,\n",
      "           4,  14,   4,  16,   4,  18,   4,  20,   4,  22,   6,   8,   6,  10,\n",
      "           6,  12,   6,  14,   6,  16,   6,  18,   6,  20,   6,  22,   8,  10,\n",
      "           8,  12,   8,  14,   8,  16,   8,  18,   8,  20,   8,  22,  10,  12,\n",
      "          10,  14,  10,  16,  10,  18,  10,  20,  10,  22,  12,  14,  12,  16,\n",
      "          12,  18,  12,  20,  12,  22,  14,  16,  14,  18,  14,  20,  14,  22,\n",
      "          16,  18,  16,  20,  16,  22,  18,  20,  18,  22,  20,  22,  79,  81,\n",
      "          83,  85,  87,  89,  91,  93,  95,  97,  99,  78,  80,  79, 101, 102,\n",
      "         103, 104, 105, 106, 107, 108, 109, 110,  78,  82,  81, 101, 111, 112,\n",
      "         113, 114, 115, 116, 117, 118, 119,  78,  84,  83, 102, 111, 120, 121,\n",
      "         122, 123, 124, 125, 126, 127,  78,  86,  85, 103, 112, 120, 128, 129,\n",
      "         130, 131, 132, 133, 134,  78,  88,  87, 104, 113, 121, 128, 135, 136,\n",
      "         137, 138, 139, 140,  78,  90,  89, 105, 114, 122, 129, 135, 141, 142,\n",
      "         143, 144, 145,  78,  92,  91, 106, 115, 123, 130, 136, 141, 146, 147,\n",
      "         148, 149,  78,  94,  93, 107, 116, 124, 131, 137, 142, 146, 150, 151,\n",
      "         152,  78,  96,  95, 108, 117, 125, 132, 138, 143, 147, 150, 153, 154,\n",
      "          78,  98,  97, 109, 118, 126, 133, 139, 144, 148, 151, 153, 155,  78,\n",
      "         100,  99, 110, 119, 127, 134, 140, 145, 149, 152, 154, 155,  80,  82,\n",
      "          80,  84,  80,  86,  80,  88,  80,  90,  80,  92,  80,  94,  80,  96,\n",
      "          80,  98,  80, 100,  82,  84,  82,  86,  82,  88,  82,  90,  82,  92,\n",
      "          82,  94,  82,  96,  82,  98,  82, 100,  84,  86,  84,  88,  84,  90,\n",
      "          84,  92,  84,  94,  84,  96,  84,  98,  84, 100,  86,  88,  86,  90,\n",
      "          86,  92,  86,  94,  86,  96,  86,  98,  86, 100,  88,  90,  88,  92,\n",
      "          88,  94,  88,  96,  88,  98,  88, 100,  90,  92,  90,  94,  90,  96,\n",
      "          90,  98,  90, 100,  92,  94,  92,  96,  92,  98,  92, 100,  94,  96,\n",
      "          94,  98,  94, 100,  96,  98,  96, 100,  98, 100, 157, 159, 161, 163,\n",
      "         165, 167, 169, 171, 173, 175, 177, 156, 158, 157, 179, 180, 181, 182,\n",
      "         183, 184, 185, 186, 187, 188, 156, 160, 159, 179, 189, 190, 191, 192,\n",
      "         193, 194, 195, 196, 197, 156, 162, 161, 180, 189, 198, 199, 200, 201,\n",
      "         202, 203, 204, 205, 156, 164, 163, 181, 190, 198, 206, 207, 208, 209,\n",
      "         210, 211, 212, 156, 166, 165, 182, 191, 199, 206, 213, 214, 215, 216,\n",
      "         217, 218, 156, 168, 167, 183, 192, 200, 207, 213, 219, 220, 221, 222,\n",
      "         223, 156, 170, 169, 184, 193, 201, 208, 214, 219, 224, 225, 226, 227,\n",
      "         156, 172, 171, 185, 194, 202, 209, 215, 220, 224, 228, 229, 230, 156,\n",
      "         174, 173, 186, 195, 203, 210, 216, 221, 225, 228, 231, 232, 156, 176,\n",
      "         175, 187, 196, 204, 211, 217, 222, 226, 229, 231, 233, 156, 178, 177,\n",
      "         188, 197, 205, 212, 218, 223, 227, 230, 232, 233, 158, 160, 158, 162,\n",
      "         158, 164, 158, 166, 158, 168, 158, 170, 158, 172, 158, 174, 158, 176,\n",
      "         158, 178, 160, 162, 160, 164, 160, 166, 160, 168, 160, 170, 160, 172,\n",
      "         160, 174, 160, 176, 160, 178, 162, 164, 162, 166, 162, 168, 162, 170,\n",
      "         162, 172, 162, 174, 162, 176, 162, 178, 164, 166, 164, 168, 164, 170,\n",
      "         164, 172, 164, 174, 164, 176, 164, 178, 166, 168, 166, 170, 166, 172,\n",
      "         166, 174, 166, 176, 166, 178, 168, 170, 168, 172, 168, 174, 168, 176,\n",
      "         168, 178, 170, 172, 170, 174, 170, 176, 170, 178, 172, 174, 172, 176,\n",
      "         172, 178, 174, 176, 174, 178, 176, 178, 235, 237, 239, 241, 243, 245,\n",
      "         247, 249, 251, 253, 255, 234, 236, 235, 257, 258, 259, 260, 261, 262,\n",
      "         263, 264, 265, 266, 234, 238, 237, 257, 267, 268, 269, 270, 271, 272,\n",
      "         273, 274, 275, 234, 240, 239, 258, 267, 276, 277, 278, 279, 280, 281,\n",
      "         282, 283, 234, 242, 241, 259, 268, 276, 284, 285, 286, 287, 288, 289,\n",
      "         290, 234, 244, 243, 260, 269, 277, 284, 291, 292, 293, 294, 295, 296,\n",
      "         234, 246, 245, 261, 270, 278, 285, 291, 297, 298, 299, 300, 301, 234,\n",
      "         248, 247, 262, 271, 279, 286, 292, 297, 302, 303, 304, 305, 234, 250,\n",
      "         249, 263, 272, 280, 287, 293, 298, 302, 306, 307, 308, 234, 252, 251,\n",
      "         264, 273, 281, 288, 294, 299, 303, 306, 309, 310, 234, 254, 253, 265,\n",
      "         274, 282, 289, 295, 300, 304, 307, 309, 311, 234, 256, 255, 266, 275,\n",
      "         283, 290, 296, 301, 305, 308, 310, 311, 236, 238, 236, 240, 236, 242,\n",
      "         236, 244, 236, 246, 236, 248, 236, 250, 236, 252, 236, 254, 236, 256,\n",
      "         238, 240, 238, 242, 238, 244, 238, 246, 238, 248, 238, 250, 238, 252,\n",
      "         238, 254, 238, 256, 240, 242, 240, 244, 240, 246, 240, 248, 240, 250,\n",
      "         240, 252, 240, 254, 240, 256, 242, 244, 242, 246, 242, 248, 242, 250,\n",
      "         242, 252, 242, 254, 242, 256, 244, 246, 244, 248, 244, 250, 244, 252,\n",
      "         244, 254, 244, 256, 246, 248, 246, 250, 246, 252, 246, 254, 246, 256,\n",
      "         248, 250, 248, 252, 248, 254, 248, 256, 250, 252, 250, 254, 250, 256,\n",
      "         252, 254, 252, 256, 254, 256, 313, 315, 317, 319, 321, 323, 325, 327,\n",
      "         329, 331, 333, 312, 314, 313, 335, 336, 337, 338, 339, 340, 341, 342,\n",
      "         343, 344, 312, 316, 315, 335, 345, 346, 347, 348, 349, 350, 351, 352,\n",
      "         353, 312, 318, 317, 336, 345, 354, 355, 356, 357, 358, 359, 360, 361,\n",
      "         312, 320, 319, 337, 346, 354, 362, 363, 364, 365, 366, 367, 368, 312,\n",
      "         322, 321, 338, 347, 355, 362, 369, 370, 371, 372, 373, 374, 312, 324,\n",
      "         323, 339, 348, 356, 363, 369, 375, 376, 377, 378, 379, 312, 326, 325,\n",
      "         340, 349, 357, 364, 370, 375, 380, 381, 382, 383, 312, 328, 327, 341,\n",
      "         350, 358, 365, 371, 376, 380, 384, 385, 386, 312, 330, 329, 342, 351,\n",
      "         359, 366, 372, 377, 381, 384, 387, 388, 312, 332, 331, 343, 352, 360,\n",
      "         367, 373, 378, 382, 385, 387, 389, 312, 334, 333, 344, 353, 361, 368,\n",
      "         374, 379, 383, 386, 388, 389, 314, 316, 314, 318, 314, 320, 314, 322,\n",
      "         314, 324, 314, 326, 314, 328, 314, 330, 314, 332, 314, 334, 316, 318,\n",
      "         316, 320, 316, 322, 316, 324, 316, 326, 316, 328, 316, 330, 316, 332,\n",
      "         316, 334, 318, 320, 318, 322, 318, 324, 318, 326, 318, 328, 318, 330,\n",
      "         318, 332, 318, 334, 320, 322, 320, 324, 320, 326, 320, 328, 320, 330,\n",
      "         320, 332, 320, 334, 322, 324, 322, 326, 322, 328, 322, 330, 322, 332,\n",
      "         322, 334, 324, 326, 324, 328, 324, 330, 324, 332, 324, 334, 326, 328,\n",
      "         326, 330, 326, 332, 326, 334, 328, 330, 328, 332, 328, 334, 330, 332,\n",
      "         330, 334, 332, 334, 391, 393, 395, 397, 399, 401, 403, 405, 407, 409,\n",
      "         411, 390, 392, 391, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "         390, 394, 393, 413, 423, 424, 425, 426, 427, 428, 429, 430, 431, 390,\n",
      "         396, 395, 414, 423, 432, 433, 434, 435, 436, 437, 438, 439, 390, 398,\n",
      "         397, 415, 424, 432, 440, 441, 442, 443, 444, 445, 446, 390, 400, 399,\n",
      "         416, 425, 433, 440, 447, 448, 449, 450, 451, 452, 390, 402, 401, 417,\n",
      "         426, 434, 441, 447, 453, 454, 455, 456, 457, 390, 404, 403, 418, 427,\n",
      "         435, 442, 448, 453, 458, 459, 460, 461, 390, 406, 405, 419, 428, 436,\n",
      "         443, 449, 454, 458, 462, 463, 464, 390, 408, 407, 420, 429, 437, 444,\n",
      "         450, 455, 459, 462, 465, 466, 390, 410, 409, 421, 430, 438, 445, 451,\n",
      "         456, 460, 463, 465, 467, 390, 412, 411, 422, 431, 439, 446, 452, 457,\n",
      "         461, 464, 466, 467, 392, 394, 392, 396, 392, 398, 392, 400, 392, 402,\n",
      "         392, 404, 392, 406, 392, 408, 392, 410, 392, 412, 394, 396, 394, 398,\n",
      "         394, 400, 394, 402, 394, 404, 394, 406, 394, 408, 394, 410, 394, 412,\n",
      "         396, 398, 396, 400, 396, 402, 396, 404, 396, 406, 396, 408, 396, 410,\n",
      "         396, 412, 398, 400, 398, 402, 398, 404, 398, 406, 398, 408, 398, 410,\n",
      "         398, 412, 400, 402, 400, 404, 400, 406, 400, 408, 400, 410, 400, 412,\n",
      "         402, 404, 402, 406, 402, 408, 402, 410, 402, 412, 404, 406, 404, 408,\n",
      "         404, 410, 404, 412, 406, 408, 406, 410, 406, 412, 408, 410, 408, 412,\n",
      "         410, 412, 469, 471, 473, 475, 477, 479, 481, 483, 485, 487, 489, 468,\n",
      "         470, 469, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 468, 472,\n",
      "         471, 491, 501, 502, 503, 504, 505, 506, 507, 508, 509, 468, 474, 473,\n",
      "         492, 501, 510, 511, 512, 513, 514, 515, 516, 517, 468, 476, 475, 493,\n",
      "         502, 510, 518, 519, 520, 521, 522, 523, 524, 468, 478, 477, 494, 503,\n",
      "         511, 518, 525, 526, 527, 528, 529, 530, 468, 480, 479, 495, 504, 512,\n",
      "         519, 525, 531, 532, 533, 534, 535, 468, 482, 481, 496, 505, 513, 520,\n",
      "         526, 531, 536, 537, 538, 539, 468, 484, 483, 497, 506, 514, 521, 527,\n",
      "         532, 536, 540, 541, 542, 468, 486, 485, 498, 507, 515, 522, 528, 533,\n",
      "         537, 540, 543, 544, 468, 488, 487, 499, 508, 516, 523, 529, 534, 538,\n",
      "         541, 543, 545, 468, 490, 489, 500, 509, 517, 524, 530, 535, 539, 542,\n",
      "         544, 545, 470, 472, 470, 474, 470, 476, 470, 478, 470, 480, 470, 482,\n",
      "         470, 484, 470, 486, 470, 488, 470, 490, 472, 474, 472, 476, 472, 478,\n",
      "         472, 480, 472, 482, 472, 484, 472, 486, 472, 488, 472, 490, 474, 476,\n",
      "         474, 478, 474, 480, 474, 482, 474, 484, 474, 486, 474, 488, 474, 490,\n",
      "         476, 478, 476, 480, 476, 482, 476, 484, 476, 486, 476, 488, 476, 490,\n",
      "         478, 480, 478, 482, 478, 484, 478, 486, 478, 488, 478, 490, 480, 482,\n",
      "         480, 484, 480, 486, 480, 488, 480, 490, 482, 484, 482, 486, 482, 488,\n",
      "         482, 490, 484, 486, 484, 488, 484, 490, 486, 488, 486, 490, 488, 490,\n",
      "         547, 549, 551, 553, 555, 557, 559, 561, 563, 565, 567, 546, 548, 547,\n",
      "         569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 546, 550, 549, 569,\n",
      "         579, 580, 581, 582, 583, 584, 585, 586, 587, 546, 552, 551, 570, 579,\n",
      "         588, 589, 590, 591, 592, 593, 594, 595, 546, 554, 553, 571, 580, 588,\n",
      "         596, 597, 598, 599, 600, 601, 602, 546, 556, 555, 572, 581, 589, 596,\n",
      "         603, 604, 605, 606, 607, 608, 546, 558, 557, 573, 582, 590, 597, 603,\n",
      "         609, 610, 611, 612, 613, 546, 560, 559, 574, 583, 591, 598, 604, 609,\n",
      "         614, 615, 616, 617, 546, 562, 561, 575, 584, 592, 599, 605, 610, 614,\n",
      "         618, 619, 620, 546, 564, 563, 576, 585, 593, 600, 606, 611, 615, 618,\n",
      "         621, 622, 546, 566, 565, 577, 586, 594, 601, 607, 612, 616, 619, 621,\n",
      "         623, 546, 568, 567, 578, 587, 595, 602, 608, 613, 617, 620, 622, 623,\n",
      "         548, 550, 548, 552, 548, 554, 548, 556, 548, 558, 548, 560, 548, 562,\n",
      "         548, 564, 548, 566, 548, 568, 550, 552, 550, 554, 550, 556, 550, 558,\n",
      "         550, 560, 550, 562, 550, 564, 550, 566, 550, 568, 552, 554, 552, 556,\n",
      "         552, 558, 552, 560, 552, 562, 552, 564, 552, 566, 552, 568, 554, 556,\n",
      "         554, 558, 554, 560, 554, 562, 554, 564, 554, 566, 554, 568, 556, 558,\n",
      "         556, 560, 556, 562, 556, 564, 556, 566, 556, 568, 558, 560, 558, 562,\n",
      "         558, 564, 558, 566, 558, 568, 560, 562, 560, 564, 560, 566, 560, 568,\n",
      "         562, 564, 562, 566, 562, 568, 564, 566, 564, 568, 566, 568, 625, 627,\n",
      "         629, 631, 633, 635, 637, 639, 641, 643, 645, 624, 626, 625, 647, 648,\n",
      "         649, 650, 651, 652, 653, 654, 655, 656, 624, 628, 627, 647, 657, 658,\n",
      "         659, 660, 661, 662, 663, 664, 665, 624, 630, 629, 648, 657, 666, 667,\n",
      "         668, 669, 670, 671, 672, 673, 624, 632, 631, 649, 658, 666, 674, 675,\n",
      "         676, 677, 678, 679, 680, 624, 634, 633, 650, 659, 667, 674, 681, 682,\n",
      "         683, 684, 685, 686, 624, 636, 635, 651, 660, 668, 675, 681, 687, 688,\n",
      "         689, 690, 691, 624, 638, 637, 652, 661, 669, 676, 682, 687, 692, 693,\n",
      "         694, 695, 624, 640, 639, 653, 662, 670, 677, 683, 688, 692, 696, 697,\n",
      "         698, 624, 642, 641, 654, 663, 671, 678, 684, 689, 693, 696, 699, 700,\n",
      "         624, 644, 643, 655, 664, 672, 679, 685, 690, 694, 697, 699, 701, 624,\n",
      "         646, 645, 656, 665, 673, 680, 686, 691, 695, 698, 700, 701, 626, 628,\n",
      "         626, 630, 626, 632, 626, 634, 626, 636, 626, 638, 626, 640, 626, 642,\n",
      "         626, 644, 626, 646, 628, 630, 628, 632, 628, 634, 628, 636, 628, 638,\n",
      "         628, 640, 628, 642, 628, 644, 628, 646, 630, 632, 630, 634, 630, 636,\n",
      "         630, 638, 630, 640, 630, 642, 630, 644, 630, 646, 632, 634, 632, 636,\n",
      "         632, 638, 632, 640, 632, 642, 632, 644, 632, 646, 634, 636, 634, 638,\n",
      "         634, 640, 634, 642, 634, 644, 634, 646, 636, 638, 636, 640, 636, 642,\n",
      "         636, 644, 636, 646, 638, 640, 638, 642, 638, 644, 638, 646, 640, 642,\n",
      "         640, 644, 640, 646, 642, 644, 642, 646, 644, 646, 703, 705, 707, 709,\n",
      "         711, 713, 715, 717, 719, 721, 723, 702, 704, 703, 725, 726, 727, 728,\n",
      "         729, 730, 731, 732, 733, 734, 702, 706, 705, 725, 735, 736, 737, 738,\n",
      "         739, 740, 741, 742, 743, 702, 708, 707, 726, 735, 744, 745, 746, 747,\n",
      "         748, 749, 750, 751, 702, 710, 709, 727, 736, 744, 752, 753, 754, 755,\n",
      "         756, 757, 758, 702, 712, 711, 728, 737, 745, 752, 759, 760, 761, 762,\n",
      "         763, 764, 702, 714, 713, 729, 738, 746, 753, 759, 765, 766, 767, 768,\n",
      "         769, 702, 716, 715, 730, 739, 747, 754, 760, 765, 770, 771, 772, 773,\n",
      "         702, 718, 717, 731, 740, 748, 755, 761, 766, 770, 774, 775, 776, 702,\n",
      "         720, 719, 732, 741, 749, 756, 762, 767, 771, 774, 777, 778, 702, 722,\n",
      "         721, 733, 742, 750, 757, 763, 768, 772, 775, 777, 779, 702, 724, 723,\n",
      "         734, 743, 751, 758, 764, 769, 773, 776, 778, 779, 704, 706, 704, 708,\n",
      "         704, 710, 704, 712, 704, 714, 704, 716, 704, 718, 704, 720, 704, 722,\n",
      "         704, 724, 706, 708, 706, 710, 706, 712, 706, 714, 706, 716, 706, 718,\n",
      "         706, 720, 706, 722, 706, 724, 708, 710, 708, 712, 708, 714, 708, 716,\n",
      "         708, 718, 708, 720, 708, 722, 708, 724, 710, 712, 710, 714, 710, 716,\n",
      "         710, 718, 710, 720, 710, 722, 710, 724, 712, 714, 712, 716, 712, 718,\n",
      "         712, 720, 712, 722, 712, 724, 714, 716, 714, 718, 714, 720, 714, 722,\n",
      "         714, 724, 716, 718, 716, 720, 716, 722, 716, 724, 718, 720, 718, 722,\n",
      "         718, 724, 720, 722, 720, 724, 722, 724]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "# TODO dataloader\n",
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "for step, data in enumerate(train_loader):\n",
    "  print(f'Step {step + 1}:')\n",
    "  print('=======')\n",
    "  print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "  print(data)\n",
    "  print()\n",
    "  print(data.x.flatten(), data.edge_index, data.batch)\n",
    "  break\n",
    "\n",
    "\n",
    "# for gen of training sample:\n",
    "# 1) mask 0 ... (all -1) (inclusivly) \n",
    "# 2) swich exactly 1 of the unmasked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS8w-Xw0ynzU"
   },
   "source": [
    "### Planar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlDyM555ypd8",
    "outputId": "5e2d2606-b605-4dbc-9fe0-f53679663f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: train 128, val 32, test 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.ShuffleList at 0x2b54a2220>, <__main__.ShuffleList at 0x2b54a2d60>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import random_split, Dataset\n",
    "import torch_geometric.utils\n",
    "\n",
    "def load_dataset_planar():\n",
    "  nx_set = list()\n",
    "  num_nodes = 64\n",
    "  graph_dataset = list() \n",
    "  if not os.path.exists(\"SPECTRE\"):\n",
    "    os.system(\"git clone https://github.com/KarolisMart/SPECTRE.git\")\n",
    "  adjs, eigvals, eigvecs, n_nodes, max_eigval, min_eigval, same_sample, n_max = torch.load('SPECTRE/data/planar_64_200.pt')\n",
    "  for adj in adjs:\n",
    "    # from adj matrix to nx\n",
    "    graph = nx.from_numpy_array(adj.numpy())\n",
    "    # from nx to pyg\n",
    "    graph.x = torch.ones(num_nodes).t()\n",
    "    graph_dataset.append(lift_nx_to_pyg(graph))\n",
    "\n",
    "  #graph_dataset_train = graph_dataset[:int(len(graph_dataset)*0.8)]\n",
    "  #graph_dataset_test = graph_dataset[int(len(graph_dataset)*0.8):]\n",
    "  #return ShuffleList(graph_dataset_train), ShuffleList(graph_dataset_test)\n",
    "\n",
    "  test_len = int(round(len(graph_dataset) * 0.2))\n",
    "  train_len = int(round((len(graph_dataset) - test_len) * 0.8))\n",
    "  val_len = len(graph_dataset) - train_len - test_len\n",
    "  print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
    "  splits = random_split(graph_dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))\n",
    "  datasets = {'train': splits[0], 'val': splits[1], 'test': splits[2]}\n",
    "  return ShuffleList(list(datasets['train'])), ShuffleList(list(datasets['test']))\n",
    "\n",
    "\n",
    "load_dataset_planar()  # get baseline by comparing 128 with 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvjLUJSMUb_E",
    "outputId": "eca6abe4-e963-4351-f674-d455b270c2de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: train 128, val 32, test 40\n"
     ]
    }
   ],
   "source": [
    "graph_dataset, _ = load_dataset_planar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OIcsvOpU9AC",
    "outputId": "b342f4ab-7be5-4074-8bf1-8aad297761e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(edge_index=[2, 8064], x=[2080, 1]),\n",
       " Data(edge_index=[2, 8064], x=[2080, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dataset[0],graph_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "obBv2OMEWaKA"
   },
   "outputs": [],
   "source": [
    "#draw_pyg(graph_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78c_wYnEiPAx"
   },
   "source": [
    "### Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2Wp0Za7iSzn",
    "outputId": "bab4242a-1963-4de7-a4b6-b6ff07858958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes  172\n",
      "number of nodes  92\n",
      "number of nodes  61\n",
      "number of nodes  64\n",
      "number of nodes  115\n",
      "number of nodes  101\n",
      "number of nodes  90\n",
      "number of nodes  123\n",
      "number of nodes  168\n",
      "number of nodes  61\n",
      "number of nodes  130\n",
      "number of nodes  62\n",
      "number of nodes  163\n",
      "number of nodes  58\n",
      "number of nodes  71\n",
      "number of nodes  156\n",
      "number of nodes  77\n",
      "number of nodes  169\n",
      "number of nodes  141\n",
      "number of nodes  103\n",
      "number of nodes  89\n",
      "number of nodes  81\n",
      "number of nodes  88\n",
      "number of nodes  164\n",
      "number of nodes  88\n",
      "number of nodes  117\n",
      "number of nodes  128\n",
      "number of nodes  69\n",
      "number of nodes  64\n",
      "number of nodes  58\n",
      "number of nodes  152\n",
      "number of nodes  97\n",
      "number of nodes  166\n",
      "number of nodes  101\n",
      "number of nodes  106\n",
      "number of nodes  57\n",
      "number of nodes  76\n",
      "number of nodes  115\n",
      "number of nodes  52\n",
      "number of nodes  112\n",
      "number of nodes  64\n",
      "number of nodes  63\n",
      "number of nodes  169\n",
      "number of nodes  54\n",
      "number of nodes  90\n",
      "number of nodes  129\n",
      "number of nodes  105\n",
      "number of nodes  53\n",
      "number of nodes  53\n",
      "number of nodes  55\n",
      "number of nodes  53\n",
      "number of nodes  88\n",
      "number of nodes  65\n",
      "number of nodes  127\n",
      "number of nodes  77\n",
      "number of nodes  94\n",
      "number of nodes  57\n",
      "number of nodes  139\n",
      "number of nodes  56\n",
      "number of nodes  148\n",
      "number of nodes  79\n",
      "number of nodes  58\n",
      "number of nodes  94\n",
      "number of nodes  112\n",
      "number of nodes  99\n",
      "number of nodes  89\n",
      "number of nodes  75\n",
      "number of nodes  137\n",
      "number of nodes  63\n",
      "number of nodes  115\n",
      "number of nodes  159\n",
      "number of nodes  74\n",
      "number of nodes  72\n",
      "number of nodes  129\n",
      "number of nodes  124\n",
      "number of nodes  72\n",
      "number of nodes  99\n",
      "number of nodes  166\n",
      "number of nodes  164\n",
      "number of nodes  160\n",
      "number of nodes  80\n",
      "number of nodes  61\n",
      "number of nodes  138\n",
      "number of nodes  78\n",
      "number of nodes  83\n",
      "number of nodes  167\n",
      "number of nodes  147\n",
      "number of nodes  70\n",
      "number of nodes  99\n",
      "number of nodes  91\n",
      "number of nodes  60\n",
      "number of nodes  135\n",
      "number of nodes  73\n",
      "number of nodes  139\n",
      "number of nodes  97\n",
      "number of nodes  84\n",
      "number of nodes  113\n",
      "number of nodes  73\n",
      "number of nodes  157\n",
      "number of nodes  102\n",
      "number of nodes  123\n",
      "number of nodes  125\n",
      "number of nodes  127\n",
      "number of nodes  79\n",
      "number of nodes  106\n",
      "number of nodes  81\n",
      "number of nodes  122\n",
      "number of nodes  71\n",
      "number of nodes  120\n",
      "number of nodes  76\n",
      "number of nodes  85\n",
      "number of nodes  98\n",
      "number of nodes  55\n",
      "number of nodes  142\n",
      "number of nodes  168\n",
      "number of nodes  161\n",
      "number of nodes  52\n",
      "number of nodes  92\n",
      "number of nodes  163\n",
      "number of nodes  187\n",
      "number of nodes  90\n",
      "number of nodes  144\n",
      "number of nodes  59\n",
      "number of nodes  157\n",
      "number of nodes  129\n",
      "number of nodes  44\n",
      "number of nodes  163\n",
      "number of nodes  162\n",
      "number of nodes  64\n",
      "number of nodes  157\n",
      "number of nodes  93\n",
      "number of nodes  143\n",
      "number of nodes  61\n",
      "number of nodes  80\n",
      "number of nodes  91\n",
      "number of nodes  91\n",
      "number of nodes  127\n",
      "number of nodes  87\n",
      "number of nodes  82\n",
      "number of nodes  102\n",
      "number of nodes  67\n",
      "number of nodes  164\n",
      "number of nodes  97\n",
      "number of nodes  92\n",
      "number of nodes  140\n",
      "number of nodes  104\n",
      "number of nodes  165\n",
      "number of nodes  62\n",
      "number of nodes  69\n",
      "number of nodes  156\n",
      "number of nodes  61\n",
      "number of nodes  85\n",
      "number of nodes  55\n",
      "number of nodes  59\n",
      "number of nodes  144\n",
      "number of nodes  132\n",
      "number of nodes  141\n",
      "number of nodes  56\n",
      "number of nodes  169\n",
      "number of nodes  125\n",
      "number of nodes  158\n",
      "number of nodes  58\n",
      "number of nodes  61\n",
      "number of nodes  141\n",
      "number of nodes  60\n",
      "number of nodes  102\n",
      "number of nodes  134\n",
      "number of nodes  69\n",
      "number of nodes  94\n",
      "number of nodes  174\n",
      "number of nodes  64\n",
      "number of nodes  116\n",
      "number of nodes  141\n",
      "number of nodes  63\n",
      "number of nodes  75\n",
      "number of nodes  149\n",
      "number of nodes  152\n",
      "number of nodes  110\n",
      "number of nodes  107\n",
      "number of nodes  100\n",
      "number of nodes  84\n",
      "number of nodes  116\n",
      "number of nodes  105\n",
      "number of nodes  109\n",
      "number of nodes  141\n",
      "number of nodes  64\n",
      "number of nodes  67\n",
      "number of nodes  125\n",
      "number of nodes  69\n",
      "number of nodes  103\n",
      "number of nodes  174\n",
      "number of nodes  139\n",
      "number of nodes  180\n",
      "number of nodes  49\n",
      "number of nodes  55\n",
      "number of nodes  119\n",
      "number of nodes  76\n",
      "number of nodes  139\n",
      "number of nodes  85\n",
      "number of nodes  90\n",
      "Dataset sizes: train 128, val 32, test 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<__main__.ShuffleList at 0x2b2528d90>, <__main__.ShuffleList at 0x2b464e5b0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset_block():\n",
    "  nx_set = list()\n",
    "  num_nodes = 64\n",
    "  graph_dataset = list() \n",
    "  if not os.path.exists(\"SPECTRE\"):\n",
    "    os.system(\"git clone https://github.com/KarolisMart/SPECTRE.git\")\n",
    "  adjs, eigvals, eigvecs, n_nodes, max_eigval, min_eigval, same_sample, n_max = torch.load('SPECTRE/data/sbm_200.pt')\n",
    "  for adj in adjs:\n",
    "    # from adj matrix to nx\n",
    "    graph = nx.from_numpy_array(adj.numpy())\n",
    "    print('number of nodes ', graph.number_of_nodes())\n",
    "    # from nx to pyg\n",
    "    graph.x = torch.ones(num_nodes).t()\n",
    "    graph_dataset.append(lift_nx_to_pyg(graph))\n",
    "\n",
    "  #graph_dataset_train = graph_dataset[:int(len(graph_dataset)*0.8)]\n",
    "  #graph_dataset_test = graph_dataset[int(len(graph_dataset)*0.8):]\n",
    "  #return ShuffleList(graph_dataset_train), ShuffleList(graph_dataset_test)\n",
    "\n",
    "  test_len = int(round(len(graph_dataset) * 0.2))\n",
    "  train_len = int(round((len(graph_dataset) - test_len) * 0.8))\n",
    "  val_len = len(graph_dataset) - train_len - test_len\n",
    "  print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
    "  splits = random_split(graph_dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))\n",
    "  datasets = {'train': splits[0], 'val': splits[1], 'test': splits[2]}\n",
    "  return ShuffleList(list(datasets['train'])), ShuffleList(list(datasets['test']))\n",
    "\n",
    "\n",
    "load_dataset_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlPP_ASmqHrQ",
    "outputId": "53905ab3-d656-45c3-da51-e3361fc27e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes  172\n",
      "number of nodes  92\n",
      "number of nodes  61\n",
      "number of nodes  64\n",
      "number of nodes  115\n",
      "number of nodes  101\n",
      "number of nodes  90\n",
      "number of nodes  123\n",
      "number of nodes  168\n",
      "number of nodes  61\n",
      "number of nodes  130\n",
      "number of nodes  62\n",
      "number of nodes  163\n",
      "number of nodes  58\n",
      "number of nodes  71\n",
      "number of nodes  156\n",
      "number of nodes  77\n",
      "number of nodes  169\n",
      "number of nodes  141\n",
      "number of nodes  103\n",
      "number of nodes  89\n",
      "number of nodes  81\n",
      "number of nodes  88\n",
      "number of nodes  164\n",
      "number of nodes  88\n",
      "number of nodes  117\n",
      "number of nodes  128\n",
      "number of nodes  69\n",
      "number of nodes  64\n",
      "number of nodes  58\n",
      "number of nodes  152\n",
      "number of nodes  97\n",
      "number of nodes  166\n",
      "number of nodes  101\n",
      "number of nodes  106\n",
      "number of nodes  57\n",
      "number of nodes  76\n",
      "number of nodes  115\n",
      "number of nodes  52\n",
      "number of nodes  112\n",
      "number of nodes  64\n",
      "number of nodes  63\n",
      "number of nodes  169\n",
      "number of nodes  54\n",
      "number of nodes  90\n",
      "number of nodes  129\n",
      "number of nodes  105\n",
      "number of nodes  53\n",
      "number of nodes  53\n",
      "number of nodes  55\n",
      "number of nodes  53\n",
      "number of nodes  88\n",
      "number of nodes  65\n",
      "number of nodes  127\n",
      "number of nodes  77\n",
      "number of nodes  94\n",
      "number of nodes  57\n",
      "number of nodes  139\n",
      "number of nodes  56\n",
      "number of nodes  148\n",
      "number of nodes  79\n",
      "number of nodes  58\n",
      "number of nodes  94\n",
      "number of nodes  112\n",
      "number of nodes  99\n",
      "number of nodes  89\n",
      "number of nodes  75\n",
      "number of nodes  137\n",
      "number of nodes  63\n",
      "number of nodes  115\n",
      "number of nodes  159\n",
      "number of nodes  74\n",
      "number of nodes  72\n",
      "number of nodes  129\n",
      "number of nodes  124\n",
      "number of nodes  72\n",
      "number of nodes  99\n",
      "number of nodes  166\n",
      "number of nodes  164\n",
      "number of nodes  160\n",
      "number of nodes  80\n",
      "number of nodes  61\n",
      "number of nodes  138\n",
      "number of nodes  78\n",
      "number of nodes  83\n",
      "number of nodes  167\n",
      "number of nodes  147\n",
      "number of nodes  70\n",
      "number of nodes  99\n",
      "number of nodes  91\n",
      "number of nodes  60\n",
      "number of nodes  135\n",
      "number of nodes  73\n",
      "number of nodes  139\n",
      "number of nodes  97\n",
      "number of nodes  84\n",
      "number of nodes  113\n",
      "number of nodes  73\n",
      "number of nodes  157\n",
      "number of nodes  102\n",
      "number of nodes  123\n",
      "number of nodes  125\n",
      "number of nodes  127\n",
      "number of nodes  79\n",
      "number of nodes  106\n",
      "number of nodes  81\n",
      "number of nodes  122\n",
      "number of nodes  71\n",
      "number of nodes  120\n",
      "number of nodes  76\n",
      "number of nodes  85\n",
      "number of nodes  98\n",
      "number of nodes  55\n",
      "number of nodes  142\n",
      "number of nodes  168\n",
      "number of nodes  161\n",
      "number of nodes  52\n",
      "number of nodes  92\n",
      "number of nodes  163\n",
      "number of nodes  187\n",
      "number of nodes  90\n",
      "number of nodes  144\n",
      "number of nodes  59\n",
      "number of nodes  157\n",
      "number of nodes  129\n",
      "number of nodes  44\n",
      "number of nodes  163\n",
      "number of nodes  162\n",
      "number of nodes  64\n",
      "number of nodes  157\n",
      "number of nodes  93\n",
      "number of nodes  143\n",
      "number of nodes  61\n",
      "number of nodes  80\n",
      "number of nodes  91\n",
      "number of nodes  91\n",
      "number of nodes  127\n",
      "number of nodes  87\n",
      "number of nodes  82\n",
      "number of nodes  102\n",
      "number of nodes  67\n",
      "number of nodes  164\n",
      "number of nodes  97\n",
      "number of nodes  92\n",
      "number of nodes  140\n",
      "number of nodes  104\n",
      "number of nodes  165\n",
      "number of nodes  62\n",
      "number of nodes  69\n",
      "number of nodes  156\n",
      "number of nodes  61\n",
      "number of nodes  85\n",
      "number of nodes  55\n",
      "number of nodes  59\n",
      "number of nodes  144\n",
      "number of nodes  132\n",
      "number of nodes  141\n",
      "number of nodes  56\n",
      "number of nodes  169\n",
      "number of nodes  125\n",
      "number of nodes  158\n",
      "number of nodes  58\n",
      "number of nodes  61\n",
      "number of nodes  141\n",
      "number of nodes  60\n",
      "number of nodes  102\n",
      "number of nodes  134\n",
      "number of nodes  69\n",
      "number of nodes  94\n",
      "number of nodes  174\n",
      "number of nodes  64\n",
      "number of nodes  116\n",
      "number of nodes  141\n",
      "number of nodes  63\n",
      "number of nodes  75\n",
      "number of nodes  149\n",
      "number of nodes  152\n",
      "number of nodes  110\n",
      "number of nodes  107\n",
      "number of nodes  100\n",
      "number of nodes  84\n",
      "number of nodes  116\n",
      "number of nodes  105\n",
      "number of nodes  109\n",
      "number of nodes  141\n",
      "number of nodes  64\n",
      "number of nodes  67\n",
      "number of nodes  125\n",
      "number of nodes  69\n",
      "number of nodes  103\n",
      "number of nodes  174\n",
      "number of nodes  139\n",
      "number of nodes  180\n",
      "number of nodes  49\n",
      "number of nodes  55\n",
      "number of nodes  119\n",
      "number of nodes  76\n",
      "number of nodes  139\n",
      "number of nodes  85\n",
      "number of nodes  90\n",
      "Dataset sizes: train 128, val 32, test 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ShuffleList at 0x2b2883b80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = load_dataset_block()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97PuI5WRyX9n",
    "outputId": "c5e4ea88-eba3-471c-e1ad-a1153961c641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 44104], x=[11175, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL2Kr3jD-WwW"
   },
   "source": [
    "## Loss & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4bFAJnqX-ZO_"
   },
   "outputs": [],
   "source": [
    "import pyemd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PRINT_TIME = False\n",
    "\n",
    "def degree_worker(G):\n",
    "    return np.array(nx.degree_histogram(G))\n",
    "\n",
    "def degree_stats(graph_ref_list, graph_pred_list, is_parallel=True, compute_emd=False):\n",
    "    ''' Compute the distance between the degree distributions of two unordered sets of graphs.\n",
    "        Args:\n",
    "            graph_ref_list, graph_target_list: two lists of networkx graphs to be evaluated\n",
    "        '''\n",
    "    sample_ref = []\n",
    "    sample_pred = []\n",
    "    # in case an empty graph is generated\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    prev = datetime.now()\n",
    "    if is_parallel:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_ref_list):\n",
    "                sample_ref.append(deg_hist)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_pred_list_remove_empty):\n",
    "                sample_pred.append(deg_hist)\n",
    "    else:\n",
    "        for i in range(len(graph_ref_list)):\n",
    "            degree_temp = np.array(nx.degree_histogram(graph_ref_list[i]))\n",
    "            sample_ref.append(degree_temp)\n",
    "        for i in range(len(graph_pred_list_remove_empty)):\n",
    "            degree_temp = np.array(\n",
    "                nx.degree_histogram(graph_pred_list_remove_empty[i]))\n",
    "            sample_pred.append(degree_temp)\n",
    "\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
    "    if compute_emd:\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv)\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian)\n",
    "\n",
    "    elapsed = datetime.now() - prev\n",
    "    if PRINT_TIME:\n",
    "        print('Time computing degree mmd: ', elapsed)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "def clustering_worker(param):\n",
    "    G, bins = param\n",
    "    clustering_coeffs_list = list(nx.clustering(G).values())\n",
    "    hist, _ = np.histogram(\n",
    "        clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def clustering_stats(graph_ref_list,\n",
    "                     graph_pred_list,\n",
    "                     bins=100,\n",
    "                     is_parallel=True, compute_emd=False):\n",
    "    sample_ref = []\n",
    "    sample_pred = []\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    prev = datetime.now()\n",
    "    if is_parallel:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for clustering_hist in executor.map(clustering_worker,\n",
    "                                                [(G, bins) for G in graph_ref_list]):\n",
    "                sample_ref.append(clustering_hist)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for clustering_hist in executor.map(\n",
    "                    clustering_worker, [(G, bins) for G in graph_pred_list_remove_empty]):\n",
    "                sample_pred.append(clustering_hist)\n",
    "\n",
    "        # check non-zero elements in hist\n",
    "        # total = 0\n",
    "        # for i in range(len(sample_pred)):\n",
    "        #    nz = np.nonzero(sample_pred[i])[0].shape[0]\n",
    "        #    total += nz\n",
    "        # print(total)\n",
    "    else:\n",
    "        for i in range(len(graph_ref_list)):\n",
    "            clustering_coeffs_list = list(nx.clustering(graph_ref_list[i]).values())\n",
    "            hist, _ = np.histogram(\n",
    "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "            sample_ref.append(hist)\n",
    "\n",
    "        for i in range(len(graph_pred_list_remove_empty)):\n",
    "            clustering_coeffs_list = list(\n",
    "                nx.clustering(graph_pred_list_remove_empty[i]).values())\n",
    "            hist, _ = np.histogram(\n",
    "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "            sample_pred.append(hist)\n",
    "\n",
    "    if compute_emd:\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd, sigma=1.0 / 10)\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd, sigma=1.0 / 10, distance_scaling=bins)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv, sigma=1.0 / 10)\n",
    "\n",
    "    elapsed = datetime.now() - prev\n",
    "    if PRINT_TIME:\n",
    "        print('Time computing clustering mmd: ', elapsed)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "# maps motif/orbit name string to its corresponding list of indices from orca output\n",
    "motif_to_indices = {\n",
    "    '3path': [1, 2],\n",
    "    '4cycle': [8],\n",
    "}\n",
    "COUNT_START_STR = 'orbit counts:'\n",
    "\n",
    "\n",
    "def edge_list_reindexed(G):\n",
    "    idx = 0\n",
    "    id2idx = dict()\n",
    "    for u in G.nodes():\n",
    "        id2idx[str(u)] = idx\n",
    "        idx += 1\n",
    "\n",
    "    edges = []\n",
    "    for (u, v) in G.edges():\n",
    "        edges.append((id2idx[str(u)], id2idx[str(v)]))\n",
    "    return edges\n",
    "\n",
    "def orca(graph):\n",
    "    # tmp_fname = f'analysis/orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
    "    tmp_fname = f'orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
    "    tmp_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)), tmp_fname)\n",
    "    # print(tmp_fname, flush=True)\n",
    "    f = open(tmp_fname, 'w')\n",
    "    f.write(\n",
    "        str(graph.number_of_nodes()) + ' ' + str(graph.number_of_edges()) + '\\n')\n",
    "    for (u, v) in edge_list_reindexed(graph):\n",
    "        f.write(str(u) + ' ' + str(v) + '\\n')\n",
    "    f.close()\n",
    "    output = sp.check_output(\n",
    "        [str(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'orca/orca')), 'node', '4', tmp_fname, 'std'])\n",
    "    output = output.decode('utf8').strip()\n",
    "    idx = output.find(COUNT_START_STR) + len(COUNT_START_STR) + 2\n",
    "    output = output[idx:]\n",
    "    node_orbit_counts = np.array([\n",
    "        list(map(int,\n",
    "                 node_cnts.strip().split(' ')))\n",
    "        for node_cnts in output.strip('\\n').split('\\n')\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        os.remove(tmp_fname)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    return node_orbit_counts\n",
    "\n",
    "\n",
    "def orbit_stats_all(graph_ref_list, graph_pred_list, compute_emd=False):\n",
    "    total_counts_ref = []\n",
    "    total_counts_pred = []\n",
    "\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    for G in graph_ref_list:\n",
    "        orbit_counts = orca(G)\n",
    "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
    "        total_counts_ref.append(orbit_counts_graph)\n",
    "\n",
    "    for G in graph_pred_list:\n",
    "        orbit_counts = orca(G)\n",
    "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
    "        total_counts_pred.append(orbit_counts_graph)\n",
    "\n",
    "    total_counts_ref = np.array(total_counts_ref)\n",
    "    total_counts_pred = np.array(total_counts_pred)\n",
    "\n",
    "    # mmd_dist = compute_mmd(\n",
    "    #     total_counts_ref,\n",
    "    #     total_counts_pred,\n",
    "    #     kernel=gaussian,\n",
    "    #     is_hist=False,\n",
    "    #     sigma=30.0)\n",
    "\n",
    "    # mmd_dist = compute_mmd(\n",
    "    #         total_counts_ref,\n",
    "    #         total_counts_pred,\n",
    "    #         kernel=gaussian_tv,\n",
    "    #         is_hist=False,\n",
    "    #         sigma=30.0)  \n",
    "\n",
    "    if compute_emd:\n",
    "        # mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=emd, sigma=30.0)\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian, is_hist=False, sigma=30.0)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian_tv, is_hist=False, sigma=30.0)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gaussian(x, y, sigma=1.0):\n",
    "    support_size = max(len(x), len(y))\n",
    "    # convert histogram values x and y to float, and make them equal len\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    if len(x) < len(y):\n",
    "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
    "    elif len(y) < len(x):\n",
    "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
    "\n",
    "    dist = np.linalg.norm(x - y, 2)\n",
    "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
    "\n",
    "\n",
    "def gaussian_tv(x, y, sigma=1.0):  \n",
    "    support_size = max(len(x), len(y))\n",
    "    # convert histogram values x and y to float, and make them equal len\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    if len(x) < len(y):\n",
    "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
    "    elif len(y) < len(x):\n",
    "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
    "\n",
    "    dist = np.abs(x - y).sum() / 2.0\n",
    "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
    "\n",
    "\n",
    "def kernel_parallel_unpacked(x, samples2, kernel):\n",
    "    d = 0\n",
    "    for s2 in samples2:\n",
    "        d += kernel(x, s2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def kernel_parallel_worker(t):\n",
    "    return kernel_parallel_unpacked(*t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def disc(samples1, samples2, kernel, is_parallel=True, *args, **kwargs):\n",
    "    ''' Discrepancy between 2 samples '''\n",
    "    d = 0\n",
    "\n",
    "    if not is_parallel:\n",
    "        for s1 in samples1:\n",
    "            for s2 in samples2:\n",
    "                d += kernel(s1, s2, *args, **kwargs)\n",
    "    else:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for dist in executor.map(kernel_parallel_worker, [\n",
    "                    (s1, samples2, partial(kernel, *args, **kwargs)) for s1 in samples1\n",
    "            ]):\n",
    "                d += dist\n",
    "    if len(samples1) * len(samples2) > 0:\n",
    "        d /= len(samples1) * len(samples2)\n",
    "    else:\n",
    "        d = 1e+6\n",
    "    #print('error')\n",
    "    return d\n",
    "\n",
    "\n",
    "def compute_mmd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' MMD between two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [s1 / (np.sum(s1) + 1e-6) for s1 in samples1]\n",
    "        samples2 = [s2 / (np.sum(s2) + 1e-6) for s2 in samples2]\n",
    "    #print(samples1)\n",
    "    return disc(samples1, samples1, kernel, *args, **kwargs) + disc(samples2, samples2, kernel, *args, **kwargs) - \\\n",
    "                2 * disc(samples1, samples2, kernel, *args, **kwargs)\n",
    "\n",
    "\n",
    "def compute_emd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' EMD between average of two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [np.mean(samples1)]\n",
    "        samples2 = [np.mean(samples2)]\n",
    "    return disc(samples1, samples2, kernel, *args,\n",
    "                            **kwargs), [samples1[0], samples2[0]]\n",
    "\n",
    "\n",
    "\n",
    "def compute_mmd_stats(graphs_ref_nx, graphs_pred_nx):\n",
    "  if 'networkx' not in str(type(graphs_ref_nx[0])):\n",
    "    graphs_ref_nx = [pyg_graph_to_nx(g) for g in graphs_ref_nx]\n",
    "  if 'networkx' not in str(type(graphs_pred_nx[0])):\n",
    "    graphs_pred_nx = [pyg_graph_to_nx(g) for g in graphs_pred_nx]\n",
    "  return degree_stats(graphs_ref_nx, graphs_pred_nx), clustering_stats(graphs_ref_nx, graphs_pred_nx)\n",
    "\n",
    "\n",
    "def compute_expected_optimal_loss(graph_dataset_train, graph_dataset_test):\n",
    "  graph_dataset_train_nx = [pyg_graph_to_nx(g) for g in graph_dataset_train]\n",
    "  graph_dataset_test_nx = [pyg_graph_to_nx(g) for g in graph_dataset_test]\n",
    "  return compute_mmd_stats(graph_dataset_train_nx, graph_dataset_test_nx)\n",
    "\n",
    "#def compute_expected_optimal_loss(graphs_ref_nx):\n",
    "#  graphs_ref_nx2 = build_test_set_nx(1337)\n",
    "#  return compute_mmd_stats(graphs_ref_nx, graphs_ref_nx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqztDRIj0UdF"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV_DnkKhs9P-"
   },
   "source": [
    "#### AttentionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eoLXRG6utBBU"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, GraphNorm, BatchNorm\n",
    "\n",
    "class AttentionNet(torch.nn.Module):\n",
    "  def __init__(self, hidden_dim=32, layer_num=6, dropout=0.1, normalization=False, single_pass_pooling=False):\n",
    "    super(AttentionNet, self).__init__()\n",
    "\n",
    "    self.mlp_list = nn.ModuleList()\n",
    "    self.conv_list = nn.ModuleList()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.dropout = dropout\n",
    "    self.graph_norms = nn.ModuleList()\n",
    "    self.normalization = normalization\n",
    "    self.single_pass_pooling = single_pass_pooling\n",
    "    assert(hidden_dim % 2 == 0)\n",
    "    input_channels = NUM_CLASSES\n",
    "\n",
    "    for i in range(layer_num):\n",
    "      in_dim = hidden_dim if i > 0 else input_channels \n",
    "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
    "      self.conv_list.append(GATv2Conv(in_channels=in_dim, out_channels=hidden_dim // 2, heads=2))\n",
    "      self.graph_norms.append(BatchNorm(hidden_dim))\n",
    "      self.mlp_list.append(Seq(Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, out_dim)))\n",
    "\n",
    "  def forward(self, x_in, edge_index, batch):\n",
    "    x = note_features_to_one_hot(x_in)\n",
    "    for i in range(len(self.conv_list)):\n",
    "      x = self.conv_list[i](x, edge_index)\n",
    "      x = F.relu(x)\n",
    "      if self.normalization:\n",
    "        x = self.graph_norms[i](x)\n",
    "      x = self.mlp_list[i](x)\n",
    "      #if i == 0:\n",
    "      #  x = self.dropout(x)\n",
    "\n",
    "    if self.single_pass_pooling:\n",
    "      x = global_mask_pool(x, x_in, batch)\n",
    "    else:\n",
    "      x = global_mean_pool(x, batch)\n",
    "    x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "    x = self.sigmoid(x) + EPSILON\n",
    "    return x.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTUh3uVOtEMO",
    "outputId": "65bca9cf-0b24-4c80-d5bf-e3281ee745e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionNet(\n",
      "  (mlp_list): ModuleList(\n",
      "    (0-4): 5 x Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_list): ModuleList(\n",
      "    (0): GATv2Conv(4, 16, heads=2)\n",
      "    (1-5): 5 x GATv2Conv(32, 16, heads=2)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      "  (graph_norms): ModuleList(\n",
      "    (0-5): 6 x BatchNorm(32)\n",
      "  )\n",
      ")\n",
      "tensor([0.5210, 0.5210, 0.5210, 0.5210, 0.5210, 0.5210, 0.5210, 0.5210, 0.5210,\n",
      "        0.5000], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AttentionNet(single_pass_pooling=True)\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data = data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaQ7f2E4dCqd"
   },
   "source": [
    "#### TransformerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nS3CzlgcdEAK"
   },
   "outputs": [],
   "source": [
    "#relu = nn.Relu() ## todo: changed to relu here\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class TransformerNet(torch.nn.Module):\n",
    "  def __init__(self, hidden_dim=32, layer_num=6, dropout=0.1, normalization=False, single_pass_pooling = False):\n",
    "    super(TransformerNet, self).__init__()\n",
    "\n",
    "    self.mlp_list = nn.ModuleList()\n",
    "    self.conv_list = nn.ModuleList()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.graph_norms = nn.ModuleList()\n",
    "    self.dropout = dropout\n",
    "    self.normalization = normalization\n",
    "    assert(hidden_dim % 2 == 0)\n",
    "    input_channels = NUM_CLASSES\n",
    "    self.single_pass_pooling = single_pass_pooling\n",
    "\n",
    "\n",
    "    for i in range(layer_num):\n",
    "      in_dim = hidden_dim if i > 0 else input_channels \n",
    "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
    "      self.conv_list.append(TransformerConv(in_channels=in_dim, out_channels=hidden_dim // 2, heads=2))\n",
    "      self.graph_norms.append(BatchNorm(hidden_dim))\n",
    "      self.mlp_list.append(Seq(Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, hidden_dim), nn.ReLU(), Lin(hidden_dim, out_dim)))\n",
    "\n",
    "  def forward(self, x_in, edge_index, batch):\n",
    "    x = note_features_to_one_hot(x_in)\n",
    "    for i in range(len(self.conv_list)):\n",
    "      x = self.conv_list[i](x, edge_index)\n",
    "      if self.normalization:\n",
    "        x = self.graph_norms[i](x)\n",
    "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "      x = self.mlp_list[i](x)\n",
    "    \n",
    "    if self.single_pass_pooling:\n",
    "      x = global_mask_pool(x, x_in, batch)\n",
    "    else:\n",
    "      x = global_mean_pool(x, batch)\n",
    "\n",
    "    x = self.sigmoid(x) + EPSILON\n",
    "    return x.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plkfTVUrXRO-",
    "outputId": "a55bbb21-48d3-456f-a39d-b39b9252a9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerNet(\n",
      "  (mlp_list): ModuleList(\n",
      "    (0-4): 5 x Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_list): ModuleList(\n",
      "    (0): TransformerConv(4, 16, heads=2)\n",
      "    (1-5): 5 x TransformerConv(32, 16, heads=2)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      "  (graph_norms): ModuleList(\n",
      "    (0-5): 6 x BatchNorm(32)\n",
      "  )\n",
      ")\n",
      "tensor([0.4885, 0.4883, 0.4884, 0.4885, 0.4884, 0.4884, 0.4883, 0.4883, 0.4884,\n",
      "        0.4884], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = TransformerNet()\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data = data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwghOZsDdEJz"
   },
   "source": [
    "#### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LEwIcO4w0VaR"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphUNet\n",
    "\n",
    "class Unet(torch.nn.Module):\n",
    "  def __init__(self, hidden_channels=32, out_channels=1, depth=4, single_pass_pooling = False):\n",
    "    super(Unet, self).__init__()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    in_channels = NUM_CLASSES\n",
    "    self.single_pass_pooling = single_pass_pooling\n",
    "    self.unet = GraphUNet(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, depth=depth)\n",
    "\n",
    "  def forward(self, x_in, edge_index, batch):\n",
    "    x = note_features_to_one_hot(x_in)\n",
    "    x = self.unet(x, edge_index)\n",
    "\n",
    "    if self.single_pass_pooling:\n",
    "      x = global_mask_pool(x, x_in, batch)\n",
    "    else:\n",
    "      x = global_mean_pool(x, batch)\n",
    "\n",
    "    x = self.sigmoid(x) + 0.0000001 # add eps to avoid div. by zero\n",
    "    return x.flatten()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBYSWAp9XOLz",
    "outputId": "f5ef7b55-9c1c-47f0-972d-46334d938ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (unet): GraphUNet(4, 32, 1, depth=4, pool_ratios=[0.5, 0.5, 0.5, 0.5])\n",
      ")\n",
      "tensor([0.5508, 0.5508, 0.5508, 0.5508, 0.5508, 0.5502, 0.5505, 0.5508, 0.5507,\n",
      "        0.5507], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerritgrossmann/anaconda3/envs/nextaid/lib/python3.9/site-packages/torch_sparse/matmul.py:97: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343673238/work/aten/src/ATen/SparseCsrTensorImpl.cpp:56.)\n",
      "  C = torch.sparse.mm(A, B)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Unet()\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir4sFS6YS5UY"
   },
   "source": [
    "#### PNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ydDINWbaaSav"
   },
   "outputs": [],
   "source": [
    "def print_inputs(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "U_8SApKdS3wo"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import PNA\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "#train_dataset = build_dataset()\n",
    "\n",
    "def dataset_to_degree_bin(train_dataset):\n",
    "  # Compute the maximum in-degree in the training data.\n",
    "  max_degree = -1\n",
    "  for data in train_dataset:\n",
    "      d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "      max_degree = max(max_degree, int(d.max()))\n",
    "\n",
    "  deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "  for data in train_dataset:\n",
    "      d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "      deg += torch.bincount(d, minlength=deg.numel())\n",
    "  return deg\n",
    "\n",
    "class PNAnet(torch.nn.Module):\n",
    "  def __init__(self, train_dataset_example, hidden_channels=32, depth=4, dropout=0.0, towers=2, single_pass_pooling=False, graph_transform=False, normalization=True, pre_post_layers=1):\n",
    "    super(PNAnet, self).__init__()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Calculate x as the difference between mult_y and hidden_dim\n",
    "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
    "    #out_channels = towers * ((out_channels // towers) + 1)\n",
    "\n",
    "    in_channels = NUM_CLASSES\n",
    "    if graph_transform:\n",
    "      in_channels += NODE_FEATURE_EXTEND\n",
    "    deg = dataset_to_degree_bin(train_dataset_example)\n",
    "    aggregators = ['mean', 'min', 'max', 'std']\n",
    "    scalers = ['identity', 'amplification', 'attenuation']\n",
    "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
    "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
    "    self.single_pass_pooling = single_pass_pooling\n",
    "    self.graph_transform = graph_transform\n",
    "\n",
    "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
    "\n",
    "\n",
    "  def forward(self, x_in, edge_index, batch):\n",
    "    x = note_features_to_one_hot(x_in)\n",
    "    if self.graph_transform:\n",
    "      x = extend_node_features(x, edge_index, batch)\n",
    "    x = self.pnanet(x, edge_index)\n",
    "\n",
    "    if self.single_pass_pooling:\n",
    "      x = global_mask_pool(x, x_in, batch)\n",
    "    else:\n",
    "      x = global_mean_pool(x, batch)\n",
    "\n",
    "    #x = x.sum(dim=1)\n",
    "    x = self.final_mlp(x)\n",
    "    x = self.sigmoid(x) + EPSILON # add eps to avoid div. by zero\n",
    "    return x.flatten()\n",
    "\n",
    "class PNAmulti(torch.nn.Module):\n",
    "  def __init__(self, train_dataset_example, hidden_channels=32, graph_transform=False, depth=4, dropout=0.0, towers=1, model_num=5, single_pass_pooling=False, normalization=True, pre_post_layers=1):\n",
    "    super(PNAmulti, self).__init__()\n",
    "    self.model_num = model_num\n",
    "    self.pna_list = nn.ModuleList()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    weights = torch.ones(model_num)\n",
    "    self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "    for i in range(model_num):\n",
    "      # towers is i+1\n",
    "      pna_model = PNAnet(train_dataset_example, hidden_channels=hidden_channels, graph_transform=graph_transform, depth=depth, dropout=dropout, towers=i+1, single_pass_pooling=single_pass_pooling, normalization=normalization, pre_post_layers=pre_post_layers)\n",
    "      self.pna_list.append(pna_model)\n",
    "\n",
    "  def get_normalized_weights(self):\n",
    "    w_sigmoid = self.sigmoid(self.weights)\n",
    "    w = w_sigmoid/w_sigmoid.sum()\n",
    "    return w\n",
    "\n",
    "  def forward(self, x, edge_index, batch):\n",
    "    x_agg = None\n",
    "    i_not = random.choice(range(len(self.pna_list))) if self.training  and len(self.pna_list) > 1 else -1 # skip 1 during training\n",
    "    w = self.get_normalized_weights()\n",
    "    for i, pna_model in enumerate(self.pna_list):\n",
    "      if i == i_not:\n",
    "        continue\n",
    "      if x_agg is None:\n",
    "        x_agg = pna_model(x, edge_index, batch) * w[i]\n",
    "      else:\n",
    "        x_agg = x_agg + pna_model( x, edge_index, batch) * w[i]\n",
    "    return x_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTvXwuajPPOJ",
    "outputId": "a919e1b7-cfc1-418e-bba0-c88274816465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNAmulti(\n",
      "  (pna_list): ModuleList(\n",
      "    (0): PNAnet(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (normalization): BatchNorm(33)\n",
      "      (pnanet): PNA(4, 33, num_layers=4)\n",
      "      (final_mlp): Sequential(\n",
      "        (0): Linear(in_features=33, out_features=33, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=33, out_features=33, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=33, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): PNAnet(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (normalization): BatchNorm(34)\n",
      "      (pnanet): PNA(4, 34, num_layers=4)\n",
      "      (final_mlp): Sequential(\n",
      "        (0): Linear(in_features=34, out_features=34, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=34, out_features=34, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=34, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): PNAnet(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (normalization): BatchNorm(33)\n",
      "      (pnanet): PNA(4, 33, num_layers=4)\n",
      "      (final_mlp): Sequential(\n",
      "        (0): Linear(in_features=33, out_features=33, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=33, out_features=33, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=33, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PNAnet(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (normalization): BatchNorm(36)\n",
      "      (pnanet): PNA(4, 36, num_layers=4)\n",
      "      (final_mlp): Sequential(\n",
      "        (0): Linear(in_features=36, out_features=36, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=36, out_features=36, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=36, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PNAnet(\n",
      "      (sigmoid): Sigmoid()\n",
      "      (normalization): BatchNorm(35)\n",
      "      (pnanet): PNA(4, 35, num_layers=4)\n",
      "      (final_mlp): Sequential(\n",
      "        (0): Linear(in_features=35, out_features=35, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=35, out_features=35, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=35, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "tensor([0.4104, 0.4104, 0.4104, 0.4104, 0.4104, 0.4104, 0.4104, 0.4104, 0.4104,\n",
      "        0.4104], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = PNAmulti(build_dataset()[0])\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSyYUfY1X6Os",
    "outputId": "507c64d0-2f3e-442d-8f1f-fb5aece60a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNAnet(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (normalization): BatchNorm(34)\n",
      "  (pnanet): PNA(4, 34, num_layers=4)\n",
      "  (final_mlp): Sequential(\n",
      "    (0): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=34, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([0.5186, 0.5186, 0.5186, 0.5186, 0.5186, 0.5186, 0.5186, 0.5186, 0.5186,\n",
      "        0.5186], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = PNAnet(build_dataset()[0])\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB6WbKuwVdix"
   },
   "source": [
    "#### PNA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6Mvgyb_hVgpv"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import PNAConv\n",
    "\n",
    "class PNA2net(torch.nn.Module):\n",
    "  def __init__(self, train_dataset_example, hidden_dim=32, layer_num=4, dropout=0.0, normalization=False, tower_num=5, single_pass_pooling=False):\n",
    "    super().__init__()\n",
    "\n",
    "    aggregators = ['mean', 'min', 'max', 'std', 'var', 'sum']\n",
    "    scalers = ['identity', 'amplification', 'attenuation']\n",
    "    deg = dataset_to_degree_bin(train_dataset_example)\n",
    "\n",
    "    # Calculate x as the difference between mult_y and hidden_dim\n",
    "    hidden_dim = tower_num * ((hidden_dim // tower_num) + 1)\n",
    "\n",
    "    self.conv_list = nn.ModuleList()\n",
    "    self.batchnorm_list = nn.ModuleList()\n",
    "    self.mlp_list = nn.ModuleList()\n",
    "\n",
    "    self.dropout = dropout\n",
    "    self.normalization = normalization\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    self.single_pass_pooling = single_pass_pooling\n",
    "\n",
    "    input_channels = NUM_CLASSES\n",
    "  \n",
    "\n",
    "    for i in range(layer_num):\n",
    "      in_dim = hidden_dim if i > 0 else input_channels \n",
    "      out_dim = hidden_dim if i < layer_num-1 else 1 \n",
    "      self.conv_list.append(PNAConv(in_channels=in_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=tower_num, pre_layers=1, post_layers=1, divide_input=False))\n",
    "      self.batchnorm_list.append(BatchNorm(hidden_dim))\n",
    "      self.mlp_list.append(Seq(nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, hidden_dim+NUM_CLASSES+in_dim), nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, hidden_dim+NUM_CLASSES+in_dim), nn.ReLU(), Lin(hidden_dim+NUM_CLASSES+in_dim, out_dim)))\n",
    "          \n",
    "  def forward(self, x_in, edge_index, batch):\n",
    "    x_onehot = note_features_to_one_hot(x_in)\n",
    "    x = x_onehot.clone()\n",
    "    for i in range(len(self.conv_list)):\n",
    "      x_in = x.clone()\n",
    "      x = self.conv_list[i](x, edge_index)\n",
    "      if self.normalization:\n",
    "        x = self.batchnorm_list[i](x)\n",
    "      x_concat = torch.concat([x, x_onehot, x_in], dim=1)\n",
    "      x = self.mlp_list[i](x_concat)\n",
    "      if i < len(self.conv_list)-1:\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "    if self.single_pass_pooling:\n",
    "      x = global_mask_pool(x, x_in, batch)\n",
    "    else:\n",
    "      x = global_mean_pool(x, batch)\n",
    "      \n",
    "    x = self.sigmoid(x) + EPSILON\n",
    "    return x.flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgjqgcfhY1gC",
    "outputId": "43446f0e-9b7e-4570-a660-f15f06c99bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNA2net(\n",
      "  (conv_list): ModuleList(\n",
      "    (0): PNAConv(4, 35, towers=5, edge_dim=None)\n",
      "    (1-3): 3 x PNAConv(35, 35, towers=5, edge_dim=None)\n",
      "  )\n",
      "  (batchnorm_list): ModuleList(\n",
      "    (0-3): 4 x BatchNorm(35)\n",
      "  )\n",
      "  (mlp_list): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=43, out_features=43, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=43, out_features=43, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=43, out_features=35, bias=True)\n",
      "    )\n",
      "    (1-2): 2 x Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=74, out_features=74, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=74, out_features=74, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=74, out_features=35, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Linear(in_features=74, out_features=74, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=74, out_features=74, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=74, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "tensor([0.4878, 0.4878, 0.4878, 0.4878, 0.4878, 0.4878, 0.4878, 0.4878, 0.4878,\n",
      "        0.4878], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = PNA2net(build_dataset()[0])\n",
    "model.to(DEVICE)\n",
    "print(model) \n",
    "for step, data in enumerate(train_loader):\n",
    "  data.to(DEVICE)\n",
    "  print(model(data.x, data.edge_index, data.batch))\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tph0rjhNS6iS"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K0Yuz5FNVdy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gDKYLoMV6z6J"
   },
   "outputs": [],
   "source": [
    "def build_model(config, dataset):\n",
    "  hidden_dim = config.hidden_dim\n",
    "  single_pass_pooling = config.single_pass\n",
    "  if config.model == \"unet\":\n",
    "    model =  Unet(hidden_channels = hidden_dim,  out_channels = 1, depth=config.hidden_layer, single_pass_pooling=single_pass_pooling)\n",
    "  elif config.model == \"attention\":\n",
    "     model = AttentionNet(hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
    "  elif config.model == \"transformer\":\n",
    "    model = TransformerNet(hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
    "  elif config.model == \"pna\":\n",
    "    model = PNAnet(dataset, hidden_channels=hidden_dim, depth=config.hidden_layer, dropout=config.dropout, towers=config.towers, single_pass_pooling=single_pass_pooling, graph_transform=config.graph_transform, normalization=config.normalization, pre_post_layers=config.pre_post_layers)\n",
    "  elif config.model == \"pnamulti\":\n",
    "    model = PNAmulti(dataset, hidden_channels=hidden_dim, depth=config.hidden_layer, dropout=config.dropout, towers=config.towers, single_pass_pooling=single_pass_pooling, graph_transform=config.graph_transform, normalization=config.normalization, pre_post_layers=config.pre_post_layers)\n",
    "  elif config.model == \"pna2\":\n",
    "    model = PNA2net(dataset, hidden_dim=hidden_dim, layer_num=config.hidden_layer, dropout=config.dropout, normalization=config.normalization, single_pass_pooling=single_pass_pooling)\n",
    "  else:\n",
    "    ValueError(\"illegal net\")\n",
    "  return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GDOIysiiIGT"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FsofkLkkP3GK"
   },
   "outputs": [],
   "source": [
    "def find_best_choice(model, g, test_choices, x, x_shape):\n",
    "  p_0_list = list()\n",
    "  p_1_list = list()\n",
    "  x_noedge_list = list()\n",
    "  x_edge_list = list()\n",
    "  mask_indicator_list = list()\n",
    "\n",
    "  for i, mask_indicator in enumerate(test_choices):\n",
    "    x_noedge, x_edge = x.clone(), x.clone()\n",
    "    x_noedge[mask_indicator] = NO_EDGE_INDICATOR\n",
    "    x_edge[mask_indicator] = EDGE_INDICATOR\n",
    "    batch = torch.zeros(x_shape[0], dtype=torch.long).to(DEVICE)\n",
    "    p_0 = model(x_noedge.view(x_shape), g.edge_index, batch=batch).item()\n",
    "    p_1 = model(x_edge.view(x_shape), g.edge_index, batch=batch).item()\n",
    "\n",
    "    p_0_list.append(p_0/(p_0+p_1))\n",
    "    p_1_list.append(p_1/(p_0+p_1))\n",
    "    x_noedge_list.append(x_noedge)\n",
    "    x_edge_list.append(x_edge)\n",
    "    mask_indicator_list.append(mask_indicator)\n",
    "\n",
    "  best_i = np.argmax(p_1_list)\n",
    "  return p_0_list[best_i], p_1_list[best_i], x_noedge_list[best_i],  x_edge_list[best_i], mask_indicator_list[best_i]\n",
    "\n",
    "\n",
    "def generate_graph_multi(model, g, choice_num=5):\n",
    "  x = g.x\n",
    "  x_shape = x.shape\n",
    "  x = x.flatten()\n",
    "  indices_of_edges = (x != DUMMY).nonzero(as_tuple=False).flatten()\n",
    "  num_edges = indices_of_edges.numel()\n",
    "  x[indices_of_edges] = MASK\n",
    "  indices_of_edges = indices_of_edges.flatten().tolist()\n",
    "\n",
    "  for _ in range(num_edges):\n",
    "    random.shuffle(indices_of_edges)\n",
    "    choice_num_i = min(choice_num, len(indices_of_edges)) \n",
    "    test_choices = indices_of_edges[0:choice_num_i]\n",
    "    p_0, p_1, x_noedge, x_edge, mask_indicator = find_best_choice(model, g, test_choices, x, x_shape)\n",
    "    random_selection_index = int(np.random.choice([0, 1], size=1, p=[p_0, p_1]))\n",
    "    x = (x_noedge, x_edge)[random_selection_index]\n",
    "    indices_of_edges.remove(mask_indicator)\n",
    "\n",
    "  g.x = x.reshape(x_shape)\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "DQay_Zy35bHr",
    "outputId": "8b841102-0192-46a7-b1b4-b1ced222de41"
   },
   "outputs": [],
   "source": [
    "def test_candidates(candidates, x, g, batch, x_shape):\n",
    "  best_edge_index = None\n",
    "  best_edge_prob = None\n",
    "\n",
    "  for edge_index in candidates:\n",
    "    x[edge_index] = -1.0\n",
    "    edge_prob = model(x.view(x_shape), g.edge_index, batch=batch).item()\n",
    "    x[edge_index] = MASK\n",
    "    if best_edge_prob is None or max(edge_prob, 1.0-edge_prob) > best_edge_prob:\n",
    "      best_edge_index = edge_index\n",
    "      best_edge_prob = edge_prob\n",
    "\n",
    "  return best_edge_index, best_edge_prob\n",
    "\n",
    "def generate_graph_singlepass(model, g, choice_num = 1, config = None):\n",
    "  # does not have multi\n",
    "  x = g.x\n",
    "  x_shape = x.shape\n",
    "  x = x.flatten()\n",
    "  indices_of_edges = (x != DUMMY).nonzero(as_tuple=False).flatten()\n",
    "  num_edges = indices_of_edges.numel()\n",
    "  x[indices_of_edges] = MASK\n",
    "  indices_of_edges = indices_of_edges.flatten().tolist()\n",
    "  random.shuffle(indices_of_edges)\n",
    "  batch = torch.zeros(x_shape[0], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "  if config is not None and config.double_inference == True:\n",
    "    indices_of_edges = list(indices_of_edges) + list(indices_of_edges)\n",
    "\n",
    "  for i in range(len(indices_of_edges)):  \n",
    "    i_dyn = min(choice_num, len(indices_of_edges) - i)\n",
    "    candidates = indices_of_edges[i:i+i_dyn]\n",
    "    #print(\"candidates \",candidates)\n",
    "    edge_index, edge_prob = test_candidates(candidates, x, g, batch, x_shape)\n",
    "\n",
    "    assert(edge_prob > 0.0 and edge_prob < 1.0)\n",
    "    if random.random() < edge_prob:\n",
    "      x[edge_index] = EDGE_INDICATOR\n",
    "    else:\n",
    "      x[edge_index] = NO_EDGE_INDICATOR\n",
    "\n",
    "  g.x = x.reshape(x_shape)\n",
    "  return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_cN5gKKUiJTR"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_graphs(num, epoch_i, model, g_like, config):\n",
    "  model.eval()\n",
    "  mean_degree_list = list()\n",
    "  var_degree_list = list()\n",
    "  nx_graph_list = list()\n",
    "  for j in range(num):\n",
    "    if config.single_pass:\n",
    "      g = generate_graph_singlepass(model, g_like, choice_num = config.candidate_selection_radius, config=config)\n",
    "    else:\n",
    "      g = generate_graph_multi(model, g_like, choice_num = config.candidate_selection_radius)\n",
    "    nx_orig_graph = draw_pyg(g, filename=f\"generated_graph_epoch{str(epoch_i).zfill(5)}_sample{str(j).zfill(5)}.jpg\")\n",
    "    degree_list = [nx_orig_graph.degree(i) for i in nx_orig_graph.nodes()]\n",
    "    mean_degree_list.append(np.mean(degree_list))\n",
    "    var_degree_list.append(np.var(degree_list))\n",
    "    nx_graph_list.append(nx_orig_graph)\n",
    "  return np.mean(mean_degree_list), np.mean(var_degree_list), nx_graph_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKxurLjrXnCo"
   },
   "source": [
    "## Generat Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "nALlwSTtXmhx"
   },
   "outputs": [],
   "source": [
    "def gen_random_training_sample_unused(x):\n",
    "  x_masked = x.clone().reshape(-1)\n",
    "  indices_of_edges = (x_masked != DUMMY).nonzero(as_tuple=False).flatten()\n",
    "  num_edges = indices_of_edges.numel()\n",
    "  num_masked = torch.randint(1, num_edges+1, size=(1,), device=DEVICE)\n",
    "  #num_masked = sample_num_masked_geometric(1, num_edges+1)\n",
    "\n",
    "  shuffled_indices_of_edges = indices_of_edges[torch.randperm(indices_of_edges.numel())]\n",
    "  indices_of_edges_to_mask = shuffled_indices_of_edges[:num_masked]\n",
    "  \n",
    "  gt_value = x_masked[indices_of_edges_to_mask[0]].item() # we will change the first value back\n",
    "  x_masked[indices_of_edges_to_mask] = MASK\n",
    "\n",
    "\n",
    "  x_masked_0 = x_masked.clone()\n",
    "  x_masked_0[indices_of_edges_to_mask[0]] = NO_EDGE_INDICATOR\n",
    "  x_masked_0 = x_masked_0.reshape(x.shape)\n",
    "\n",
    "  x_masked_1 = x_masked.clone()\n",
    "  x_masked_1[indices_of_edges_to_mask[0]] = EDGE_INDICATOR\n",
    "  x_masked_1 = x_masked_1.reshape(x.shape)\n",
    "\n",
    "  if gt_value == NO_EDGE_INDICATOR:\n",
    "    return x_masked_0, x_masked_1\n",
    "  elif gt_value == EDGE_INDICATOR:\n",
    "    return x_masked_1, x_masked_0 \n",
    "  assert(False)\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "G-gIs8GVYm5l"
   },
   "outputs": [],
   "source": [
    "def sample_mask_and_flip(batch, x):\n",
    "  num_graphs = batch.view(-1)[-1]\n",
    "  x_id_mask = list()\n",
    "  x_id_flip = list()\n",
    "\n",
    "  for graph_id in range(num_graphs+1):\n",
    "    #num_nodes = (batch == graph_id).sum()\n",
    "    #indices_of_edges = (x != DUMMY and batch == graph_id).nonzero(as_tuple=False)\n",
    "    edge_indicator = x.view(-1) != DUMMY\n",
    "    batch_indicator = batch.view(-1) == graph_id \n",
    "    indices_of_edges = torch.nonzero(batch_indicator & edge_indicator)\n",
    "    indices_of_edges = indices_of_edges.flatten().tolist()\n",
    "    random.shuffle(indices_of_edges)\n",
    "    x_id_flip.append(indices_of_edges[-1])\n",
    "    num_masked = random.choice(range(len(indices_of_edges)))\n",
    "    x_id_mask = x_id_mask + indices_of_edges[:num_masked]\n",
    "\n",
    "  mask_and_flip_indicator = torch.zeros(batch.numel(), dtype=torch.long).to(DEVICE)\n",
    "  mask_and_flip_indicator[x_id_mask] = 1\n",
    "  mask_and_flip_indicator[x_id_flip] = 2\n",
    "\n",
    "  return mask_and_flip_indicator\n",
    "\n",
    "def mask_x_based_on_indicator(x, mask_and_flip_indicator):\n",
    "  x_new = x.clone()\n",
    "  to_mask = mask_and_flip_indicator == 1\n",
    "  x_new.view(-1)[to_mask] = MASK\n",
    "  return x_new\n",
    "\n",
    "\n",
    "# this is done inplace!\n",
    "def flip_x_based_on_indicator(x, mask_and_flip_indicator):\n",
    "  to_flip = (mask_and_flip_indicator == 2)#.to(DEVICE)\n",
    "  x = x.clone()  #sadly in-place operation not allowed here\n",
    "\n",
    "  #x[to_flip] = -x[to_flip]\n",
    "\n",
    "  x[to_flip] = torch.where(x[to_flip] == EDGE_INDICATOR, torch.tensor(NO_EDGE_INDICATOR).to(DEVICE), torch.tensor(EDGE_INDICATOR).to(DEVICE))\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "1La4pbjWakyz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "  data.to(DEVICE)\n",
    "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x)\n",
    "  break\n",
    "\n",
    "\n",
    "x = (mask_x_based_on_indicator(data.x, mask_and_flip_indicator))\n",
    "xz = x.clone()\n",
    "xx = (flip_x_based_on_indicator(x, mask_and_flip_indicator))\n",
    "\n",
    "(xz == xx).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TulPGAH8R0Eo"
   },
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "flDsYTRKR1ZB"
   },
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "def reduce_loss(loss_for_graph, config):\n",
    "  # loss_for_graph should be > 0\n",
    "  if config.loss == \"l1\":\n",
    "    return loss_for_graph.sum() \n",
    "  elif config.loss == \"l2\":\n",
    "    return F.mse_loss(loss_for_graph, torch.zeros_like(loss_for_graph))\n",
    "  elif config.loss == \"bce\":\n",
    "    return bce_loss(loss_for_graph, torch.zeros_like(loss_for_graph))\n",
    "  else:\n",
    "    raise ValueError(\"Illegal loss value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVx9HGc89VFe"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "K4M0XZ5Vpk5Z"
   },
   "outputs": [],
   "source": [
    "def prepare_x_for_single_pass(mask_and_flip_indicator, x):\n",
    "  x_new = x.clone()\n",
    "  to_mask = mask_and_flip_indicator == 1\n",
    "  x_new.view(-1)[to_mask] = MASK\n",
    "  choice_indicator = mask_and_flip_indicator == 2\n",
    "  x_new.view(-1)[choice_indicator] = -1 # this will be set to zero in the one hot encoding step\n",
    "  gt = x.clone().flatten()[choice_indicator]\n",
    "  gt = torch.where(gt == EDGE_INDICATOR, torch.tensor(1.0, device = DEVICE), torch.zeros_like(gt))\n",
    "  return x_new, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gj5td9zue7HT"
   },
   "outputs": [],
   "source": [
    "def single_pass_prediction(data, model, config):\n",
    "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x)\n",
    "  x_in, out_gt = prepare_x_for_single_pass(mask_and_flip_indicator, data.x)\n",
    "\n",
    "  if model.training: x_in = flip_edges_randomly(x_in, config.noise_probability)\n",
    "\n",
    "  out_prediction = model(x_in, data.edge_index, data.batch) # between 0 ,1 \n",
    "\n",
    "  #print(\"out gt: \", out_gt.flatten(), \"   out pred:   \", out_prediction.flatten())\n",
    "\n",
    "  loss_for_graph =  torch.abs(out_prediction.flatten() - out_gt.flatten()) # between 0 and 1\n",
    "  return loss_for_graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "sUfUS3OYfNxu"
   },
   "outputs": [],
   "source": [
    "def multi_pass_prediction(data, model, config):\n",
    "  mask_and_flip_indicator = sample_mask_and_flip(data.batch, data.x) # this then should also return gt \n",
    "  x = mask_x_based_on_indicator(data.x, mask_and_flip_indicator)\n",
    "\n",
    "  if model.training: x = flip_edges_randomly(x, config.noise_probability)\n",
    "\n",
    "  out_correct = model(x, data.edge_index, data.batch)  # is supposed to be 1 everywhere\n",
    "  x = flip_x_based_on_indicator(x, mask_and_flip_indicator)  # important to use x here and not data.x\n",
    "  out_incorrect = model(x, data.edge_index, data.batch) # is supposed to be 0 everywhere\n",
    "  loss_for_graph =  out_incorrect - out_correct # between -1 and 1\n",
    "  loss_for_graph = (loss_for_graph + 1.0) / 2.0   # between 0 and 1,\n",
    "  return loss_for_graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-_84ieMIoYh5"
   },
   "outputs": [],
   "source": [
    "def feed_data_to_model(data, model, config):\n",
    "  if config.single_pass == True:\n",
    "    return single_pass_prediction(data, model, config)\n",
    "  return multi_pass_prediction(data, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 264], x=[78, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dataset_train, graph_dataset_test  = build_dataset()\n",
    "graph_dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8RF9E-wfDXW"
   },
   "source": [
    "### Start Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "lDBAh9XPiO-a"
   },
   "outputs": [],
   "source": [
    "def start_agent(config):\n",
    "  graph_dataset_train, graph_dataset_test = build_dataset()\n",
    "  #graphs_ref_nx = build_test_set_nx(seed=42)\n",
    "  degree_loss, cluster_loss = compute_expected_optimal_loss(graph_dataset_train, graph_dataset_test)\n",
    "  print('Reference distance is: ', degree_loss, cluster_loss)\n",
    "  wandb.log({\"stats/degree_train\": degree_loss,  \"stats/cluster_train\": cluster_loss})\n",
    "\n",
    "  model = build_model(config, graph_dataset)\n",
    "  wandb.log({\"num_parameters\": sum(p.numel() for p in model.parameters())})\n",
    "\n",
    "  optimizer = Adam(model.parameters(), lr = config.learning_rate) \n",
    "  train_loader = DataLoader(graph_dataset_train, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "  for epoch_i in range(NUM_EPOCHS):\n",
    "    loss_list = list()\n",
    "    for step, data in enumerate(train_loader):\n",
    "      data.to(DEVICE)\n",
    "      model.train()\n",
    "      model.zero_grad()\n",
    "\n",
    "      loss_for_graph = feed_data_to_model(data, model, config) # between 0 and 1, probabilty of the incorrect choice\n",
    "      loss = reduce_loss(loss_for_graph, config)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      loss =  ((loss_for_graph.mean()).item()  -0.5 ) * 2.0 # between - 0.5 and 0.5 -> between - 1 and 1\n",
    "      loss_list.append(loss)\n",
    "      #wandb.log({\"epoch\": epoch_i+1, \"loss\": np.mean(loss_list)})\n",
    "\n",
    "    \n",
    "    # early stopping\n",
    "    if np.mean(loss_list) > -0.000001 and (epoch_i > 199 and epoch_i % 100 == 0):\n",
    "      return\n",
    "\n",
    "    print('epoch: {:06}  loss: {:.5f}'.format(epoch_i+1 ,np.mean(loss_list)))\n",
    "\n",
    "    if epoch_i % GENERATE_X_EPOCHS == 0 or epoch_i == NUM_EPOCHS-1:\n",
    "      dummy_graph = graph_dataset_train[0].clone().to(DEVICE)\n",
    "      print(\"start graph generation\")\n",
    "      deg_mean, deg_var, graphs_pred_nx = generate_graphs(NUM_GRAPHS_GENERATE, epoch_i, model, dummy_graph, config)\n",
    "      print(\"finish graph generation\")\n",
    "      generator_loss = (DEGREE-deg_mean)**2 + deg_var\n",
    "      wandb.log({\"graph-unmasking/gen-loss\": generator_loss, \"mean degree\": deg_mean, \"mean degree var\":deg_var, \"graph-unmasking/loss\": np.mean(loss_list)})\n",
    "      degree_loss, cluster_loss = compute_mmd_stats(graphs_pred_nx, graph_dataset_test)\n",
    "      wandb.log({\"stats/degree\": degree_loss,  \"stats/cluster\": cluster_loss})\n",
    "      print(deg_mean, deg_var, degree_loss, cluster_loss)\n",
    "\n",
    "  torch.save(model.state_dict(), \"model.weights\")\n",
    "  wandb.log_artifact(\"model.weights\", name=f'nn_weights_{SWEEP_ID}', type='weights') \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "qPcPralV7IF0"
   },
   "outputs": [],
   "source": [
    "def start_agent_envelope():\n",
    "  try:\n",
    "    with wandb.init():\n",
    "      config = wandb.config\n",
    "      set_seeds(SWEEP_ID)\n",
    "      print(SWEEP_ID, config)\n",
    "      if not IN_COLAB:\n",
    "        for python_file in sorted(glob.glob('*.ipynb')):\n",
    "          wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}\", type=\"my_dataset\")\n",
    "        for python_file in sorted(glob.glob('*.py')):\n",
    "          wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}\", type=\"my_dataset\")\n",
    "      return start_agent(config)\n",
    "  except Exception:\n",
    "    print(traceback.format_exc())\n",
    "    wandb.log({\"graph-unmasking/gen-loss\": -1,  \"epoch\": -1, \"graph-unmasking/loss\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHuZY5f_4NrN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8k6n2wdp\n",
      "Sweep URL: https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c2fn7rx5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcandidate_selection_radius: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdouble_inference: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgraph_transform: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: bce\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: pna\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_probability: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_post_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_pass: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttowers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gerritgrossmann/Documents/devel/MaskedGraphGen/wandb/run-20230605_155012-c2fn7rx5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nextaid/toad_retry/runs/c2fn7rx5' target=\"_blank\">effortless-sweep-1</a></strong> to <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">https://wandb.ai/nextaid/toad_retry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nextaid/toad_retry/runs/c2fn7rx5' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/runs/c2fn7rx5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8k6n2wdp {'batch_size': 500, 'candidate_selection_radius': 1, 'double_inference': False, 'dropout': 0, 'graph_transform': False, 'hidden_dim': 16, 'hidden_layer': 5, 'learning_rate': 1e-05, 'loss': 'bce', 'model': 'pna', 'noise_probability': 0, 'normalization': False, 'pre_post_layers': 1, 'single_pass': True, 'towers': 1}\n",
      "Reference distance is:  0.0 0.0008565843180121657\n",
      "epoch: 000001  loss: -0.00841\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.1 2.2222222222222223 0.5705763933136443 0.3330456981745606\n",
      "epoch: 000002  loss: -0.00894\n",
      "epoch: 000003  loss: -0.00949\n",
      "epoch: 000004  loss: -0.00968\n",
      "epoch: 000005  loss: -0.01113\n",
      "epoch: 000006  loss: -0.01168\n",
      "epoch: 000007  loss: -0.01167\n",
      "epoch: 000008  loss: -0.01080\n",
      "epoch: 000009  loss: -0.01257\n",
      "epoch: 000010  loss: -0.01447\n",
      "epoch: 000011  loss: -0.01544\n",
      "epoch: 000012  loss: -0.01499\n",
      "epoch: 000013  loss: -0.01657\n",
      "epoch: 000014  loss: -0.01542\n",
      "epoch: 000015  loss: -0.01880\n",
      "epoch: 000016  loss: -0.01654\n",
      "epoch: 000017  loss: -0.01925\n",
      "epoch: 000018  loss: -0.01582\n",
      "epoch: 000019  loss: -0.01946\n",
      "epoch: 000020  loss: -0.02132\n",
      "epoch: 000021  loss: -0.01901\n",
      "epoch: 000022  loss: -0.02124\n",
      "epoch: 000023  loss: -0.02369\n",
      "epoch: 000024  loss: -0.02310\n",
      "epoch: 000025  loss: -0.02123\n",
      "epoch: 000026  loss: -0.02368\n",
      "epoch: 000027  loss: -0.02629\n",
      "epoch: 000028  loss: -0.02645\n",
      "epoch: 000029  loss: -0.02553\n",
      "epoch: 000030  loss: -0.02599\n",
      "epoch: 000031  loss: -0.02783\n",
      "epoch: 000032  loss: -0.02686\n",
      "epoch: 000033  loss: -0.02809\n",
      "epoch: 000034  loss: -0.02659\n",
      "epoch: 000035  loss: -0.03109\n",
      "epoch: 000036  loss: -0.03245\n",
      "epoch: 000037  loss: -0.03496\n",
      "epoch: 000038  loss: -0.02786\n",
      "epoch: 000039  loss: -0.03252\n",
      "epoch: 000040  loss: -0.03335\n",
      "epoch: 000041  loss: -0.03161\n",
      "epoch: 000042  loss: -0.03447\n",
      "epoch: 000043  loss: -0.03386\n",
      "epoch: 000044  loss: -0.03667\n",
      "epoch: 000045  loss: -0.03835\n",
      "epoch: 000046  loss: -0.04130\n",
      "epoch: 000047  loss: -0.03643\n",
      "epoch: 000048  loss: -0.04236\n",
      "epoch: 000049  loss: -0.03649\n",
      "epoch: 000050  loss: -0.04031\n",
      "epoch: 000051  loss: -0.03860\n",
      "epoch: 000052  loss: -0.04068\n",
      "epoch: 000053  loss: -0.04484\n",
      "epoch: 000054  loss: -0.04334\n",
      "epoch: 000055  loss: -0.04123\n",
      "epoch: 000056  loss: -0.04722\n",
      "epoch: 000057  loss: -0.03935\n",
      "epoch: 000058  loss: -0.04908\n",
      "epoch: 000059  loss: -0.04668\n",
      "epoch: 000060  loss: -0.05003\n",
      "epoch: 000061  loss: -0.04931\n",
      "epoch: 000062  loss: -0.05416\n",
      "epoch: 000063  loss: -0.04748\n",
      "epoch: 000064  loss: -0.05301\n",
      "epoch: 000065  loss: -0.04505\n",
      "epoch: 000066  loss: -0.05908\n",
      "epoch: 000067  loss: -0.05448\n",
      "epoch: 000068  loss: -0.05694\n",
      "epoch: 000069  loss: -0.05225\n",
      "epoch: 000070  loss: -0.05672\n",
      "epoch: 000071  loss: -0.05622\n",
      "epoch: 000072  loss: -0.05054\n",
      "epoch: 000073  loss: -0.05805\n",
      "epoch: 000074  loss: -0.06604\n",
      "epoch: 000075  loss: -0.06350\n",
      "epoch: 000076  loss: -0.06726\n",
      "epoch: 000077  loss: -0.06568\n",
      "epoch: 000078  loss: -0.06702\n",
      "epoch: 000079  loss: -0.06590\n",
      "epoch: 000080  loss: -0.07151\n",
      "epoch: 000081  loss: -0.06813\n",
      "epoch: 000082  loss: -0.07959\n",
      "epoch: 000083  loss: -0.05934\n",
      "epoch: 000084  loss: -0.07030\n",
      "epoch: 000085  loss: -0.07468\n",
      "epoch: 000086  loss: -0.07937\n",
      "epoch: 000087  loss: -0.07239\n",
      "epoch: 000088  loss: -0.07726\n",
      "epoch: 000089  loss: -0.07350\n",
      "epoch: 000090  loss: -0.07516\n",
      "epoch: 000091  loss: -0.07763\n",
      "epoch: 000092  loss: -0.07259\n",
      "epoch: 000093  loss: -0.07824\n",
      "epoch: 000094  loss: -0.07132\n",
      "epoch: 000095  loss: -0.09336\n",
      "epoch: 000096  loss: -0.08398\n",
      "epoch: 000097  loss: -0.08563\n",
      "epoch: 000098  loss: -0.07724\n",
      "epoch: 000099  loss: -0.09261\n",
      "epoch: 000100  loss: -0.07907\n",
      "epoch: 000101  loss: -0.09335\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.366666666666666 2.3833333333333337 0.5670998979236679 0.3330572259486399\n",
      "epoch: 000102  loss: -0.09710\n",
      "epoch: 000103  loss: -0.09332\n",
      "epoch: 000104  loss: -0.08644\n",
      "epoch: 000105  loss: -0.10013\n",
      "epoch: 000106  loss: -0.09661\n",
      "epoch: 000107  loss: -0.08990\n",
      "epoch: 000108  loss: -0.09200\n",
      "epoch: 000109  loss: -0.10011\n",
      "epoch: 000110  loss: -0.10738\n",
      "epoch: 000111  loss: -0.10218\n",
      "epoch: 000112  loss: -0.11725\n",
      "epoch: 000113  loss: -0.10445\n",
      "epoch: 000114  loss: -0.11521\n",
      "epoch: 000115  loss: -0.11112\n",
      "epoch: 000116  loss: -0.11436\n",
      "epoch: 000117  loss: -0.11698\n",
      "epoch: 000118  loss: -0.12350\n",
      "epoch: 000119  loss: -0.12358\n",
      "epoch: 000120  loss: -0.11772\n",
      "epoch: 000121  loss: -0.13298\n",
      "epoch: 000122  loss: -0.12290\n",
      "epoch: 000123  loss: -0.11510\n",
      "epoch: 000124  loss: -0.12646\n",
      "epoch: 000125  loss: -0.13791\n",
      "epoch: 000126  loss: -0.13763\n",
      "epoch: 000127  loss: -0.13780\n",
      "epoch: 000128  loss: -0.12790\n",
      "epoch: 000129  loss: -0.13566\n",
      "epoch: 000130  loss: -0.14945\n",
      "epoch: 000131  loss: -0.14175\n",
      "epoch: 000132  loss: -0.16292\n",
      "epoch: 000133  loss: -0.14981\n",
      "epoch: 000134  loss: -0.15638\n",
      "epoch: 000135  loss: -0.15746\n",
      "epoch: 000136  loss: -0.15610\n",
      "epoch: 000137  loss: -0.15447\n",
      "epoch: 000138  loss: -0.13592\n",
      "epoch: 000139  loss: -0.16157\n",
      "epoch: 000140  loss: -0.14508\n",
      "epoch: 000141  loss: -0.18000\n",
      "epoch: 000142  loss: -0.17448\n",
      "epoch: 000143  loss: -0.16068\n",
      "epoch: 000144  loss: -0.19035\n",
      "epoch: 000145  loss: -0.16933\n",
      "epoch: 000146  loss: -0.18411\n",
      "epoch: 000147  loss: -0.15745\n",
      "epoch: 000148  loss: -0.17536\n",
      "epoch: 000149  loss: -0.18917\n",
      "epoch: 000150  loss: -0.18059\n",
      "epoch: 000151  loss: -0.18244\n",
      "epoch: 000152  loss: -0.19253\n",
      "epoch: 000153  loss: -0.18564\n",
      "epoch: 000154  loss: -0.18385\n",
      "epoch: 000155  loss: -0.20311\n",
      "epoch: 000156  loss: -0.18436\n",
      "epoch: 000157  loss: -0.18880\n",
      "epoch: 000158  loss: -0.19737\n",
      "epoch: 000159  loss: -0.18984\n",
      "epoch: 000160  loss: -0.18016\n",
      "epoch: 000161  loss: -0.19987\n",
      "epoch: 000162  loss: -0.19647\n",
      "epoch: 000163  loss: -0.18771\n",
      "epoch: 000164  loss: -0.18898\n",
      "epoch: 000165  loss: -0.19176\n",
      "epoch: 000166  loss: -0.21059\n",
      "epoch: 000167  loss: -0.19582\n",
      "epoch: 000168  loss: -0.20962\n",
      "epoch: 000169  loss: -0.19188\n",
      "epoch: 000170  loss: -0.21384\n",
      "epoch: 000171  loss: -0.20287\n",
      "epoch: 000172  loss: -0.18794\n",
      "epoch: 000173  loss: -0.19602\n",
      "epoch: 000174  loss: -0.17765\n",
      "epoch: 000175  loss: -0.19530\n",
      "epoch: 000176  loss: -0.19711\n",
      "epoch: 000177  loss: -0.20485\n",
      "epoch: 000178  loss: -0.21057\n",
      "epoch: 000179  loss: -0.20235\n",
      "epoch: 000180  loss: -0.19786\n",
      "epoch: 000181  loss: -0.20939\n",
      "epoch: 000182  loss: -0.22331\n",
      "epoch: 000183  loss: -0.21105\n",
      "epoch: 000184  loss: -0.19453\n",
      "epoch: 000185  loss: -0.20450\n",
      "epoch: 000186  loss: -0.23417\n",
      "epoch: 000187  loss: -0.20180\n",
      "epoch: 000188  loss: -0.21224\n",
      "epoch: 000189  loss: -0.20738\n",
      "epoch: 000190  loss: -0.18329\n",
      "epoch: 000191  loss: -0.20301\n",
      "epoch: 000192  loss: -0.22838\n",
      "epoch: 000193  loss: -0.22458\n",
      "epoch: 000194  loss: -0.23533\n",
      "epoch: 000195  loss: -0.22540\n",
      "epoch: 000196  loss: -0.20828\n",
      "epoch: 000197  loss: -0.22204\n",
      "epoch: 000198  loss: -0.19703\n",
      "epoch: 000199  loss: -0.19516\n",
      "epoch: 000200  loss: -0.20610\n",
      "epoch: 000201  loss: -0.21824\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.533333333333333 2.3722222222222227 0.6247178159846185 0.3382945025163926\n",
      "epoch: 000202  loss: -0.19297\n",
      "epoch: 000203  loss: -0.23908\n",
      "epoch: 000204  loss: -0.18767\n",
      "epoch: 000205  loss: -0.21318\n",
      "epoch: 000206  loss: -0.21272\n",
      "epoch: 000207  loss: -0.22003\n",
      "epoch: 000208  loss: -0.19847\n",
      "epoch: 000209  loss: -0.21823\n",
      "epoch: 000210  loss: -0.23076\n",
      "epoch: 000211  loss: -0.22387\n",
      "epoch: 000212  loss: -0.21010\n",
      "epoch: 000213  loss: -0.23177\n",
      "epoch: 000214  loss: -0.23273\n",
      "epoch: 000215  loss: -0.20586\n",
      "epoch: 000216  loss: -0.21617\n",
      "epoch: 000217  loss: -0.22860\n",
      "epoch: 000218  loss: -0.20830\n",
      "epoch: 000219  loss: -0.21732\n",
      "epoch: 000220  loss: -0.21658\n",
      "epoch: 000221  loss: -0.21875\n",
      "epoch: 000222  loss: -0.22316\n",
      "epoch: 000223  loss: -0.22144\n",
      "epoch: 000224  loss: -0.21357\n",
      "epoch: 000225  loss: -0.21154\n",
      "epoch: 000226  loss: -0.21017\n",
      "epoch: 000227  loss: -0.19790\n",
      "epoch: 000228  loss: -0.19485\n",
      "epoch: 000229  loss: -0.19649\n",
      "epoch: 000230  loss: -0.20219\n",
      "epoch: 000231  loss: -0.22579\n",
      "epoch: 000232  loss: -0.23159\n",
      "epoch: 000233  loss: -0.20981\n",
      "epoch: 000234  loss: -0.21453\n",
      "epoch: 000235  loss: -0.22333\n",
      "epoch: 000236  loss: -0.20409\n",
      "epoch: 000237  loss: -0.20544\n",
      "epoch: 000238  loss: -0.20918\n",
      "epoch: 000239  loss: -0.20037\n",
      "epoch: 000240  loss: -0.21928\n",
      "epoch: 000241  loss: -0.20552\n",
      "epoch: 000242  loss: -0.23729\n",
      "epoch: 000243  loss: -0.21637\n",
      "epoch: 000244  loss: -0.19495\n",
      "epoch: 000245  loss: -0.21139\n",
      "epoch: 000246  loss: -0.20565\n",
      "epoch: 000247  loss: -0.20799\n",
      "epoch: 000248  loss: -0.21018\n",
      "epoch: 000249  loss: -0.21253\n",
      "epoch: 000250  loss: -0.19642\n",
      "epoch: 000251  loss: -0.22332\n",
      "epoch: 000252  loss: -0.22154\n",
      "epoch: 000253  loss: -0.19516\n",
      "epoch: 000254  loss: -0.20350\n",
      "epoch: 000255  loss: -0.20688\n",
      "epoch: 000256  loss: -0.20023\n",
      "epoch: 000257  loss: -0.21710\n",
      "epoch: 000258  loss: -0.18951\n",
      "epoch: 000259  loss: -0.20106\n",
      "epoch: 000260  loss: -0.22730\n",
      "epoch: 000261  loss: -0.20751\n",
      "epoch: 000262  loss: -0.17954\n",
      "epoch: 000263  loss: -0.22333\n",
      "epoch: 000264  loss: -0.22103\n",
      "epoch: 000265  loss: -0.20272\n",
      "epoch: 000266  loss: -0.21286\n",
      "epoch: 000267  loss: -0.20244\n",
      "epoch: 000268  loss: -0.21459\n",
      "epoch: 000269  loss: -0.19596\n",
      "epoch: 000270  loss: -0.21819\n",
      "epoch: 000271  loss: -0.19373\n",
      "epoch: 000272  loss: -0.18747\n",
      "epoch: 000273  loss: -0.20284\n",
      "epoch: 000274  loss: -0.20044\n",
      "epoch: 000275  loss: -0.20865\n",
      "epoch: 000276  loss: -0.18198\n",
      "epoch: 000277  loss: -0.20680\n",
      "epoch: 000278  loss: -0.20780\n",
      "epoch: 000279  loss: -0.21798\n",
      "epoch: 000280  loss: -0.21580\n",
      "epoch: 000281  loss: -0.20510\n",
      "epoch: 000282  loss: -0.20873\n",
      "epoch: 000283  loss: -0.20746\n",
      "epoch: 000284  loss: -0.20400\n",
      "epoch: 000285  loss: -0.20619\n",
      "epoch: 000286  loss: -0.19960\n",
      "epoch: 000287  loss: -0.21861\n",
      "epoch: 000288  loss: -0.22115\n",
      "epoch: 000289  loss: -0.19760\n",
      "epoch: 000290  loss: -0.22305\n",
      "epoch: 000291  loss: -0.19934\n",
      "epoch: 000292  loss: -0.21158\n",
      "epoch: 000293  loss: -0.20160\n",
      "epoch: 000294  loss: -0.20453\n",
      "epoch: 000295  loss: -0.21266\n",
      "epoch: 000296  loss: -0.21162\n",
      "epoch: 000297  loss: -0.21139\n",
      "epoch: 000298  loss: -0.20699\n",
      "epoch: 000299  loss: -0.22920\n",
      "epoch: 000300  loss: -0.19408\n",
      "epoch: 000301  loss: -0.19647\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.333333333333333 2.138888888888889 0.6539186678834374 0.3330684142280654\n",
      "epoch: 000302  loss: -0.22281\n",
      "epoch: 000303  loss: -0.21370\n",
      "epoch: 000304  loss: -0.21752\n",
      "epoch: 000305  loss: -0.22400\n",
      "epoch: 000306  loss: -0.21555\n",
      "epoch: 000307  loss: -0.22282\n",
      "epoch: 000308  loss: -0.18766\n",
      "epoch: 000309  loss: -0.21829\n",
      "epoch: 000310  loss: -0.20210\n",
      "epoch: 000311  loss: -0.21005\n",
      "epoch: 000312  loss: -0.19394\n",
      "epoch: 000313  loss: -0.21108\n",
      "epoch: 000314  loss: -0.19966\n",
      "epoch: 000315  loss: -0.18524\n",
      "epoch: 000316  loss: -0.20085\n",
      "epoch: 000317  loss: -0.19531\n",
      "epoch: 000318  loss: -0.22406\n",
      "epoch: 000319  loss: -0.20689\n",
      "epoch: 000320  loss: -0.20040\n",
      "epoch: 000321  loss: -0.19980\n",
      "epoch: 000322  loss: -0.21120\n",
      "epoch: 000323  loss: -0.19446\n",
      "epoch: 000324  loss: -0.20227\n",
      "epoch: 000325  loss: -0.20006\n",
      "epoch: 000326  loss: -0.20402\n",
      "epoch: 000327  loss: -0.18670\n",
      "epoch: 000328  loss: -0.18158\n",
      "epoch: 000329  loss: -0.20728\n",
      "epoch: 000330  loss: -0.20233\n",
      "epoch: 000331  loss: -0.23002\n",
      "epoch: 000332  loss: -0.19014\n",
      "epoch: 000333  loss: -0.19374\n",
      "epoch: 000334  loss: -0.20005\n",
      "epoch: 000335  loss: -0.19116\n",
      "epoch: 000336  loss: -0.20131\n",
      "epoch: 000337  loss: -0.19133\n",
      "epoch: 000338  loss: -0.20397\n",
      "epoch: 000339  loss: -0.19036\n",
      "epoch: 000340  loss: -0.20046\n",
      "epoch: 000341  loss: -0.19904\n",
      "epoch: 000342  loss: -0.20094\n",
      "epoch: 000343  loss: -0.20675\n",
      "epoch: 000344  loss: -0.20000\n",
      "epoch: 000345  loss: -0.19523\n",
      "epoch: 000346  loss: -0.21734\n",
      "epoch: 000347  loss: -0.20530\n",
      "epoch: 000348  loss: -0.22016\n",
      "epoch: 000349  loss: -0.20845\n",
      "epoch: 000350  loss: -0.20210\n",
      "epoch: 000351  loss: -0.24095\n",
      "epoch: 000352  loss: -0.19452\n",
      "epoch: 000353  loss: -0.20859\n",
      "epoch: 000354  loss: -0.21588\n",
      "epoch: 000355  loss: -0.19789\n",
      "epoch: 000356  loss: -0.21078\n",
      "epoch: 000357  loss: -0.22589\n",
      "epoch: 000358  loss: -0.18103\n",
      "epoch: 000359  loss: -0.21170\n",
      "epoch: 000360  loss: -0.20482\n",
      "epoch: 000361  loss: -0.22081\n",
      "epoch: 000362  loss: -0.19264\n",
      "epoch: 000363  loss: -0.18349\n",
      "epoch: 000364  loss: -0.22910\n",
      "epoch: 000365  loss: -0.20402\n",
      "epoch: 000366  loss: -0.20797\n",
      "epoch: 000367  loss: -0.22960\n",
      "epoch: 000368  loss: -0.20385\n",
      "epoch: 000369  loss: -0.21374\n",
      "epoch: 000370  loss: -0.21248\n",
      "epoch: 000371  loss: -0.20203\n",
      "epoch: 000372  loss: -0.21405\n",
      "epoch: 000373  loss: -0.21840\n",
      "epoch: 000374  loss: -0.21240\n",
      "epoch: 000375  loss: -0.20497\n",
      "epoch: 000376  loss: -0.19719\n",
      "epoch: 000377  loss: -0.20783\n",
      "epoch: 000378  loss: -0.20545\n",
      "epoch: 000379  loss: -0.20597\n",
      "epoch: 000380  loss: -0.20860\n",
      "epoch: 000381  loss: -0.20463\n",
      "epoch: 000382  loss: -0.21493\n",
      "epoch: 000383  loss: -0.21026\n",
      "epoch: 000384  loss: -0.22243\n",
      "epoch: 000385  loss: -0.19417\n",
      "epoch: 000386  loss: -0.19036\n",
      "epoch: 000387  loss: -0.21103\n",
      "epoch: 000388  loss: -0.22311\n",
      "epoch: 000389  loss: -0.22470\n",
      "epoch: 000390  loss: -0.22824\n",
      "epoch: 000391  loss: -0.20612\n",
      "epoch: 000392  loss: -0.21574\n",
      "epoch: 000393  loss: -0.18841\n",
      "epoch: 000394  loss: -0.19048\n",
      "epoch: 000395  loss: -0.21569\n",
      "epoch: 000396  loss: -0.20049\n",
      "epoch: 000397  loss: -0.22565\n",
      "epoch: 000398  loss: -0.20776\n",
      "epoch: 000399  loss: -0.22671\n",
      "epoch: 000400  loss: -0.23256\n",
      "epoch: 000401  loss: -0.21505\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.2333333333333325 2.2666666666666666 0.5730660195673887 0.3341015909636989\n",
      "epoch: 000402  loss: -0.21103\n",
      "epoch: 000403  loss: -0.20740\n",
      "epoch: 000404  loss: -0.23253\n",
      "epoch: 000405  loss: -0.22706\n",
      "epoch: 000406  loss: -0.23127\n",
      "epoch: 000407  loss: -0.22272\n",
      "epoch: 000408  loss: -0.20371\n",
      "epoch: 000409  loss: -0.20254\n",
      "epoch: 000410  loss: -0.21670\n",
      "epoch: 000411  loss: -0.20427\n",
      "epoch: 000412  loss: -0.21822\n",
      "epoch: 000413  loss: -0.19439\n",
      "epoch: 000414  loss: -0.21797\n",
      "epoch: 000415  loss: -0.20187\n",
      "epoch: 000416  loss: -0.19337\n",
      "epoch: 000417  loss: -0.22266\n",
      "epoch: 000418  loss: -0.20036\n",
      "epoch: 000419  loss: -0.20473\n",
      "epoch: 000420  loss: -0.21619\n",
      "epoch: 000421  loss: -0.18759\n",
      "epoch: 000422  loss: -0.22892\n",
      "epoch: 000423  loss: -0.23152\n",
      "epoch: 000424  loss: -0.19740\n",
      "epoch: 000425  loss: -0.21391\n",
      "epoch: 000426  loss: -0.19858\n",
      "epoch: 000427  loss: -0.19724\n",
      "epoch: 000428  loss: -0.20905\n",
      "epoch: 000429  loss: -0.20648\n",
      "epoch: 000430  loss: -0.19674\n",
      "epoch: 000431  loss: -0.21220\n",
      "epoch: 000432  loss: -0.19675\n",
      "epoch: 000433  loss: -0.21566\n",
      "epoch: 000434  loss: -0.17115\n",
      "epoch: 000435  loss: -0.19885\n",
      "epoch: 000436  loss: -0.21725\n",
      "epoch: 000437  loss: -0.20430\n",
      "epoch: 000438  loss: -0.20160\n",
      "epoch: 000439  loss: -0.20978\n",
      "epoch: 000440  loss: -0.21431\n",
      "epoch: 000441  loss: -0.20977\n",
      "epoch: 000442  loss: -0.21174\n",
      "epoch: 000443  loss: -0.22079\n",
      "epoch: 000444  loss: -0.20395\n",
      "epoch: 000445  loss: -0.22986\n",
      "epoch: 000446  loss: -0.22520\n",
      "epoch: 000447  loss: -0.20020\n",
      "epoch: 000448  loss: -0.22630\n",
      "epoch: 000449  loss: -0.21338\n",
      "epoch: 000450  loss: -0.23056\n",
      "epoch: 000451  loss: -0.21022\n",
      "epoch: 000452  loss: -0.20409\n",
      "epoch: 000453  loss: -0.19339\n",
      "epoch: 000454  loss: -0.21206\n",
      "epoch: 000455  loss: -0.18734\n",
      "epoch: 000456  loss: -0.22193\n",
      "epoch: 000457  loss: -0.18502\n",
      "epoch: 000458  loss: -0.19600\n",
      "epoch: 000459  loss: -0.20036\n",
      "epoch: 000460  loss: -0.20885\n",
      "epoch: 000461  loss: -0.20693\n",
      "epoch: 000462  loss: -0.17968\n",
      "epoch: 000463  loss: -0.19558\n",
      "epoch: 000464  loss: -0.18234\n",
      "epoch: 000465  loss: -0.19930\n",
      "epoch: 000466  loss: -0.21494\n",
      "epoch: 000467  loss: -0.20754\n",
      "epoch: 000468  loss: -0.20066\n",
      "epoch: 000469  loss: -0.19102\n",
      "epoch: 000470  loss: -0.19406\n",
      "epoch: 000471  loss: -0.21951\n",
      "epoch: 000472  loss: -0.22828\n",
      "epoch: 000473  loss: -0.19670\n",
      "epoch: 000474  loss: -0.21010\n",
      "epoch: 000475  loss: -0.20984\n",
      "epoch: 000476  loss: -0.18957\n",
      "epoch: 000477  loss: -0.19761\n",
      "epoch: 000478  loss: -0.19847\n",
      "epoch: 000479  loss: -0.21933\n",
      "epoch: 000480  loss: -0.19720\n",
      "epoch: 000481  loss: -0.18748\n",
      "epoch: 000482  loss: -0.20246\n",
      "epoch: 000483  loss: -0.19811\n",
      "epoch: 000484  loss: -0.19678\n",
      "epoch: 000485  loss: -0.17690\n",
      "epoch: 000486  loss: -0.20324\n",
      "epoch: 000487  loss: -0.20761\n",
      "epoch: 000488  loss: -0.19778\n",
      "epoch: 000489  loss: -0.20486\n",
      "epoch: 000490  loss: -0.21432\n",
      "epoch: 000491  loss: -0.20026\n",
      "epoch: 000492  loss: -0.20202\n",
      "epoch: 000493  loss: -0.20565\n",
      "epoch: 000494  loss: -0.19271\n",
      "epoch: 000495  loss: -0.19672\n",
      "epoch: 000496  loss: -0.20138\n",
      "epoch: 000497  loss: -0.20182\n",
      "epoch: 000498  loss: -0.18374\n",
      "epoch: 000499  loss: -0.22056\n",
      "epoch: 000500  loss: -0.21319\n",
      "epoch: 000501  loss: -0.20305\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.366666666666665 2.477777777777778 0.5761981216152252 0.3330686189849192\n",
      "epoch: 000502  loss: -0.19112\n",
      "epoch: 000503  loss: -0.18766\n",
      "epoch: 000504  loss: -0.19736\n",
      "epoch: 000505  loss: -0.21346\n",
      "epoch: 000506  loss: -0.22612\n",
      "epoch: 000507  loss: -0.19679\n",
      "epoch: 000508  loss: -0.20759\n",
      "epoch: 000509  loss: -0.19775\n",
      "epoch: 000510  loss: -0.20293\n",
      "epoch: 000511  loss: -0.21057\n",
      "epoch: 000512  loss: -0.22448\n",
      "epoch: 000513  loss: -0.19315\n",
      "epoch: 000514  loss: -0.18302\n",
      "epoch: 000515  loss: -0.19538\n",
      "epoch: 000516  loss: -0.22828\n",
      "epoch: 000517  loss: -0.22590\n",
      "epoch: 000518  loss: -0.18853\n",
      "epoch: 000519  loss: -0.22136\n",
      "epoch: 000520  loss: -0.21609\n",
      "epoch: 000521  loss: -0.21453\n",
      "epoch: 000522  loss: -0.22465\n",
      "epoch: 000523  loss: -0.23460\n",
      "epoch: 000524  loss: -0.19727\n",
      "epoch: 000525  loss: -0.20401\n",
      "epoch: 000526  loss: -0.21428\n",
      "epoch: 000527  loss: -0.22210\n",
      "epoch: 000528  loss: -0.18359\n",
      "epoch: 000529  loss: -0.20844\n",
      "epoch: 000530  loss: -0.20160\n",
      "epoch: 000531  loss: -0.20752\n",
      "epoch: 000532  loss: -0.23600\n",
      "epoch: 000533  loss: -0.20324\n",
      "epoch: 000534  loss: -0.20937\n",
      "epoch: 000535  loss: -0.21038\n",
      "epoch: 000536  loss: -0.22103\n",
      "epoch: 000537  loss: -0.22651\n",
      "epoch: 000538  loss: -0.20579\n",
      "epoch: 000539  loss: -0.23996\n",
      "epoch: 000540  loss: -0.22265\n",
      "epoch: 000541  loss: -0.20498\n",
      "epoch: 000542  loss: -0.23960\n",
      "epoch: 000543  loss: -0.22635\n",
      "epoch: 000544  loss: -0.21052\n",
      "epoch: 000545  loss: -0.21509\n",
      "epoch: 000546  loss: -0.21985\n",
      "epoch: 000547  loss: -0.22593\n",
      "epoch: 000548  loss: -0.20936\n",
      "epoch: 000549  loss: -0.18943\n",
      "epoch: 000550  loss: -0.19176\n",
      "epoch: 000551  loss: -0.21100\n",
      "epoch: 000552  loss: -0.23349\n",
      "epoch: 000553  loss: -0.19216\n",
      "epoch: 000554  loss: -0.22536\n",
      "epoch: 000555  loss: -0.21091\n",
      "epoch: 000556  loss: -0.20809\n",
      "epoch: 000557  loss: -0.19305\n",
      "epoch: 000558  loss: -0.21642\n",
      "epoch: 000559  loss: -0.22004\n",
      "epoch: 000560  loss: -0.18411\n",
      "epoch: 000561  loss: -0.19381\n",
      "epoch: 000562  loss: -0.19197\n",
      "epoch: 000563  loss: -0.20687\n",
      "epoch: 000564  loss: -0.21215\n",
      "epoch: 000565  loss: -0.18855\n",
      "epoch: 000566  loss: -0.19952\n",
      "epoch: 000567  loss: -0.19086\n",
      "epoch: 000568  loss: -0.21322\n",
      "epoch: 000569  loss: -0.21407\n",
      "epoch: 000570  loss: -0.18543\n",
      "epoch: 000571  loss: -0.19578\n",
      "epoch: 000572  loss: -0.20769\n",
      "epoch: 000573  loss: -0.19296\n",
      "epoch: 000574  loss: -0.17537\n",
      "epoch: 000575  loss: -0.21761\n",
      "epoch: 000576  loss: -0.21183\n",
      "epoch: 000577  loss: -0.20815\n",
      "epoch: 000578  loss: -0.18675\n",
      "epoch: 000579  loss: -0.20431\n",
      "epoch: 000580  loss: -0.18644\n",
      "epoch: 000581  loss: -0.17404\n",
      "epoch: 000582  loss: -0.20964\n",
      "epoch: 000583  loss: -0.19900\n",
      "epoch: 000584  loss: -0.20049\n",
      "epoch: 000585  loss: -0.19503\n",
      "epoch: 000586  loss: -0.19462\n",
      "epoch: 000587  loss: -0.20622\n",
      "epoch: 000588  loss: -0.21000\n",
      "epoch: 000589  loss: -0.19197\n",
      "epoch: 000590  loss: -0.19265\n",
      "epoch: 000591  loss: -0.20685\n",
      "epoch: 000592  loss: -0.21048\n",
      "epoch: 000593  loss: -0.19421\n",
      "epoch: 000594  loss: -0.19582\n",
      "epoch: 000595  loss: -0.20094\n",
      "epoch: 000596  loss: -0.20364\n",
      "epoch: 000597  loss: -0.19733\n",
      "epoch: 000598  loss: -0.21395\n",
      "epoch: 000599  loss: -0.17377\n",
      "epoch: 000600  loss: -0.21407\n",
      "epoch: 000601  loss: -0.18108\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.3500000000000005 2.7972222222222225 0.5597616589084968 0.33321921710104846\n",
      "epoch: 000602  loss: -0.19912\n",
      "epoch: 000603  loss: -0.18975\n",
      "epoch: 000604  loss: -0.20010\n",
      "epoch: 000605  loss: -0.21362\n",
      "epoch: 000606  loss: -0.17375\n",
      "epoch: 000607  loss: -0.21480\n",
      "epoch: 000608  loss: -0.19257\n",
      "epoch: 000609  loss: -0.19750\n",
      "epoch: 000610  loss: -0.17729\n",
      "epoch: 000611  loss: -0.19938\n",
      "epoch: 000612  loss: -0.18716\n",
      "epoch: 000613  loss: -0.19571\n",
      "epoch: 000614  loss: -0.19857\n",
      "epoch: 000615  loss: -0.19321\n",
      "epoch: 000616  loss: -0.19014\n",
      "epoch: 000617  loss: -0.19431\n",
      "epoch: 000618  loss: -0.17752\n",
      "epoch: 000619  loss: -0.19436\n",
      "epoch: 000620  loss: -0.19152\n",
      "epoch: 000621  loss: -0.21729\n",
      "epoch: 000622  loss: -0.21519\n",
      "epoch: 000623  loss: -0.19168\n",
      "epoch: 000624  loss: -0.21543\n",
      "epoch: 000625  loss: -0.20972\n",
      "epoch: 000626  loss: -0.20132\n",
      "epoch: 000627  loss: -0.19574\n",
      "epoch: 000628  loss: -0.20975\n",
      "epoch: 000629  loss: -0.19825\n",
      "epoch: 000630  loss: -0.19701\n",
      "epoch: 000631  loss: -0.19850\n",
      "epoch: 000632  loss: -0.21731\n",
      "epoch: 000633  loss: -0.20635\n",
      "epoch: 000634  loss: -0.19120\n",
      "epoch: 000635  loss: -0.17107\n",
      "epoch: 000636  loss: -0.20756\n",
      "epoch: 000637  loss: -0.20692\n",
      "epoch: 000638  loss: -0.20692\n",
      "epoch: 000639  loss: -0.21201\n",
      "epoch: 000640  loss: -0.21348\n",
      "epoch: 000641  loss: -0.20504\n",
      "epoch: 000642  loss: -0.20562\n",
      "epoch: 000643  loss: -0.20833\n",
      "epoch: 000644  loss: -0.21108\n",
      "epoch: 000645  loss: -0.19178\n",
      "epoch: 000646  loss: -0.21459\n",
      "epoch: 000647  loss: -0.19747\n",
      "epoch: 000648  loss: -0.21825\n",
      "epoch: 000649  loss: -0.19896\n",
      "epoch: 000650  loss: -0.20072\n",
      "epoch: 000651  loss: -0.20332\n",
      "epoch: 000652  loss: -0.21553\n",
      "epoch: 000653  loss: -0.19505\n",
      "epoch: 000654  loss: -0.19910\n",
      "epoch: 000655  loss: -0.20656\n",
      "epoch: 000656  loss: -0.18677\n",
      "epoch: 000657  loss: -0.23601\n",
      "epoch: 000658  loss: -0.19237\n",
      "epoch: 000659  loss: -0.21192\n",
      "epoch: 000660  loss: -0.20661\n",
      "epoch: 000661  loss: -0.21681\n",
      "epoch: 000662  loss: -0.22049\n",
      "epoch: 000663  loss: -0.19722\n",
      "epoch: 000664  loss: -0.20669\n",
      "epoch: 000665  loss: -0.19632\n",
      "epoch: 000666  loss: -0.21878\n",
      "epoch: 000667  loss: -0.21789\n",
      "epoch: 000668  loss: -0.19585\n",
      "epoch: 000669  loss: -0.21559\n",
      "epoch: 000670  loss: -0.20085\n",
      "epoch: 000671  loss: -0.21270\n",
      "epoch: 000672  loss: -0.21269\n",
      "epoch: 000673  loss: -0.21372\n",
      "epoch: 000674  loss: -0.20078\n",
      "epoch: 000675  loss: -0.21103\n",
      "epoch: 000676  loss: -0.19499\n",
      "epoch: 000677  loss: -0.21058\n",
      "epoch: 000678  loss: -0.20217\n",
      "epoch: 000679  loss: -0.19143\n",
      "epoch: 000680  loss: -0.18395\n",
      "epoch: 000681  loss: -0.21812\n",
      "epoch: 000682  loss: -0.22433\n",
      "epoch: 000683  loss: -0.21061\n",
      "epoch: 000684  loss: -0.20851\n",
      "epoch: 000685  loss: -0.19528\n",
      "epoch: 000686  loss: -0.22522\n",
      "epoch: 000687  loss: -0.20067\n",
      "epoch: 000688  loss: -0.20336\n",
      "epoch: 000689  loss: -0.19293\n",
      "epoch: 000690  loss: -0.18478\n",
      "epoch: 000691  loss: -0.21021\n",
      "epoch: 000692  loss: -0.19799\n",
      "epoch: 000693  loss: -0.22472\n",
      "epoch: 000694  loss: -0.20298\n",
      "epoch: 000695  loss: -0.22280\n",
      "epoch: 000696  loss: -0.20337\n",
      "epoch: 000697  loss: -0.20524\n",
      "epoch: 000698  loss: -0.21041\n",
      "epoch: 000699  loss: -0.20644\n",
      "epoch: 000700  loss: -0.20481\n",
      "epoch: 000701  loss: -0.19005\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.200000000000001 2.272222222222222 0.599620643826204 0.33329440084051515\n",
      "epoch: 000702  loss: -0.20598\n",
      "epoch: 000703  loss: -0.21773\n",
      "epoch: 000704  loss: -0.21531\n",
      "epoch: 000705  loss: -0.20705\n",
      "epoch: 000706  loss: -0.20582\n",
      "epoch: 000707  loss: -0.21469\n",
      "epoch: 000708  loss: -0.20701\n",
      "epoch: 000709  loss: -0.21872\n",
      "epoch: 000710  loss: -0.21095\n",
      "epoch: 000711  loss: -0.21382\n",
      "epoch: 000712  loss: -0.21402\n",
      "epoch: 000713  loss: -0.21192\n",
      "epoch: 000714  loss: -0.22330\n",
      "epoch: 000715  loss: -0.19873\n",
      "epoch: 000716  loss: -0.21968\n",
      "epoch: 000717  loss: -0.19850\n",
      "epoch: 000718  loss: -0.21486\n",
      "epoch: 000719  loss: -0.22027\n",
      "epoch: 000720  loss: -0.21718\n",
      "epoch: 000721  loss: -0.20244\n",
      "epoch: 000722  loss: -0.20473\n",
      "epoch: 000723  loss: -0.20535\n",
      "epoch: 000724  loss: -0.20482\n",
      "epoch: 000725  loss: -0.18940\n",
      "epoch: 000726  loss: -0.22564\n",
      "epoch: 000727  loss: -0.21764\n",
      "epoch: 000728  loss: -0.22545\n",
      "epoch: 000729  loss: -0.21121\n",
      "epoch: 000730  loss: -0.20191\n",
      "epoch: 000731  loss: -0.22034\n",
      "epoch: 000732  loss: -0.20558\n",
      "epoch: 000733  loss: -0.18990\n",
      "epoch: 000734  loss: -0.19240\n",
      "epoch: 000735  loss: -0.22676\n",
      "epoch: 000736  loss: -0.22571\n",
      "epoch: 000737  loss: -0.20733\n",
      "epoch: 000738  loss: -0.22504\n",
      "epoch: 000739  loss: -0.20684\n",
      "epoch: 000740  loss: -0.20710\n",
      "epoch: 000741  loss: -0.20933\n",
      "epoch: 000742  loss: -0.19938\n",
      "epoch: 000743  loss: -0.18385\n",
      "epoch: 000744  loss: -0.20742\n",
      "epoch: 000745  loss: -0.17970\n",
      "epoch: 000746  loss: -0.19044\n",
      "epoch: 000747  loss: -0.19353\n",
      "epoch: 000748  loss: -0.17680\n",
      "epoch: 000749  loss: -0.21809\n",
      "epoch: 000750  loss: -0.20694\n",
      "epoch: 000751  loss: -0.19222\n",
      "epoch: 000752  loss: -0.18576\n",
      "epoch: 000753  loss: -0.20634\n",
      "epoch: 000754  loss: -0.20260\n",
      "epoch: 000755  loss: -0.19965\n",
      "epoch: 000756  loss: -0.19949\n",
      "epoch: 000757  loss: -0.18608\n",
      "epoch: 000758  loss: -0.19452\n",
      "epoch: 000759  loss: -0.18378\n",
      "epoch: 000760  loss: -0.20615\n",
      "epoch: 000761  loss: -0.19582\n",
      "epoch: 000762  loss: -0.19365\n",
      "epoch: 000763  loss: -0.19186\n",
      "epoch: 000764  loss: -0.21749\n",
      "epoch: 000765  loss: -0.18457\n",
      "epoch: 000766  loss: -0.21790\n",
      "epoch: 000767  loss: -0.18809\n",
      "epoch: 000768  loss: -0.20642\n",
      "epoch: 000769  loss: -0.18988\n",
      "epoch: 000770  loss: -0.18685\n",
      "epoch: 000771  loss: -0.20904\n",
      "epoch: 000772  loss: -0.19522\n",
      "epoch: 000773  loss: -0.21164\n",
      "epoch: 000774  loss: -0.19951\n",
      "epoch: 000775  loss: -0.20921\n",
      "epoch: 000776  loss: -0.21705\n",
      "epoch: 000777  loss: -0.22385\n",
      "epoch: 000778  loss: -0.16583\n",
      "epoch: 000779  loss: -0.20835\n",
      "epoch: 000780  loss: -0.20419\n",
      "epoch: 000781  loss: -0.20144\n",
      "epoch: 000782  loss: -0.20412\n",
      "epoch: 000783  loss: -0.19588\n",
      "epoch: 000784  loss: -0.22317\n",
      "epoch: 000785  loss: -0.21719\n",
      "epoch: 000786  loss: -0.22069\n",
      "epoch: 000787  loss: -0.19394\n",
      "epoch: 000788  loss: -0.19207\n",
      "epoch: 000789  loss: -0.20938\n",
      "epoch: 000790  loss: -0.22366\n",
      "epoch: 000791  loss: -0.20997\n",
      "epoch: 000792  loss: -0.20822\n",
      "epoch: 000793  loss: -0.21853\n",
      "epoch: 000794  loss: -0.19476\n",
      "epoch: 000795  loss: -0.20775\n",
      "epoch: 000796  loss: -0.21332\n",
      "epoch: 000797  loss: -0.19569\n",
      "epoch: 000798  loss: -0.22034\n",
      "epoch: 000799  loss: -0.23470\n",
      "epoch: 000800  loss: -0.19705\n",
      "epoch: 000801  loss: -0.19719\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.433333333333334 2.9499999999999997 0.6520207849607131 0.33305784978571207\n",
      "epoch: 000802  loss: -0.20496\n",
      "epoch: 000803  loss: -0.21050\n",
      "epoch: 000804  loss: -0.23536\n",
      "epoch: 000805  loss: -0.19285\n",
      "epoch: 000806  loss: -0.20677\n",
      "epoch: 000807  loss: -0.19502\n",
      "epoch: 000808  loss: -0.21262\n",
      "epoch: 000809  loss: -0.21135\n",
      "epoch: 000810  loss: -0.20289\n",
      "epoch: 000811  loss: -0.21302\n",
      "epoch: 000812  loss: -0.22279\n",
      "epoch: 000813  loss: -0.21424\n",
      "epoch: 000814  loss: -0.20129\n",
      "epoch: 000815  loss: -0.21156\n",
      "epoch: 000816  loss: -0.20534\n",
      "epoch: 000817  loss: -0.21323\n",
      "epoch: 000818  loss: -0.18969\n",
      "epoch: 000819  loss: -0.20379\n",
      "epoch: 000820  loss: -0.22279\n",
      "epoch: 000821  loss: -0.20704\n",
      "epoch: 000822  loss: -0.20099\n",
      "epoch: 000823  loss: -0.22350\n",
      "epoch: 000824  loss: -0.18936\n",
      "epoch: 000825  loss: -0.20737\n",
      "epoch: 000826  loss: -0.20745\n",
      "epoch: 000827  loss: -0.20243\n",
      "epoch: 000828  loss: -0.19251\n",
      "epoch: 000829  loss: -0.22624\n",
      "epoch: 000830  loss: -0.21588\n",
      "epoch: 000831  loss: -0.21688\n",
      "epoch: 000832  loss: -0.18327\n",
      "epoch: 000833  loss: -0.19347\n",
      "epoch: 000834  loss: -0.19780\n",
      "epoch: 000835  loss: -0.21726\n",
      "epoch: 000836  loss: -0.21298\n",
      "epoch: 000837  loss: -0.21050\n",
      "epoch: 000838  loss: -0.21166\n",
      "epoch: 000839  loss: -0.22098\n",
      "epoch: 000840  loss: -0.19366\n",
      "epoch: 000841  loss: -0.19462\n",
      "epoch: 000842  loss: -0.21182\n",
      "epoch: 000843  loss: -0.19177\n",
      "epoch: 000844  loss: -0.21829\n",
      "epoch: 000845  loss: -0.21513\n",
      "epoch: 000846  loss: -0.19850\n",
      "epoch: 000847  loss: -0.21466\n",
      "epoch: 000848  loss: -0.20198\n",
      "epoch: 000849  loss: -0.21725\n",
      "epoch: 000850  loss: -0.22330\n",
      "epoch: 000851  loss: -0.22453\n",
      "epoch: 000852  loss: -0.19469\n",
      "epoch: 000853  loss: -0.19897\n",
      "epoch: 000854  loss: -0.20740\n",
      "epoch: 000855  loss: -0.21687\n",
      "epoch: 000856  loss: -0.22317\n",
      "epoch: 000857  loss: -0.20018\n",
      "epoch: 000858  loss: -0.19279\n",
      "epoch: 000859  loss: -0.22260\n",
      "epoch: 000860  loss: -0.21568\n",
      "epoch: 000861  loss: -0.20061\n",
      "epoch: 000862  loss: -0.19541\n",
      "epoch: 000863  loss: -0.20851\n",
      "epoch: 000864  loss: -0.20681\n",
      "epoch: 000865  loss: -0.21107\n",
      "epoch: 000866  loss: -0.22369\n",
      "epoch: 000867  loss: -0.17579\n",
      "epoch: 000868  loss: -0.20786\n",
      "epoch: 000869  loss: -0.21372\n",
      "epoch: 000870  loss: -0.21318\n",
      "epoch: 000871  loss: -0.22422\n",
      "epoch: 000872  loss: -0.20126\n",
      "epoch: 000873  loss: -0.19050\n",
      "epoch: 000874  loss: -0.18880\n",
      "epoch: 000875  loss: -0.21460\n",
      "epoch: 000876  loss: -0.19214\n",
      "epoch: 000877  loss: -0.20117\n",
      "epoch: 000878  loss: -0.20922\n",
      "epoch: 000879  loss: -0.20565\n",
      "epoch: 000880  loss: -0.20252\n",
      "epoch: 000881  loss: -0.19195\n",
      "epoch: 000882  loss: -0.20987\n",
      "epoch: 000883  loss: -0.22558\n",
      "epoch: 000884  loss: -0.20695\n",
      "epoch: 000885  loss: -0.19173\n",
      "epoch: 000886  loss: -0.20134\n",
      "epoch: 000887  loss: -0.18778\n",
      "epoch: 000888  loss: -0.18869\n",
      "epoch: 000889  loss: -0.21956\n",
      "epoch: 000890  loss: -0.18230\n",
      "epoch: 000891  loss: -0.22330\n",
      "epoch: 000892  loss: -0.21094\n",
      "epoch: 000893  loss: -0.19812\n",
      "epoch: 000894  loss: -0.21831\n",
      "epoch: 000895  loss: -0.20517\n",
      "epoch: 000896  loss: -0.17084\n",
      "epoch: 000897  loss: -0.20067\n",
      "epoch: 000898  loss: -0.19872\n",
      "epoch: 000899  loss: -0.18252\n",
      "epoch: 000900  loss: -0.19789\n",
      "epoch: 000901  loss: -0.20623\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.549999999999999 2.308333333333333 0.6109905866460397 0.3330612361092399\n",
      "epoch: 000902  loss: -0.20630\n",
      "epoch: 000903  loss: -0.18049\n",
      "epoch: 000904  loss: -0.20286\n",
      "epoch: 000905  loss: -0.18848\n",
      "epoch: 000906  loss: -0.18551\n",
      "epoch: 000907  loss: -0.19969\n",
      "epoch: 000908  loss: -0.18923\n",
      "epoch: 000909  loss: -0.19737\n",
      "epoch: 000910  loss: -0.20508\n",
      "epoch: 000911  loss: -0.19494\n",
      "epoch: 000912  loss: -0.18729\n",
      "epoch: 000913  loss: -0.19236\n",
      "epoch: 000914  loss: -0.20511\n",
      "epoch: 000915  loss: -0.19913\n",
      "epoch: 000916  loss: -0.20767\n",
      "epoch: 000917  loss: -0.22077\n",
      "epoch: 000918  loss: -0.19649\n",
      "epoch: 000919  loss: -0.20216\n",
      "epoch: 000920  loss: -0.23093\n",
      "epoch: 000921  loss: -0.21433\n",
      "epoch: 000922  loss: -0.18209\n",
      "epoch: 000923  loss: -0.18029\n",
      "epoch: 000924  loss: -0.21887\n",
      "epoch: 000925  loss: -0.18516\n",
      "epoch: 000926  loss: -0.20280\n",
      "epoch: 000927  loss: -0.19391\n",
      "epoch: 000928  loss: -0.22112\n",
      "epoch: 000929  loss: -0.19369\n",
      "epoch: 000930  loss: -0.20153\n",
      "epoch: 000931  loss: -0.20107\n",
      "epoch: 000932  loss: -0.19415\n",
      "epoch: 000933  loss: -0.19322\n",
      "epoch: 000934  loss: -0.22561\n",
      "epoch: 000935  loss: -0.20861\n",
      "epoch: 000936  loss: -0.21758\n",
      "epoch: 000937  loss: -0.19536\n",
      "epoch: 000938  loss: -0.20495\n",
      "epoch: 000939  loss: -0.19019\n",
      "epoch: 000940  loss: -0.20081\n",
      "epoch: 000941  loss: -0.18107\n",
      "epoch: 000942  loss: -0.19082\n",
      "epoch: 000943  loss: -0.19685\n",
      "epoch: 000944  loss: -0.21201\n",
      "epoch: 000945  loss: -0.21321\n",
      "epoch: 000946  loss: -0.19489\n",
      "epoch: 000947  loss: -0.22226\n",
      "epoch: 000948  loss: -0.20299\n",
      "epoch: 000949  loss: -0.21498\n",
      "epoch: 000950  loss: -0.21312\n",
      "epoch: 000951  loss: -0.21501\n",
      "epoch: 000952  loss: -0.20256\n",
      "epoch: 000953  loss: -0.19720\n",
      "epoch: 000954  loss: -0.21445\n",
      "epoch: 000955  loss: -0.21901\n",
      "epoch: 000956  loss: -0.19250\n",
      "epoch: 000957  loss: -0.22858\n",
      "epoch: 000958  loss: -0.20701\n",
      "epoch: 000959  loss: -0.21785\n",
      "epoch: 000960  loss: -0.21732\n",
      "epoch: 000961  loss: -0.19980\n",
      "epoch: 000962  loss: -0.18818\n",
      "epoch: 000963  loss: -0.21266\n",
      "epoch: 000964  loss: -0.21012\n",
      "epoch: 000965  loss: -0.21251\n",
      "epoch: 000966  loss: -0.21108\n",
      "epoch: 000967  loss: -0.22550\n",
      "epoch: 000968  loss: -0.20908\n",
      "epoch: 000969  loss: -0.18569\n",
      "epoch: 000970  loss: -0.20825\n",
      "epoch: 000971  loss: -0.19665\n",
      "epoch: 000972  loss: -0.20896\n",
      "epoch: 000973  loss: -0.20865\n",
      "epoch: 000974  loss: -0.20171\n",
      "epoch: 000975  loss: -0.20271\n",
      "epoch: 000976  loss: -0.18708\n",
      "epoch: 000977  loss: -0.20732\n",
      "epoch: 000978  loss: -0.18877\n",
      "epoch: 000979  loss: -0.22640\n",
      "epoch: 000980  loss: -0.21659\n",
      "epoch: 000981  loss: -0.20210\n",
      "epoch: 000982  loss: -0.20468\n",
      "epoch: 000983  loss: -0.21038\n",
      "epoch: 000984  loss: -0.21504\n",
      "epoch: 000985  loss: -0.21854\n",
      "epoch: 000986  loss: -0.22507\n",
      "epoch: 000987  loss: -0.21881\n",
      "epoch: 000988  loss: -0.21195\n",
      "epoch: 000989  loss: -0.20812\n",
      "epoch: 000990  loss: -0.19724\n",
      "epoch: 000991  loss: -0.18025\n",
      "epoch: 000992  loss: -0.20362\n",
      "epoch: 000993  loss: -0.19689\n",
      "epoch: 000994  loss: -0.21231\n",
      "epoch: 000995  loss: -0.21038\n",
      "epoch: 000996  loss: -0.18406\n",
      "epoch: 000997  loss: -0.21399\n",
      "epoch: 000998  loss: -0.19653\n",
      "epoch: 000999  loss: -0.19453\n",
      "epoch: 001000  loss: -0.18423\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.7666666666666675 2.0444444444444443 0.7041357225324696 0.33314580951276784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>graph-unmasking/gen-loss</td><td></td></tr><tr><td>graph-unmasking/loss</td><td></td></tr><tr><td>mean degree</td><td></td></tr><tr><td>mean degree var</td><td></td></tr><tr><td>num_parameters</td><td></td></tr><tr><td>stats/cluster</td><td></td></tr><tr><td>stats/cluster_train</td><td></td></tr><tr><td>stats/degree</td><td></td></tr><tr><td>stats/degree_train</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>graph-unmasking/gen-loss</td><td>9.69889</td></tr><tr><td>graph-unmasking/loss</td><td>-0.18423</td></tr><tr><td>mean degree</td><td>5.76667</td></tr><tr><td>mean degree var</td><td>2.04444</td></tr><tr><td>num_parameters</td><td>20573</td></tr><tr><td>stats/cluster</td><td>0.33315</td></tr><tr><td>stats/cluster_train</td><td>0.00086</td></tr><tr><td>stats/degree</td><td>0.70414</td></tr><tr><td>stats/degree_train</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-1</strong> at: <a href='https://wandb.ai/nextaid/toad_retry/runs/c2fn7rx5' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/runs/c2fn7rx5</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230605_155012-c2fn7rx5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cnvysh45 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcandidate_selection_radius: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdouble_inference: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgraph_transform: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: bce\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: pna\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_probability: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_post_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_pass: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttowers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gerritgrossmann/Documents/devel/MaskedGraphGen/wandb/run-20230605_161759-cnvysh45</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nextaid/toad_retry/runs/cnvysh45' target=\"_blank\">glamorous-sweep-2</a></strong> to <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">https://wandb.ai/nextaid/toad_retry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nextaid/toad_retry/runs/cnvysh45' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/runs/cnvysh45</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8k6n2wdp {'batch_size': 500, 'candidate_selection_radius': 1, 'double_inference': False, 'dropout': 0, 'graph_transform': False, 'hidden_dim': 16, 'hidden_layer': 5, 'learning_rate': 0.0001, 'loss': 'bce', 'model': 'pna', 'noise_probability': 0.05, 'normalization': False, 'pre_post_layers': 1, 'single_pass': True, 'towers': 1}\n",
      "Reference distance is:  0.0 0.0008565843180121657\n",
      "epoch: 000001  loss: -0.01059\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.1 2.2222222222222223 0.5705763933136443 0.3330456981745606\n",
      "epoch: 000002  loss: -0.01486\n",
      "epoch: 000003  loss: -0.02451\n",
      "epoch: 000004  loss: -0.02832\n",
      "epoch: 000005  loss: -0.03073\n",
      "epoch: 000006  loss: -0.03981\n",
      "epoch: 000007  loss: -0.04311\n",
      "epoch: 000008  loss: -0.05736\n",
      "epoch: 000009  loss: -0.06487\n",
      "epoch: 000010  loss: -0.08311\n",
      "epoch: 000011  loss: -0.08193\n",
      "epoch: 000012  loss: -0.09351\n",
      "epoch: 000013  loss: -0.09443\n",
      "epoch: 000014  loss: -0.12871\n",
      "epoch: 000015  loss: -0.12571\n",
      "epoch: 000016  loss: -0.14511\n",
      "epoch: 000017  loss: -0.15394\n",
      "epoch: 000018  loss: -0.18532\n",
      "epoch: 000019  loss: -0.20397\n",
      "epoch: 000020  loss: -0.21303\n",
      "epoch: 000021  loss: -0.21830\n",
      "epoch: 000022  loss: -0.23779\n",
      "epoch: 000023  loss: -0.20386\n",
      "epoch: 000024  loss: -0.21279\n",
      "epoch: 000025  loss: -0.21010\n",
      "epoch: 000026  loss: -0.21083\n",
      "epoch: 000027  loss: -0.20531\n",
      "epoch: 000028  loss: -0.17976\n",
      "epoch: 000029  loss: -0.17875\n",
      "epoch: 000030  loss: -0.17879\n",
      "epoch: 000031  loss: -0.18602\n",
      "epoch: 000032  loss: -0.19327\n",
      "epoch: 000033  loss: -0.21127\n",
      "epoch: 000034  loss: -0.21035\n",
      "epoch: 000035  loss: -0.21067\n",
      "epoch: 000036  loss: -0.23742\n",
      "epoch: 000037  loss: -0.18695\n",
      "epoch: 000038  loss: -0.21058\n",
      "epoch: 000039  loss: -0.22075\n",
      "epoch: 000040  loss: -0.20812\n",
      "epoch: 000041  loss: -0.18527\n",
      "epoch: 000042  loss: -0.19567\n",
      "epoch: 000043  loss: -0.19645\n",
      "epoch: 000044  loss: -0.21825\n",
      "epoch: 000045  loss: -0.21318\n",
      "epoch: 000046  loss: -0.20281\n",
      "epoch: 000047  loss: -0.18451\n",
      "epoch: 000048  loss: -0.20670\n",
      "epoch: 000049  loss: -0.21966\n",
      "epoch: 000050  loss: -0.21959\n",
      "epoch: 000051  loss: -0.20557\n",
      "epoch: 000052  loss: -0.21867\n",
      "epoch: 000053  loss: -0.19916\n",
      "epoch: 000054  loss: -0.19030\n",
      "epoch: 000055  loss: -0.20469\n",
      "epoch: 000056  loss: -0.18408\n",
      "epoch: 000057  loss: -0.18448\n",
      "epoch: 000058  loss: -0.20261\n",
      "epoch: 000059  loss: -0.18152\n",
      "epoch: 000060  loss: -0.20256\n",
      "epoch: 000061  loss: -0.18617\n",
      "epoch: 000062  loss: -0.21858\n",
      "epoch: 000063  loss: -0.20239\n",
      "epoch: 000064  loss: -0.21823\n",
      "epoch: 000065  loss: -0.23363\n",
      "epoch: 000066  loss: -0.21359\n",
      "epoch: 000067  loss: -0.23701\n",
      "epoch: 000068  loss: -0.26001\n",
      "epoch: 000069  loss: -0.23130\n",
      "epoch: 000070  loss: -0.20937\n",
      "epoch: 000071  loss: -0.21757\n",
      "epoch: 000072  loss: -0.19649\n",
      "epoch: 000073  loss: -0.18261\n",
      "epoch: 000074  loss: -0.17195\n",
      "epoch: 000075  loss: -0.17051\n",
      "epoch: 000076  loss: -0.17119\n",
      "epoch: 000077  loss: -0.18918\n",
      "epoch: 000078  loss: -0.17630\n",
      "epoch: 000079  loss: -0.17276\n",
      "epoch: 000080  loss: -0.19606\n",
      "epoch: 000081  loss: -0.18056\n",
      "epoch: 000082  loss: -0.19618\n",
      "epoch: 000083  loss: -0.18508\n",
      "epoch: 000084  loss: -0.18015\n",
      "epoch: 000085  loss: -0.19616\n",
      "epoch: 000086  loss: -0.19810\n",
      "epoch: 000087  loss: -0.21486\n",
      "epoch: 000088  loss: -0.21483\n",
      "epoch: 000089  loss: -0.21549\n",
      "epoch: 000090  loss: -0.23647\n",
      "epoch: 000091  loss: -0.20640\n",
      "epoch: 000092  loss: -0.22530\n",
      "epoch: 000093  loss: -0.22021\n",
      "epoch: 000094  loss: -0.24242\n",
      "epoch: 000095  loss: -0.21475\n",
      "epoch: 000096  loss: -0.19713\n",
      "epoch: 000097  loss: -0.21277\n",
      "epoch: 000098  loss: -0.18740\n",
      "epoch: 000099  loss: -0.20350\n",
      "epoch: 000100  loss: -0.21179\n",
      "epoch: 000101  loss: -0.19977\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.366666666666666 2.3833333333333337 0.5670998979236679 0.3330572259486399\n",
      "epoch: 000102  loss: -0.19852\n",
      "epoch: 000103  loss: -0.20950\n",
      "epoch: 000104  loss: -0.22180\n",
      "epoch: 000105  loss: -0.21738\n",
      "epoch: 000106  loss: -0.20916\n",
      "epoch: 000107  loss: -0.20623\n",
      "epoch: 000108  loss: -0.20798\n",
      "epoch: 000109  loss: -0.18201\n",
      "epoch: 000110  loss: -0.20725\n",
      "epoch: 000111  loss: -0.22755\n",
      "epoch: 000112  loss: -0.20005\n",
      "epoch: 000113  loss: -0.22799\n",
      "epoch: 000114  loss: -0.19136\n",
      "epoch: 000115  loss: -0.22733\n",
      "epoch: 000116  loss: -0.21740\n",
      "epoch: 000117  loss: -0.22589\n",
      "epoch: 000118  loss: -0.20200\n",
      "epoch: 000119  loss: -0.20935\n",
      "epoch: 000120  loss: -0.20904\n",
      "epoch: 000121  loss: -0.20867\n",
      "epoch: 000122  loss: -0.19801\n",
      "epoch: 000123  loss: -0.20445\n",
      "epoch: 000124  loss: -0.22210\n",
      "epoch: 000125  loss: -0.20835\n",
      "epoch: 000126  loss: -0.23722\n",
      "epoch: 000127  loss: -0.21343\n",
      "epoch: 000128  loss: -0.22219\n",
      "epoch: 000129  loss: -0.21524\n",
      "epoch: 000130  loss: -0.21476\n",
      "epoch: 000131  loss: -0.22967\n",
      "epoch: 000132  loss: -0.20825\n",
      "epoch: 000133  loss: -0.20653\n",
      "epoch: 000134  loss: -0.20669\n",
      "epoch: 000135  loss: -0.18731\n",
      "epoch: 000136  loss: -0.18571\n",
      "epoch: 000137  loss: -0.20239\n",
      "epoch: 000138  loss: -0.19876\n",
      "epoch: 000139  loss: -0.20052\n",
      "epoch: 000140  loss: -0.19429\n",
      "epoch: 000141  loss: -0.21081\n",
      "epoch: 000142  loss: -0.20457\n",
      "epoch: 000143  loss: -0.17595\n",
      "epoch: 000144  loss: -0.18956\n",
      "epoch: 000145  loss: -0.20882\n",
      "epoch: 000146  loss: -0.19220\n",
      "epoch: 000147  loss: -0.21440\n",
      "epoch: 000148  loss: -0.18436\n",
      "epoch: 000149  loss: -0.20110\n",
      "epoch: 000150  loss: -0.21153\n",
      "epoch: 000151  loss: -0.18036\n",
      "epoch: 000152  loss: -0.23000\n",
      "epoch: 000153  loss: -0.22270\n",
      "epoch: 000154  loss: -0.22511\n",
      "epoch: 000155  loss: -0.20840\n",
      "epoch: 000156  loss: -0.23039\n",
      "epoch: 000157  loss: -0.23511\n",
      "epoch: 000158  loss: -0.23317\n",
      "epoch: 000159  loss: -0.21362\n",
      "epoch: 000160  loss: -0.23277\n",
      "epoch: 000161  loss: -0.20622\n",
      "epoch: 000162  loss: -0.18816\n",
      "epoch: 000163  loss: -0.18981\n",
      "epoch: 000164  loss: -0.20496\n",
      "epoch: 000165  loss: -0.19132\n",
      "epoch: 000166  loss: -0.19314\n",
      "epoch: 000167  loss: -0.19443\n",
      "epoch: 000168  loss: -0.18865\n",
      "epoch: 000169  loss: -0.18926\n",
      "epoch: 000170  loss: -0.17140\n",
      "epoch: 000171  loss: -0.19645\n",
      "epoch: 000172  loss: -0.16494\n",
      "epoch: 000173  loss: -0.20795\n",
      "epoch: 000174  loss: -0.19852\n",
      "epoch: 000175  loss: -0.20074\n",
      "epoch: 000176  loss: -0.20749\n",
      "epoch: 000177  loss: -0.18805\n",
      "epoch: 000178  loss: -0.20342\n",
      "epoch: 000179  loss: -0.18442\n",
      "epoch: 000180  loss: -0.18108\n",
      "epoch: 000181  loss: -0.20003\n",
      "epoch: 000182  loss: -0.19649\n",
      "epoch: 000183  loss: -0.20310\n",
      "epoch: 000184  loss: -0.21132\n",
      "epoch: 000185  loss: -0.19191\n",
      "epoch: 000186  loss: -0.21898\n",
      "epoch: 000187  loss: -0.20047\n",
      "epoch: 000188  loss: -0.18223\n",
      "epoch: 000189  loss: -0.20556\n",
      "epoch: 000190  loss: -0.20431\n",
      "epoch: 000191  loss: -0.18623\n",
      "epoch: 000192  loss: -0.18286\n",
      "epoch: 000193  loss: -0.21459\n",
      "epoch: 000194  loss: -0.21973\n",
      "epoch: 000195  loss: -0.20358\n",
      "epoch: 000196  loss: -0.23319\n",
      "epoch: 000197  loss: -0.24049\n",
      "epoch: 000198  loss: -0.20972\n",
      "epoch: 000199  loss: -0.23069\n",
      "epoch: 000200  loss: -0.22120\n",
      "epoch: 000201  loss: -0.22802\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.533333333333333 2.3722222222222227 0.6247178159846185 0.3382945025163926\n",
      "epoch: 000202  loss: -0.20028\n",
      "epoch: 000203  loss: -0.24827\n",
      "epoch: 000204  loss: -0.23293\n",
      "epoch: 000205  loss: -0.22324\n",
      "epoch: 000206  loss: -0.22614\n",
      "epoch: 000207  loss: -0.21447\n",
      "epoch: 000208  loss: -0.20215\n",
      "epoch: 000209  loss: -0.20077\n",
      "epoch: 000210  loss: -0.17234\n",
      "epoch: 000211  loss: -0.19775\n",
      "epoch: 000212  loss: -0.20303\n",
      "epoch: 000213  loss: -0.19913\n",
      "epoch: 000214  loss: -0.18719\n",
      "epoch: 000215  loss: -0.21530\n",
      "epoch: 000216  loss: -0.20926\n",
      "epoch: 000217  loss: -0.22350\n",
      "epoch: 000218  loss: -0.21402\n",
      "epoch: 000219  loss: -0.19115\n",
      "epoch: 000220  loss: -0.20871\n",
      "epoch: 000221  loss: -0.19974\n",
      "epoch: 000222  loss: -0.18859\n",
      "epoch: 000223  loss: -0.19424\n",
      "epoch: 000224  loss: -0.19043\n",
      "epoch: 000225  loss: -0.18649\n",
      "epoch: 000226  loss: -0.17944\n",
      "epoch: 000227  loss: -0.19274\n",
      "epoch: 000228  loss: -0.18490\n",
      "epoch: 000229  loss: -0.19782\n",
      "epoch: 000230  loss: -0.21200\n",
      "epoch: 000231  loss: -0.22143\n",
      "epoch: 000232  loss: -0.19833\n",
      "epoch: 000233  loss: -0.19957\n",
      "epoch: 000234  loss: -0.20536\n",
      "epoch: 000235  loss: -0.22072\n",
      "epoch: 000236  loss: -0.21281\n",
      "epoch: 000237  loss: -0.21251\n",
      "epoch: 000238  loss: -0.20287\n",
      "epoch: 000239  loss: -0.18989\n",
      "epoch: 000240  loss: -0.17228\n",
      "epoch: 000241  loss: -0.19043\n",
      "epoch: 000242  loss: -0.20047\n",
      "epoch: 000243  loss: -0.19264\n",
      "epoch: 000244  loss: -0.18557\n",
      "epoch: 000245  loss: -0.21672\n",
      "epoch: 000246  loss: -0.21942\n",
      "epoch: 000247  loss: -0.18257\n",
      "epoch: 000248  loss: -0.22210\n",
      "epoch: 000249  loss: -0.19670\n",
      "epoch: 000250  loss: -0.22073\n",
      "epoch: 000251  loss: -0.19794\n",
      "epoch: 000252  loss: -0.18941\n",
      "epoch: 000253  loss: -0.21477\n",
      "epoch: 000254  loss: -0.20008\n",
      "epoch: 000255  loss: -0.19884\n",
      "epoch: 000256  loss: -0.19392\n",
      "epoch: 000257  loss: -0.17843\n",
      "epoch: 000258  loss: -0.20063\n",
      "epoch: 000259  loss: -0.20371\n",
      "epoch: 000260  loss: -0.19908\n",
      "epoch: 000261  loss: -0.21370\n",
      "epoch: 000262  loss: -0.20403\n",
      "epoch: 000263  loss: -0.19227\n",
      "epoch: 000264  loss: -0.17544\n",
      "epoch: 000265  loss: -0.20420\n",
      "epoch: 000266  loss: -0.19813\n",
      "epoch: 000267  loss: -0.20625\n",
      "epoch: 000268  loss: -0.19755\n",
      "epoch: 000269  loss: -0.21462\n",
      "epoch: 000270  loss: -0.21692\n",
      "epoch: 000271  loss: -0.19890\n",
      "epoch: 000272  loss: -0.19725\n",
      "epoch: 000273  loss: -0.22255\n",
      "epoch: 000274  loss: -0.20441\n",
      "epoch: 000275  loss: -0.21482\n",
      "epoch: 000276  loss: -0.21980\n",
      "epoch: 000277  loss: -0.20601\n",
      "epoch: 000278  loss: -0.19214\n",
      "epoch: 000279  loss: -0.20629\n",
      "epoch: 000280  loss: -0.21532\n",
      "epoch: 000281  loss: -0.17595\n",
      "epoch: 000282  loss: -0.19786\n",
      "epoch: 000283  loss: -0.21085\n",
      "epoch: 000284  loss: -0.20292\n",
      "epoch: 000285  loss: -0.19897\n",
      "epoch: 000286  loss: -0.21062\n",
      "epoch: 000287  loss: -0.19835\n",
      "epoch: 000288  loss: -0.18760\n",
      "epoch: 000289  loss: -0.20998\n",
      "epoch: 000290  loss: -0.18338\n",
      "epoch: 000291  loss: -0.19771\n",
      "epoch: 000292  loss: -0.21016\n",
      "epoch: 000293  loss: -0.21230\n",
      "epoch: 000294  loss: -0.21881\n",
      "epoch: 000295  loss: -0.22042\n",
      "epoch: 000296  loss: -0.19612\n",
      "epoch: 000297  loss: -0.23494\n",
      "epoch: 000298  loss: -0.21192\n",
      "epoch: 000299  loss: -0.21569\n",
      "epoch: 000300  loss: -0.19477\n",
      "epoch: 000301  loss: -0.19646\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.333333333333333 2.138888888888889 0.6539186678834374 0.3330684142280654\n",
      "epoch: 000302  loss: -0.21276\n",
      "epoch: 000303  loss: -0.18840\n",
      "epoch: 000304  loss: -0.20282\n",
      "epoch: 000305  loss: -0.19465\n",
      "epoch: 000306  loss: -0.19070\n",
      "epoch: 000307  loss: -0.18710\n",
      "epoch: 000308  loss: -0.20021\n",
      "epoch: 000309  loss: -0.16411\n",
      "epoch: 000310  loss: -0.19829\n",
      "epoch: 000311  loss: -0.17575\n",
      "epoch: 000312  loss: -0.20747\n",
      "epoch: 000313  loss: -0.20537\n",
      "epoch: 000314  loss: -0.20273\n",
      "epoch: 000315  loss: -0.20878\n",
      "epoch: 000316  loss: -0.22009\n",
      "epoch: 000317  loss: -0.24217\n",
      "epoch: 000318  loss: -0.24166\n",
      "epoch: 000319  loss: -0.19334\n",
      "epoch: 000320  loss: -0.20239\n",
      "epoch: 000321  loss: -0.18206\n",
      "epoch: 000322  loss: -0.19726\n",
      "epoch: 000323  loss: -0.20940\n",
      "epoch: 000324  loss: -0.21075\n",
      "epoch: 000325  loss: -0.19846\n",
      "epoch: 000326  loss: -0.23192\n",
      "epoch: 000327  loss: -0.21774\n",
      "epoch: 000328  loss: -0.22023\n",
      "epoch: 000329  loss: -0.22933\n",
      "epoch: 000330  loss: -0.22220\n",
      "epoch: 000331  loss: -0.21388\n",
      "epoch: 000332  loss: -0.22715\n",
      "epoch: 000333  loss: -0.21644\n",
      "epoch: 000334  loss: -0.20740\n",
      "epoch: 000335  loss: -0.22322\n",
      "epoch: 000336  loss: -0.23877\n",
      "epoch: 000337  loss: -0.20297\n",
      "epoch: 000338  loss: -0.21084\n",
      "epoch: 000339  loss: -0.22465\n",
      "epoch: 000340  loss: -0.20811\n",
      "epoch: 000341  loss: -0.23073\n",
      "epoch: 000342  loss: -0.20780\n",
      "epoch: 000343  loss: -0.23566\n",
      "epoch: 000344  loss: -0.22255\n",
      "epoch: 000345  loss: -0.22635\n",
      "epoch: 000346  loss: -0.20352\n",
      "epoch: 000347  loss: -0.23113\n",
      "epoch: 000348  loss: -0.21435\n",
      "epoch: 000349  loss: -0.21357\n",
      "epoch: 000350  loss: -0.20221\n",
      "epoch: 000351  loss: -0.20047\n",
      "epoch: 000352  loss: -0.22147\n",
      "epoch: 000353  loss: -0.21553\n",
      "epoch: 000354  loss: -0.20662\n",
      "epoch: 000355  loss: -0.21853\n",
      "epoch: 000356  loss: -0.21506\n",
      "epoch: 000357  loss: -0.21930\n",
      "epoch: 000358  loss: -0.23112\n",
      "epoch: 000359  loss: -0.19866\n",
      "epoch: 000360  loss: -0.21546\n",
      "epoch: 000361  loss: -0.21096\n",
      "epoch: 000362  loss: -0.21573\n",
      "epoch: 000363  loss: -0.21512\n",
      "epoch: 000364  loss: -0.23353\n",
      "epoch: 000365  loss: -0.20550\n",
      "epoch: 000366  loss: -0.21345\n",
      "epoch: 000367  loss: -0.22866\n",
      "epoch: 000368  loss: -0.21726\n",
      "epoch: 000369  loss: -0.22452\n",
      "epoch: 000370  loss: -0.19648\n",
      "epoch: 000371  loss: -0.19197\n",
      "epoch: 000372  loss: -0.17296\n",
      "epoch: 000373  loss: -0.19428\n",
      "epoch: 000374  loss: -0.19495\n",
      "epoch: 000375  loss: -0.21183\n",
      "epoch: 000376  loss: -0.18433\n",
      "epoch: 000377  loss: -0.21165\n",
      "epoch: 000378  loss: -0.19088\n",
      "epoch: 000379  loss: -0.18484\n",
      "epoch: 000380  loss: -0.22004\n",
      "epoch: 000381  loss: -0.20917\n",
      "epoch: 000382  loss: -0.19680\n",
      "epoch: 000383  loss: -0.19788\n",
      "epoch: 000384  loss: -0.21470\n",
      "epoch: 000385  loss: -0.21345\n",
      "epoch: 000386  loss: -0.22279\n",
      "epoch: 000387  loss: -0.23051\n",
      "epoch: 000388  loss: -0.17899\n",
      "epoch: 000389  loss: -0.21748\n",
      "epoch: 000390  loss: -0.21286\n",
      "epoch: 000391  loss: -0.21889\n",
      "epoch: 000392  loss: -0.19280\n",
      "epoch: 000393  loss: -0.20303\n",
      "epoch: 000394  loss: -0.20732\n",
      "epoch: 000395  loss: -0.18289\n",
      "epoch: 000396  loss: -0.19095\n",
      "epoch: 000397  loss: -0.19495\n",
      "epoch: 000398  loss: -0.21680\n",
      "epoch: 000399  loss: -0.18759\n",
      "epoch: 000400  loss: -0.19633\n",
      "epoch: 000401  loss: -0.22558\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.216666666666667 2.452777777777778 0.5838410402431311 0.3339604668252455\n",
      "epoch: 000402  loss: -0.20393\n",
      "epoch: 000403  loss: -0.19199\n",
      "epoch: 000404  loss: -0.22748\n",
      "epoch: 000405  loss: -0.21852\n",
      "epoch: 000406  loss: -0.20066\n",
      "epoch: 000407  loss: -0.20677\n",
      "epoch: 000408  loss: -0.20750\n",
      "epoch: 000409  loss: -0.21643\n",
      "epoch: 000410  loss: -0.19636\n",
      "epoch: 000411  loss: -0.24290\n",
      "epoch: 000412  loss: -0.19551\n",
      "epoch: 000413  loss: -0.18116\n",
      "epoch: 000414  loss: -0.18821\n",
      "epoch: 000415  loss: -0.18590\n",
      "epoch: 000416  loss: -0.20497\n",
      "epoch: 000417  loss: -0.17755\n",
      "epoch: 000418  loss: -0.18334\n",
      "epoch: 000419  loss: -0.18832\n",
      "epoch: 000420  loss: -0.21393\n",
      "epoch: 000421  loss: -0.20884\n",
      "epoch: 000422  loss: -0.22293\n",
      "epoch: 000423  loss: -0.26118\n",
      "epoch: 000424  loss: -0.22185\n",
      "epoch: 000425  loss: -0.23247\n",
      "epoch: 000426  loss: -0.23451\n",
      "epoch: 000427  loss: -0.21251\n",
      "epoch: 000428  loss: -0.19351\n",
      "epoch: 000429  loss: -0.20717\n",
      "epoch: 000430  loss: -0.19978\n",
      "epoch: 000431  loss: -0.21989\n",
      "epoch: 000432  loss: -0.19195\n",
      "epoch: 000433  loss: -0.18848\n",
      "epoch: 000434  loss: -0.19300\n",
      "epoch: 000435  loss: -0.21607\n",
      "epoch: 000436  loss: -0.20460\n",
      "epoch: 000437  loss: -0.20986\n",
      "epoch: 000438  loss: -0.21722\n",
      "epoch: 000439  loss: -0.21504\n",
      "epoch: 000440  loss: -0.20781\n",
      "epoch: 000441  loss: -0.19163\n",
      "epoch: 000442  loss: -0.21692\n",
      "epoch: 000443  loss: -0.19899\n",
      "epoch: 000444  loss: -0.18427\n",
      "epoch: 000445  loss: -0.21020\n",
      "epoch: 000446  loss: -0.19441\n",
      "epoch: 000447  loss: -0.19679\n",
      "epoch: 000448  loss: -0.21187\n",
      "epoch: 000449  loss: -0.22020\n",
      "epoch: 000450  loss: -0.21469\n",
      "epoch: 000451  loss: -0.19949\n",
      "epoch: 000452  loss: -0.22196\n",
      "epoch: 000453  loss: -0.19363\n",
      "epoch: 000454  loss: -0.19658\n",
      "epoch: 000455  loss: -0.19483\n",
      "epoch: 000456  loss: -0.21833\n",
      "epoch: 000457  loss: -0.21320\n",
      "epoch: 000458  loss: -0.20698\n",
      "epoch: 000459  loss: -0.20966\n",
      "epoch: 000460  loss: -0.21557\n",
      "epoch: 000461  loss: -0.24339\n",
      "epoch: 000462  loss: -0.21374\n",
      "epoch: 000463  loss: -0.24917\n",
      "epoch: 000464  loss: -0.23623\n",
      "epoch: 000465  loss: -0.19719\n",
      "epoch: 000466  loss: -0.21803\n",
      "epoch: 000467  loss: -0.22359\n",
      "epoch: 000468  loss: -0.20722\n",
      "epoch: 000469  loss: -0.20190\n",
      "epoch: 000470  loss: -0.21682\n",
      "epoch: 000471  loss: -0.21154\n",
      "epoch: 000472  loss: -0.18447\n",
      "epoch: 000473  loss: -0.21177\n",
      "epoch: 000474  loss: -0.21013\n",
      "epoch: 000475  loss: -0.18576\n",
      "epoch: 000476  loss: -0.21781\n",
      "epoch: 000477  loss: -0.20053\n",
      "epoch: 000478  loss: -0.21837\n",
      "epoch: 000479  loss: -0.20624\n",
      "epoch: 000480  loss: -0.21349\n",
      "epoch: 000481  loss: -0.20968\n",
      "epoch: 000482  loss: -0.20112\n",
      "epoch: 000483  loss: -0.19676\n",
      "epoch: 000484  loss: -0.20620\n",
      "epoch: 000485  loss: -0.20118\n",
      "epoch: 000486  loss: -0.20263\n",
      "epoch: 000487  loss: -0.19860\n",
      "epoch: 000488  loss: -0.19033\n",
      "epoch: 000489  loss: -0.21735\n",
      "epoch: 000490  loss: -0.19481\n",
      "epoch: 000491  loss: -0.18087\n",
      "epoch: 000492  loss: -0.20972\n",
      "epoch: 000493  loss: -0.19212\n",
      "epoch: 000494  loss: -0.21179\n",
      "epoch: 000495  loss: -0.22288\n",
      "epoch: 000496  loss: -0.21438\n",
      "epoch: 000497  loss: -0.21040\n",
      "epoch: 000498  loss: -0.22278\n",
      "epoch: 000499  loss: -0.22257\n",
      "epoch: 000500  loss: -0.23747\n",
      "epoch: 000501  loss: -0.21860\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.366666666666665 2.477777777777778 0.5761981216152252 0.3330686189849192\n",
      "epoch: 000502  loss: -0.23253\n",
      "epoch: 000503  loss: -0.21289\n",
      "epoch: 000504  loss: -0.22187\n",
      "epoch: 000505  loss: -0.20479\n",
      "epoch: 000506  loss: -0.20283\n",
      "epoch: 000507  loss: -0.19310\n",
      "epoch: 000508  loss: -0.18892\n",
      "epoch: 000509  loss: -0.20409\n",
      "epoch: 000510  loss: -0.19634\n",
      "epoch: 000511  loss: -0.19759\n",
      "epoch: 000512  loss: -0.20206\n",
      "epoch: 000513  loss: -0.19367\n",
      "epoch: 000514  loss: -0.21472\n",
      "epoch: 000515  loss: -0.18617\n",
      "epoch: 000516  loss: -0.19652\n",
      "epoch: 000517  loss: -0.18883\n",
      "epoch: 000518  loss: -0.20048\n",
      "epoch: 000519  loss: -0.19588\n",
      "epoch: 000520  loss: -0.18566\n",
      "epoch: 000521  loss: -0.18814\n",
      "epoch: 000522  loss: -0.18849\n",
      "epoch: 000523  loss: -0.19121\n",
      "epoch: 000524  loss: -0.17976\n",
      "epoch: 000525  loss: -0.18654\n",
      "epoch: 000526  loss: -0.18970\n",
      "epoch: 000527  loss: -0.19609\n",
      "epoch: 000528  loss: -0.18522\n",
      "epoch: 000529  loss: -0.20770\n",
      "epoch: 000530  loss: -0.21239\n",
      "epoch: 000531  loss: -0.20213\n",
      "epoch: 000532  loss: -0.22684\n",
      "epoch: 000533  loss: -0.21859\n",
      "epoch: 000534  loss: -0.23499\n",
      "epoch: 000535  loss: -0.21607\n",
      "epoch: 000536  loss: -0.23592\n",
      "epoch: 000537  loss: -0.18951\n",
      "epoch: 000538  loss: -0.20706\n",
      "epoch: 000539  loss: -0.21124\n",
      "epoch: 000540  loss: -0.18416\n",
      "epoch: 000541  loss: -0.18539\n",
      "epoch: 000542  loss: -0.18771\n",
      "epoch: 000543  loss: -0.18313\n",
      "epoch: 000544  loss: -0.19288\n",
      "epoch: 000545  loss: -0.21160\n",
      "epoch: 000546  loss: -0.19008\n",
      "epoch: 000547  loss: -0.19758\n",
      "epoch: 000548  loss: -0.22400\n",
      "epoch: 000549  loss: -0.20826\n",
      "epoch: 000550  loss: -0.22207\n",
      "epoch: 000551  loss: -0.20412\n",
      "epoch: 000552  loss: -0.17767\n",
      "epoch: 000553  loss: -0.22768\n",
      "epoch: 000554  loss: -0.18888\n",
      "epoch: 000555  loss: -0.19047\n",
      "epoch: 000556  loss: -0.20236\n",
      "epoch: 000557  loss: -0.20843\n",
      "epoch: 000558  loss: -0.20057\n",
      "epoch: 000559  loss: -0.21524\n",
      "epoch: 000560  loss: -0.20977\n",
      "epoch: 000561  loss: -0.23329\n",
      "epoch: 000562  loss: -0.22977\n",
      "epoch: 000563  loss: -0.21284\n",
      "epoch: 000564  loss: -0.22095\n",
      "epoch: 000565  loss: -0.21327\n",
      "epoch: 000566  loss: -0.19477\n",
      "epoch: 000567  loss: -0.20455\n",
      "epoch: 000568  loss: -0.21110\n",
      "epoch: 000569  loss: -0.19855\n",
      "epoch: 000570  loss: -0.21610\n",
      "epoch: 000571  loss: -0.21448\n",
      "epoch: 000572  loss: -0.22746\n",
      "epoch: 000573  loss: -0.19656\n",
      "epoch: 000574  loss: -0.20969\n",
      "epoch: 000575  loss: -0.19957\n",
      "epoch: 000576  loss: -0.20948\n",
      "epoch: 000577  loss: -0.21639\n",
      "epoch: 000578  loss: -0.20943\n",
      "epoch: 000579  loss: -0.19626\n",
      "epoch: 000580  loss: -0.21875\n",
      "epoch: 000581  loss: -0.21131\n",
      "epoch: 000582  loss: -0.20009\n",
      "epoch: 000583  loss: -0.18253\n",
      "epoch: 000584  loss: -0.19962\n",
      "epoch: 000585  loss: -0.22208\n",
      "epoch: 000586  loss: -0.18965\n",
      "epoch: 000587  loss: -0.21419\n",
      "epoch: 000588  loss: -0.18312\n",
      "epoch: 000589  loss: -0.21757\n",
      "epoch: 000590  loss: -0.21722\n",
      "epoch: 000591  loss: -0.22185\n",
      "epoch: 000592  loss: -0.21396\n",
      "epoch: 000593  loss: -0.21450\n",
      "epoch: 000594  loss: -0.20602\n",
      "epoch: 000595  loss: -0.20498\n",
      "epoch: 000596  loss: -0.21048\n",
      "epoch: 000597  loss: -0.20093\n",
      "epoch: 000598  loss: -0.21225\n",
      "epoch: 000599  loss: -0.18524\n",
      "epoch: 000600  loss: -0.19505\n",
      "epoch: 000601  loss: -0.20823\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.3500000000000005 2.7972222222222225 0.5597616589084968 0.33321921710104846\n",
      "epoch: 000602  loss: -0.20174\n",
      "epoch: 000603  loss: -0.21092\n",
      "epoch: 000604  loss: -0.19545\n",
      "epoch: 000605  loss: -0.24335\n",
      "epoch: 000606  loss: -0.17785\n",
      "epoch: 000607  loss: -0.24217\n",
      "epoch: 000608  loss: -0.21522\n",
      "epoch: 000609  loss: -0.20883\n",
      "epoch: 000610  loss: -0.21735\n",
      "epoch: 000611  loss: -0.21430\n",
      "epoch: 000612  loss: -0.23057\n",
      "epoch: 000613  loss: -0.19004\n",
      "epoch: 000614  loss: -0.21520\n",
      "epoch: 000615  loss: -0.22118\n",
      "epoch: 000616  loss: -0.19321\n",
      "epoch: 000617  loss: -0.20235\n",
      "epoch: 000618  loss: -0.18539\n",
      "epoch: 000619  loss: -0.20398\n",
      "epoch: 000620  loss: -0.19927\n",
      "epoch: 000621  loss: -0.19546\n",
      "epoch: 000622  loss: -0.21066\n",
      "epoch: 000623  loss: -0.20515\n",
      "epoch: 000624  loss: -0.21945\n",
      "epoch: 000625  loss: -0.21426\n",
      "epoch: 000626  loss: -0.21174\n",
      "epoch: 000627  loss: -0.22046\n",
      "epoch: 000628  loss: -0.20844\n",
      "epoch: 000629  loss: -0.20590\n",
      "epoch: 000630  loss: -0.22197\n",
      "epoch: 000631  loss: -0.20698\n",
      "epoch: 000632  loss: -0.21475\n",
      "epoch: 000633  loss: -0.20153\n",
      "epoch: 000634  loss: -0.20288\n",
      "epoch: 000635  loss: -0.18303\n",
      "epoch: 000636  loss: -0.19489\n",
      "epoch: 000637  loss: -0.21361\n",
      "epoch: 000638  loss: -0.18829\n",
      "epoch: 000639  loss: -0.21595\n",
      "epoch: 000640  loss: -0.21601\n",
      "epoch: 000641  loss: -0.18988\n",
      "epoch: 000642  loss: -0.21038\n",
      "epoch: 000643  loss: -0.20652\n",
      "epoch: 000644  loss: -0.22443\n",
      "epoch: 000645  loss: -0.19818\n",
      "epoch: 000646  loss: -0.20754\n",
      "epoch: 000647  loss: -0.19892\n",
      "epoch: 000648  loss: -0.22219\n",
      "epoch: 000649  loss: -0.21903\n",
      "epoch: 000650  loss: -0.21298\n",
      "epoch: 000651  loss: -0.21028\n",
      "epoch: 000652  loss: -0.23251\n",
      "epoch: 000653  loss: -0.20302\n",
      "epoch: 000654  loss: -0.22064\n",
      "epoch: 000655  loss: -0.22049\n",
      "epoch: 000656  loss: -0.19396\n",
      "epoch: 000657  loss: -0.21948\n",
      "epoch: 000658  loss: -0.18943\n",
      "epoch: 000659  loss: -0.18992\n",
      "epoch: 000660  loss: -0.21058\n",
      "epoch: 000661  loss: -0.19970\n",
      "epoch: 000662  loss: -0.18365\n",
      "epoch: 000663  loss: -0.20605\n",
      "epoch: 000664  loss: -0.20431\n",
      "epoch: 000665  loss: -0.19065\n",
      "epoch: 000666  loss: -0.19423\n",
      "epoch: 000667  loss: -0.17811\n",
      "epoch: 000668  loss: -0.19399\n",
      "epoch: 000669  loss: -0.17659\n",
      "epoch: 000670  loss: -0.18355\n",
      "epoch: 000671  loss: -0.20233\n",
      "epoch: 000672  loss: -0.21473\n",
      "epoch: 000673  loss: -0.21332\n",
      "epoch: 000674  loss: -0.22223\n",
      "epoch: 000675  loss: -0.21429\n",
      "epoch: 000676  loss: -0.21833\n",
      "epoch: 000677  loss: -0.21138\n",
      "epoch: 000678  loss: -0.21945\n",
      "epoch: 000679  loss: -0.19741\n",
      "epoch: 000680  loss: -0.20773\n",
      "epoch: 000681  loss: -0.18597\n",
      "epoch: 000682  loss: -0.21710\n",
      "epoch: 000683  loss: -0.21388\n",
      "epoch: 000684  loss: -0.21656\n",
      "epoch: 000685  loss: -0.21883\n",
      "epoch: 000686  loss: -0.21029\n",
      "epoch: 000687  loss: -0.22912\n",
      "epoch: 000688  loss: -0.20929\n",
      "epoch: 000689  loss: -0.19562\n",
      "epoch: 000690  loss: -0.18969\n",
      "epoch: 000691  loss: -0.19328\n",
      "epoch: 000692  loss: -0.20533\n",
      "epoch: 000693  loss: -0.18984\n",
      "epoch: 000694  loss: -0.20273\n",
      "epoch: 000695  loss: -0.18301\n",
      "epoch: 000696  loss: -0.18615\n",
      "epoch: 000697  loss: -0.20304\n",
      "epoch: 000698  loss: -0.19340\n",
      "epoch: 000699  loss: -0.19423\n",
      "epoch: 000700  loss: -0.20358\n",
      "epoch: 000701  loss: -0.20162\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.200000000000001 2.272222222222222 0.599620643826204 0.33329440084051515\n",
      "epoch: 000702  loss: -0.21343\n",
      "epoch: 000703  loss: -0.21723\n",
      "epoch: 000704  loss: -0.19488\n",
      "epoch: 000705  loss: -0.21097\n",
      "epoch: 000706  loss: -0.18915\n",
      "epoch: 000707  loss: -0.20516\n",
      "epoch: 000708  loss: -0.18488\n",
      "epoch: 000709  loss: -0.20076\n",
      "epoch: 000710  loss: -0.22305\n",
      "epoch: 000711  loss: -0.20603\n",
      "epoch: 000712  loss: -0.21315\n",
      "epoch: 000713  loss: -0.20610\n",
      "epoch: 000714  loss: -0.21439\n",
      "epoch: 000715  loss: -0.17892\n",
      "epoch: 000716  loss: -0.20524\n",
      "epoch: 000717  loss: -0.19089\n",
      "epoch: 000718  loss: -0.20434\n",
      "epoch: 000719  loss: -0.19500\n",
      "epoch: 000720  loss: -0.21790\n",
      "epoch: 000721  loss: -0.22350\n",
      "epoch: 000722  loss: -0.22710\n",
      "epoch: 000723  loss: -0.23071\n",
      "epoch: 000724  loss: -0.21691\n",
      "epoch: 000725  loss: -0.20143\n",
      "epoch: 000726  loss: -0.17498\n",
      "epoch: 000727  loss: -0.19327\n",
      "epoch: 000728  loss: -0.17625\n",
      "epoch: 000729  loss: -0.20304\n",
      "epoch: 000730  loss: -0.18920\n",
      "epoch: 000731  loss: -0.20386\n",
      "epoch: 000732  loss: -0.20134\n",
      "epoch: 000733  loss: -0.23015\n",
      "epoch: 000734  loss: -0.22328\n",
      "epoch: 000735  loss: -0.21478\n",
      "epoch: 000736  loss: -0.22640\n",
      "epoch: 000737  loss: -0.21600\n",
      "epoch: 000738  loss: -0.20235\n",
      "epoch: 000739  loss: -0.22056\n",
      "epoch: 000740  loss: -0.19350\n",
      "epoch: 000741  loss: -0.21657\n",
      "epoch: 000742  loss: -0.21802\n",
      "epoch: 000743  loss: -0.20100\n",
      "epoch: 000744  loss: -0.22770\n",
      "epoch: 000745  loss: -0.19858\n",
      "epoch: 000746  loss: -0.20522\n",
      "epoch: 000747  loss: -0.21119\n",
      "epoch: 000748  loss: -0.21097\n",
      "epoch: 000749  loss: -0.20130\n",
      "epoch: 000750  loss: -0.20948\n",
      "epoch: 000751  loss: -0.25052\n",
      "epoch: 000752  loss: -0.25223\n",
      "epoch: 000753  loss: -0.24885\n",
      "epoch: 000754  loss: -0.24928\n",
      "epoch: 000755  loss: -0.22416\n",
      "epoch: 000756  loss: -0.22255\n",
      "epoch: 000757  loss: -0.17924\n",
      "epoch: 000758  loss: -0.18012\n",
      "epoch: 000759  loss: -0.18489\n",
      "epoch: 000760  loss: -0.20016\n",
      "epoch: 000761  loss: -0.19961\n",
      "epoch: 000762  loss: -0.19581\n",
      "epoch: 000763  loss: -0.20986\n",
      "epoch: 000764  loss: -0.21219\n",
      "epoch: 000765  loss: -0.21688\n",
      "epoch: 000766  loss: -0.20768\n",
      "epoch: 000767  loss: -0.22240\n",
      "epoch: 000768  loss: -0.21528\n",
      "epoch: 000769  loss: -0.23190\n",
      "epoch: 000770  loss: -0.20535\n",
      "epoch: 000771  loss: -0.21802\n",
      "epoch: 000772  loss: -0.22307\n",
      "epoch: 000773  loss: -0.19618\n",
      "epoch: 000774  loss: -0.20938\n",
      "epoch: 000775  loss: -0.19275\n",
      "epoch: 000776  loss: -0.19701\n",
      "epoch: 000777  loss: -0.20284\n",
      "epoch: 000778  loss: -0.22455\n",
      "epoch: 000779  loss: -0.22561\n",
      "epoch: 000780  loss: -0.20358\n",
      "epoch: 000781  loss: -0.21923\n",
      "epoch: 000782  loss: -0.20819\n",
      "epoch: 000783  loss: -0.21661\n",
      "epoch: 000784  loss: -0.17667\n",
      "epoch: 000785  loss: -0.22529\n",
      "epoch: 000786  loss: -0.20282\n",
      "epoch: 000787  loss: -0.20750\n",
      "epoch: 000788  loss: -0.21271\n",
      "epoch: 000789  loss: -0.20610\n",
      "epoch: 000790  loss: -0.21785\n",
      "epoch: 000791  loss: -0.24027\n",
      "epoch: 000792  loss: -0.20481\n",
      "epoch: 000793  loss: -0.22655\n",
      "epoch: 000794  loss: -0.20660\n",
      "epoch: 000795  loss: -0.22445\n",
      "epoch: 000796  loss: -0.20716\n",
      "epoch: 000797  loss: -0.18341\n",
      "epoch: 000798  loss: -0.20716\n",
      "epoch: 000799  loss: -0.19728\n",
      "epoch: 000800  loss: -0.19733\n",
      "epoch: 000801  loss: -0.18355\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.433333333333334 2.9499999999999997 0.6520207849607131 0.33305784978571207\n",
      "epoch: 000802  loss: -0.19472\n",
      "epoch: 000803  loss: -0.19448\n",
      "epoch: 000804  loss: -0.18615\n",
      "epoch: 000805  loss: -0.21907\n",
      "epoch: 000806  loss: -0.19886\n",
      "epoch: 000807  loss: -0.20830\n",
      "epoch: 000808  loss: -0.19739\n",
      "epoch: 000809  loss: -0.20962\n",
      "epoch: 000810  loss: -0.21657\n",
      "epoch: 000811  loss: -0.20326\n",
      "epoch: 000812  loss: -0.21729\n",
      "epoch: 000813  loss: -0.18489\n",
      "epoch: 000814  loss: -0.17902\n",
      "epoch: 000815  loss: -0.19799\n",
      "epoch: 000816  loss: -0.21886\n",
      "epoch: 000817  loss: -0.25278\n",
      "epoch: 000818  loss: -0.24333\n",
      "epoch: 000819  loss: -0.19007\n",
      "epoch: 000820  loss: -0.18916\n",
      "epoch: 000821  loss: -0.19792\n",
      "epoch: 000822  loss: -0.22475\n",
      "epoch: 000823  loss: -0.20630\n",
      "epoch: 000824  loss: -0.17188\n",
      "epoch: 000825  loss: -0.20446\n",
      "epoch: 000826  loss: -0.22474\n",
      "epoch: 000827  loss: -0.24323\n",
      "epoch: 000828  loss: -0.23850\n",
      "epoch: 000829  loss: -0.25245\n",
      "epoch: 000830  loss: -0.28362\n",
      "epoch: 000831  loss: -0.24690\n",
      "epoch: 000832  loss: -0.24204\n",
      "epoch: 000833  loss: -0.24111\n",
      "epoch: 000834  loss: -0.24801\n",
      "epoch: 000835  loss: -0.29664\n",
      "epoch: 000836  loss: -0.19613\n",
      "epoch: 000837  loss: -0.25801\n",
      "epoch: 000838  loss: -0.29590\n",
      "epoch: 000839  loss: -0.23502\n",
      "epoch: 000840  loss: -0.26729\n",
      "epoch: 000841  loss: -0.30141\n",
      "epoch: 000842  loss: -0.24812\n",
      "epoch: 000843  loss: -0.32340\n",
      "epoch: 000844  loss: -0.29967\n",
      "epoch: 000845  loss: -0.22444\n",
      "epoch: 000846  loss: -0.27910\n",
      "epoch: 000847  loss: -0.29402\n",
      "epoch: 000848  loss: -0.22961\n",
      "epoch: 000849  loss: -0.22502\n",
      "epoch: 000850  loss: -0.28938\n",
      "epoch: 000851  loss: -0.29283\n",
      "epoch: 000852  loss: -0.27006\n",
      "epoch: 000853  loss: -0.28821\n",
      "epoch: 000854  loss: -0.30568\n",
      "epoch: 000855  loss: -0.27815\n",
      "epoch: 000856  loss: -0.27918\n",
      "epoch: 000857  loss: -0.29470\n",
      "epoch: 000858  loss: -0.27217\n",
      "epoch: 000859  loss: -0.28140\n",
      "epoch: 000860  loss: -0.31308\n",
      "epoch: 000861  loss: -0.28166\n",
      "epoch: 000862  loss: -0.28253\n",
      "epoch: 000863  loss: -0.34056\n",
      "epoch: 000864  loss: -0.34313\n",
      "epoch: 000865  loss: -0.25805\n",
      "epoch: 000866  loss: -0.27808\n",
      "epoch: 000867  loss: -0.33573\n",
      "epoch: 000868  loss: -0.27949\n",
      "epoch: 000869  loss: -0.23252\n",
      "epoch: 000870  loss: -0.28698\n",
      "epoch: 000871  loss: -0.32310\n",
      "epoch: 000872  loss: -0.28014\n",
      "epoch: 000873  loss: -0.30419\n",
      "epoch: 000874  loss: -0.26036\n",
      "epoch: 000875  loss: -0.31802\n",
      "epoch: 000876  loss: -0.34047\n",
      "epoch: 000877  loss: -0.26555\n",
      "epoch: 000878  loss: -0.26476\n",
      "epoch: 000879  loss: -0.29392\n",
      "epoch: 000880  loss: -0.31109\n",
      "epoch: 000881  loss: -0.30726\n",
      "epoch: 000882  loss: -0.28555\n",
      "epoch: 000883  loss: -0.28133\n",
      "epoch: 000884  loss: -0.31586\n",
      "epoch: 000885  loss: -0.27916\n",
      "epoch: 000886  loss: -0.28982\n",
      "epoch: 000887  loss: -0.31635\n",
      "epoch: 000888  loss: -0.33814\n",
      "epoch: 000889  loss: -0.33896\n",
      "epoch: 000890  loss: -0.26144\n",
      "epoch: 000891  loss: -0.28745\n",
      "epoch: 000892  loss: -0.26793\n",
      "epoch: 000893  loss: -0.27906\n",
      "epoch: 000894  loss: -0.30534\n",
      "epoch: 000895  loss: -0.32669\n",
      "epoch: 000896  loss: -0.28639\n",
      "epoch: 000897  loss: -0.30240\n",
      "epoch: 000898  loss: -0.32662\n",
      "epoch: 000899  loss: -0.31601\n",
      "epoch: 000900  loss: -0.30471\n",
      "epoch: 000901  loss: -0.34738\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.549999999999999 2.308333333333333 0.6109905866460397 0.3330612361092399\n",
      "epoch: 000902  loss: -0.33891\n",
      "epoch: 000903  loss: -0.28170\n",
      "epoch: 000904  loss: -0.31888\n",
      "epoch: 000905  loss: -0.31780\n",
      "epoch: 000906  loss: -0.31023\n",
      "epoch: 000907  loss: -0.32282\n",
      "epoch: 000908  loss: -0.34447\n",
      "epoch: 000909  loss: -0.32860\n",
      "epoch: 000910  loss: -0.26959\n",
      "epoch: 000911  loss: -0.29938\n",
      "epoch: 000912  loss: -0.31098\n",
      "epoch: 000913  loss: -0.28796\n",
      "epoch: 000914  loss: -0.30800\n",
      "epoch: 000915  loss: -0.33807\n",
      "epoch: 000916  loss: -0.31374\n",
      "epoch: 000917  loss: -0.30964\n",
      "epoch: 000918  loss: -0.32073\n",
      "epoch: 000919  loss: -0.33342\n",
      "epoch: 000920  loss: -0.30528\n",
      "epoch: 000921  loss: -0.29670\n",
      "epoch: 000922  loss: -0.34121\n",
      "epoch: 000923  loss: -0.35217\n",
      "epoch: 000924  loss: -0.31749\n",
      "epoch: 000925  loss: -0.34694\n",
      "epoch: 000926  loss: -0.35210\n",
      "epoch: 000927  loss: -0.34942\n",
      "epoch: 000928  loss: -0.32184\n",
      "epoch: 000929  loss: -0.36890\n",
      "epoch: 000930  loss: -0.32596\n",
      "epoch: 000931  loss: -0.29925\n",
      "epoch: 000932  loss: -0.32407\n",
      "epoch: 000933  loss: -0.32254\n",
      "epoch: 000934  loss: -0.32462\n",
      "epoch: 000935  loss: -0.32576\n",
      "epoch: 000936  loss: -0.34091\n",
      "epoch: 000937  loss: -0.33066\n",
      "epoch: 000938  loss: -0.31934\n",
      "epoch: 000939  loss: -0.31877\n",
      "epoch: 000940  loss: -0.31586\n",
      "epoch: 000941  loss: -0.29979\n",
      "epoch: 000942  loss: -0.33828\n",
      "epoch: 000943  loss: -0.32445\n",
      "epoch: 000944  loss: -0.32679\n",
      "epoch: 000945  loss: -0.35582\n",
      "epoch: 000946  loss: -0.32736\n",
      "epoch: 000947  loss: -0.33026\n",
      "epoch: 000948  loss: -0.35241\n",
      "epoch: 000949  loss: -0.37049\n",
      "epoch: 000950  loss: -0.34422\n",
      "epoch: 000951  loss: -0.33988\n",
      "epoch: 000952  loss: -0.29925\n",
      "epoch: 000953  loss: -0.31138\n",
      "epoch: 000954  loss: -0.29895\n",
      "epoch: 000955  loss: -0.32506\n",
      "epoch: 000956  loss: -0.29698\n",
      "epoch: 000957  loss: -0.32462\n",
      "epoch: 000958  loss: -0.31993\n",
      "epoch: 000959  loss: -0.34890\n",
      "epoch: 000960  loss: -0.33390\n",
      "epoch: 000961  loss: -0.30247\n",
      "epoch: 000962  loss: -0.35223\n",
      "epoch: 000963  loss: -0.38051\n",
      "epoch: 000964  loss: -0.27885\n",
      "epoch: 000965  loss: -0.29763\n",
      "epoch: 000966  loss: -0.33152\n",
      "epoch: 000967  loss: -0.37278\n",
      "epoch: 000968  loss: -0.31256\n",
      "epoch: 000969  loss: -0.30598\n",
      "epoch: 000970  loss: -0.32622\n",
      "epoch: 000971  loss: -0.32459\n",
      "epoch: 000972  loss: -0.30964\n",
      "epoch: 000973  loss: -0.28621\n",
      "epoch: 000974  loss: -0.34947\n",
      "epoch: 000975  loss: -0.32790\n",
      "epoch: 000976  loss: -0.34739\n",
      "epoch: 000977  loss: -0.34000\n",
      "epoch: 000978  loss: -0.30057\n",
      "epoch: 000979  loss: -0.31869\n",
      "epoch: 000980  loss: -0.32348\n",
      "epoch: 000981  loss: -0.32535\n",
      "epoch: 000982  loss: -0.33175\n",
      "epoch: 000983  loss: -0.31100\n",
      "epoch: 000984  loss: -0.29103\n",
      "epoch: 000985  loss: -0.29164\n",
      "epoch: 000986  loss: -0.35017\n",
      "epoch: 000987  loss: -0.31116\n",
      "epoch: 000988  loss: -0.30409\n",
      "epoch: 000989  loss: -0.32284\n",
      "epoch: 000990  loss: -0.34463\n",
      "epoch: 000991  loss: -0.32013\n",
      "epoch: 000992  loss: -0.27621\n",
      "epoch: 000993  loss: -0.33186\n",
      "epoch: 000994  loss: -0.36144\n",
      "epoch: 000995  loss: -0.33229\n",
      "epoch: 000996  loss: -0.30289\n",
      "epoch: 000997  loss: -0.33342\n",
      "epoch: 000998  loss: -0.33478\n",
      "epoch: 000999  loss: -0.33116\n",
      "epoch: 001000  loss: -0.32702\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.7666666666666675 2.0444444444444443 0.7041357225324696 0.33314580951276784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>graph-unmasking/gen-loss</td><td></td></tr><tr><td>graph-unmasking/loss</td><td></td></tr><tr><td>mean degree</td><td></td></tr><tr><td>mean degree var</td><td></td></tr><tr><td>num_parameters</td><td></td></tr><tr><td>stats/cluster</td><td></td></tr><tr><td>stats/cluster_train</td><td></td></tr><tr><td>stats/degree</td><td></td></tr><tr><td>stats/degree_train</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>graph-unmasking/gen-loss</td><td>9.69889</td></tr><tr><td>graph-unmasking/loss</td><td>-0.32702</td></tr><tr><td>mean degree</td><td>5.76667</td></tr><tr><td>mean degree var</td><td>2.04444</td></tr><tr><td>num_parameters</td><td>20573</td></tr><tr><td>stats/cluster</td><td>0.33315</td></tr><tr><td>stats/cluster_train</td><td>0.00086</td></tr><tr><td>stats/degree</td><td>0.70414</td></tr><tr><td>stats/degree_train</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-sweep-2</strong> at: <a href='https://wandb.ai/nextaid/toad_retry/runs/cnvysh45' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/runs/cnvysh45</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230605_161759-cnvysh45/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 53kgoukj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcandidate_selection_radius: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdouble_inference: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgraph_transform: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: bce\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: pna\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_probability: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnormalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpre_post_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsingle_pass: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttowers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gerritgrossmann/Documents/devel/MaskedGraphGen/wandb/run-20230605_200838-53kgoukj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nextaid/toad_retry/runs/53kgoukj' target=\"_blank\">apricot-sweep-3</a></strong> to <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nextaid/toad_retry' target=\"_blank\">https://wandb.ai/nextaid/toad_retry</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/sweeps/8k6n2wdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nextaid/toad_retry/runs/53kgoukj' target=\"_blank\">https://wandb.ai/nextaid/toad_retry/runs/53kgoukj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8k6n2wdp {'batch_size': 500, 'candidate_selection_radius': 1, 'double_inference': False, 'dropout': 0, 'graph_transform': False, 'hidden_dim': 16, 'hidden_layer': 6, 'learning_rate': 0.0001, 'loss': 'bce', 'model': 'pna', 'noise_probability': 0, 'normalization': False, 'pre_post_layers': 1, 'single_pass': True, 'towers': 1}\n",
      "Reference distance is:  0.0 0.0008565843180121657\n",
      "epoch: 000001  loss: -0.05484\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.1 2.2222222222222223 0.5705763933136443 0.3330456981745606\n",
      "epoch: 000002  loss: -0.06174\n",
      "epoch: 000003  loss: -0.07861\n",
      "epoch: 000004  loss: -0.09341\n",
      "epoch: 000005  loss: -0.10971\n",
      "epoch: 000006  loss: -0.11387\n",
      "epoch: 000007  loss: -0.15658\n",
      "epoch: 000008  loss: -0.15761\n",
      "epoch: 000009  loss: -0.16152\n",
      "epoch: 000010  loss: -0.19583\n",
      "epoch: 000011  loss: -0.19691\n",
      "epoch: 000012  loss: -0.19595\n",
      "epoch: 000013  loss: -0.22730\n",
      "epoch: 000014  loss: -0.24284\n",
      "epoch: 000015  loss: -0.21791\n",
      "epoch: 000016  loss: -0.20985\n",
      "epoch: 000017  loss: -0.21477\n",
      "epoch: 000018  loss: -0.23537\n",
      "epoch: 000019  loss: -0.21274\n",
      "epoch: 000020  loss: -0.20543\n",
      "epoch: 000021  loss: -0.19284\n",
      "epoch: 000022  loss: -0.18604\n",
      "epoch: 000023  loss: -0.19479\n",
      "epoch: 000024  loss: -0.18573\n",
      "epoch: 000025  loss: -0.19977\n",
      "epoch: 000026  loss: -0.17587\n",
      "epoch: 000027  loss: -0.19875\n",
      "epoch: 000028  loss: -0.18852\n",
      "epoch: 000029  loss: -0.18796\n",
      "epoch: 000030  loss: -0.19755\n",
      "epoch: 000031  loss: -0.20313\n",
      "epoch: 000032  loss: -0.19838\n",
      "epoch: 000033  loss: -0.19116\n",
      "epoch: 000034  loss: -0.21409\n",
      "epoch: 000035  loss: -0.21542\n",
      "epoch: 000036  loss: -0.21085\n",
      "epoch: 000037  loss: -0.20515\n",
      "epoch: 000038  loss: -0.19579\n",
      "epoch: 000039  loss: -0.20220\n",
      "epoch: 000040  loss: -0.19355\n",
      "epoch: 000041  loss: -0.18466\n",
      "epoch: 000042  loss: -0.19810\n",
      "epoch: 000043  loss: -0.18497\n",
      "epoch: 000044  loss: -0.21696\n",
      "epoch: 000045  loss: -0.19227\n",
      "epoch: 000046  loss: -0.24331\n",
      "epoch: 000047  loss: -0.19843\n",
      "epoch: 000048  loss: -0.22677\n",
      "epoch: 000049  loss: -0.25593\n",
      "epoch: 000050  loss: -0.20995\n",
      "epoch: 000051  loss: -0.20996\n",
      "epoch: 000052  loss: -0.19741\n",
      "epoch: 000053  loss: -0.19159\n",
      "epoch: 000054  loss: -0.20366\n",
      "epoch: 000055  loss: -0.20281\n",
      "epoch: 000056  loss: -0.20226\n",
      "epoch: 000057  loss: -0.16910\n",
      "epoch: 000058  loss: -0.20005\n",
      "epoch: 000059  loss: -0.21750\n",
      "epoch: 000060  loss: -0.20526\n",
      "epoch: 000061  loss: -0.19448\n",
      "epoch: 000062  loss: -0.23659\n",
      "epoch: 000063  loss: -0.21203\n",
      "epoch: 000064  loss: -0.20123\n",
      "epoch: 000065  loss: -0.23236\n",
      "epoch: 000066  loss: -0.20831\n",
      "epoch: 000067  loss: -0.23139\n",
      "epoch: 000068  loss: -0.21299\n",
      "epoch: 000069  loss: -0.19622\n",
      "epoch: 000070  loss: -0.22353\n",
      "epoch: 000071  loss: -0.18908\n",
      "epoch: 000072  loss: -0.21272\n",
      "epoch: 000073  loss: -0.18959\n",
      "epoch: 000074  loss: -0.20508\n",
      "epoch: 000075  loss: -0.19105\n",
      "epoch: 000076  loss: -0.19620\n",
      "epoch: 000077  loss: -0.21214\n",
      "epoch: 000078  loss: -0.17603\n",
      "epoch: 000079  loss: -0.21648\n",
      "epoch: 000080  loss: -0.21456\n",
      "epoch: 000081  loss: -0.22050\n",
      "epoch: 000082  loss: -0.20990\n",
      "epoch: 000083  loss: -0.22601\n",
      "epoch: 000084  loss: -0.21214\n",
      "epoch: 000085  loss: -0.19569\n",
      "epoch: 000086  loss: -0.22815\n",
      "epoch: 000087  loss: -0.20566\n",
      "epoch: 000088  loss: -0.21550\n",
      "epoch: 000089  loss: -0.21696\n",
      "epoch: 000090  loss: -0.20912\n",
      "epoch: 000091  loss: -0.18338\n",
      "epoch: 000092  loss: -0.18955\n",
      "epoch: 000093  loss: -0.20045\n",
      "epoch: 000094  loss: -0.19278\n",
      "epoch: 000095  loss: -0.19035\n",
      "epoch: 000096  loss: -0.17337\n",
      "epoch: 000097  loss: -0.18513\n",
      "epoch: 000098  loss: -0.20852\n",
      "epoch: 000099  loss: -0.18836\n",
      "epoch: 000100  loss: -0.21211\n",
      "epoch: 000101  loss: -0.21020\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.366666666666666 2.3833333333333337 0.5670998979236679 0.3330572259486399\n",
      "epoch: 000102  loss: -0.19945\n",
      "epoch: 000103  loss: -0.20035\n",
      "epoch: 000104  loss: -0.25091\n",
      "epoch: 000105  loss: -0.20642\n",
      "epoch: 000106  loss: -0.22379\n",
      "epoch: 000107  loss: -0.20888\n",
      "epoch: 000108  loss: -0.21003\n",
      "epoch: 000109  loss: -0.20940\n",
      "epoch: 000110  loss: -0.21442\n",
      "epoch: 000111  loss: -0.19484\n",
      "epoch: 000112  loss: -0.18929\n",
      "epoch: 000113  loss: -0.22194\n",
      "epoch: 000114  loss: -0.19442\n",
      "epoch: 000115  loss: -0.18536\n",
      "epoch: 000116  loss: -0.19755\n",
      "epoch: 000117  loss: -0.20548\n",
      "epoch: 000118  loss: -0.18634\n",
      "epoch: 000119  loss: -0.18357\n",
      "epoch: 000120  loss: -0.19079\n",
      "epoch: 000121  loss: -0.19961\n",
      "epoch: 000122  loss: -0.19525\n",
      "epoch: 000123  loss: -0.20562\n",
      "epoch: 000124  loss: -0.20887\n",
      "epoch: 000125  loss: -0.22392\n",
      "epoch: 000126  loss: -0.22408\n",
      "epoch: 000127  loss: -0.23448\n",
      "epoch: 000128  loss: -0.21653\n",
      "epoch: 000129  loss: -0.19613\n",
      "epoch: 000130  loss: -0.19315\n",
      "epoch: 000131  loss: -0.19949\n",
      "epoch: 000132  loss: -0.20053\n",
      "epoch: 000133  loss: -0.19815\n",
      "epoch: 000134  loss: -0.18541\n",
      "epoch: 000135  loss: -0.17689\n",
      "epoch: 000136  loss: -0.20050\n",
      "epoch: 000137  loss: -0.20250\n",
      "epoch: 000138  loss: -0.16729\n",
      "epoch: 000139  loss: -0.18422\n",
      "epoch: 000140  loss: -0.19238\n",
      "epoch: 000141  loss: -0.20887\n",
      "epoch: 000142  loss: -0.18357\n",
      "epoch: 000143  loss: -0.19712\n",
      "epoch: 000144  loss: -0.21755\n",
      "epoch: 000145  loss: -0.19210\n",
      "epoch: 000146  loss: -0.20900\n",
      "epoch: 000147  loss: -0.20668\n",
      "epoch: 000148  loss: -0.21109\n",
      "epoch: 000149  loss: -0.19144\n",
      "epoch: 000150  loss: -0.21858\n",
      "epoch: 000151  loss: -0.19386\n",
      "epoch: 000152  loss: -0.19724\n",
      "epoch: 000153  loss: -0.19270\n",
      "epoch: 000154  loss: -0.20273\n",
      "epoch: 000155  loss: -0.17984\n",
      "epoch: 000156  loss: -0.19652\n",
      "epoch: 000157  loss: -0.20366\n",
      "epoch: 000158  loss: -0.21604\n",
      "epoch: 000159  loss: -0.21084\n",
      "epoch: 000160  loss: -0.20388\n",
      "epoch: 000161  loss: -0.21371\n",
      "epoch: 000162  loss: -0.20656\n",
      "epoch: 000163  loss: -0.20949\n",
      "epoch: 000164  loss: -0.20078\n",
      "epoch: 000165  loss: -0.22819\n",
      "epoch: 000166  loss: -0.21836\n",
      "epoch: 000167  loss: -0.18388\n",
      "epoch: 000168  loss: -0.22852\n",
      "epoch: 000169  loss: -0.19478\n",
      "epoch: 000170  loss: -0.21283\n",
      "epoch: 000171  loss: -0.20632\n",
      "epoch: 000172  loss: -0.19423\n",
      "epoch: 000173  loss: -0.20155\n",
      "epoch: 000174  loss: -0.20102\n",
      "epoch: 000175  loss: -0.21054\n",
      "epoch: 000176  loss: -0.20190\n",
      "epoch: 000177  loss: -0.20512\n",
      "epoch: 000178  loss: -0.20692\n",
      "epoch: 000179  loss: -0.19187\n",
      "epoch: 000180  loss: -0.19713\n",
      "epoch: 000181  loss: -0.21212\n",
      "epoch: 000182  loss: -0.19126\n",
      "epoch: 000183  loss: -0.22630\n",
      "epoch: 000184  loss: -0.21055\n",
      "epoch: 000185  loss: -0.20372\n",
      "epoch: 000186  loss: -0.21895\n",
      "epoch: 000187  loss: -0.22879\n",
      "epoch: 000188  loss: -0.21400\n",
      "epoch: 000189  loss: -0.19166\n",
      "epoch: 000190  loss: -0.21933\n",
      "epoch: 000191  loss: -0.19202\n",
      "epoch: 000192  loss: -0.20443\n",
      "epoch: 000193  loss: -0.20988\n",
      "epoch: 000194  loss: -0.22772\n",
      "epoch: 000195  loss: -0.18360\n",
      "epoch: 000196  loss: -0.17768\n",
      "epoch: 000197  loss: -0.20684\n",
      "epoch: 000198  loss: -0.18292\n",
      "epoch: 000199  loss: -0.20532\n",
      "epoch: 000200  loss: -0.18754\n",
      "epoch: 000201  loss: -0.19715\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.533333333333333 2.3722222222222227 0.6247178159846185 0.3382945025163926\n",
      "epoch: 000202  loss: -0.19234\n",
      "epoch: 000203  loss: -0.19225\n",
      "epoch: 000204  loss: -0.21124\n",
      "epoch: 000205  loss: -0.21384\n",
      "epoch: 000206  loss: -0.21523\n",
      "epoch: 000207  loss: -0.23821\n",
      "epoch: 000208  loss: -0.22809\n",
      "epoch: 000209  loss: -0.23164\n",
      "epoch: 000210  loss: -0.22187\n",
      "epoch: 000211  loss: -0.23014\n",
      "epoch: 000212  loss: -0.18557\n",
      "epoch: 000213  loss: -0.21368\n",
      "epoch: 000214  loss: -0.20778\n",
      "epoch: 000215  loss: -0.20169\n",
      "epoch: 000216  loss: -0.20236\n",
      "epoch: 000217  loss: -0.20676\n",
      "epoch: 000218  loss: -0.19494\n",
      "epoch: 000219  loss: -0.19267\n",
      "epoch: 000220  loss: -0.19896\n",
      "epoch: 000221  loss: -0.17017\n",
      "epoch: 000222  loss: -0.18153\n",
      "epoch: 000223  loss: -0.20336\n",
      "epoch: 000224  loss: -0.19639\n",
      "epoch: 000225  loss: -0.22523\n",
      "epoch: 000226  loss: -0.19377\n",
      "epoch: 000227  loss: -0.21380\n",
      "epoch: 000228  loss: -0.19802\n",
      "epoch: 000229  loss: -0.22321\n",
      "epoch: 000230  loss: -0.21345\n",
      "epoch: 000231  loss: -0.20677\n",
      "epoch: 000232  loss: -0.21776\n",
      "epoch: 000233  loss: -0.21322\n",
      "epoch: 000234  loss: -0.19286\n",
      "epoch: 000235  loss: -0.21038\n",
      "epoch: 000236  loss: -0.19586\n",
      "epoch: 000237  loss: -0.18827\n",
      "epoch: 000238  loss: -0.18284\n",
      "epoch: 000239  loss: -0.18944\n",
      "epoch: 000240  loss: -0.18784\n",
      "epoch: 000241  loss: -0.18382\n",
      "epoch: 000242  loss: -0.19652\n",
      "epoch: 000243  loss: -0.21617\n",
      "epoch: 000244  loss: -0.17368\n",
      "epoch: 000245  loss: -0.20492\n",
      "epoch: 000246  loss: -0.20914\n",
      "epoch: 000247  loss: -0.19121\n",
      "epoch: 000248  loss: -0.19257\n",
      "epoch: 000249  loss: -0.20411\n",
      "epoch: 000250  loss: -0.20764\n",
      "epoch: 000251  loss: -0.19765\n",
      "epoch: 000252  loss: -0.20622\n",
      "epoch: 000253  loss: -0.21080\n",
      "epoch: 000254  loss: -0.22040\n",
      "epoch: 000255  loss: -0.20348\n",
      "epoch: 000256  loss: -0.21713\n",
      "epoch: 000257  loss: -0.21082\n",
      "epoch: 000258  loss: -0.18739\n",
      "epoch: 000259  loss: -0.22926\n",
      "epoch: 000260  loss: -0.19617\n",
      "epoch: 000261  loss: -0.21458\n",
      "epoch: 000262  loss: -0.21605\n",
      "epoch: 000263  loss: -0.16944\n",
      "epoch: 000264  loss: -0.18278\n",
      "epoch: 000265  loss: -0.19094\n",
      "epoch: 000266  loss: -0.18903\n",
      "epoch: 000267  loss: -0.19385\n",
      "epoch: 000268  loss: -0.20098\n",
      "epoch: 000269  loss: -0.19433\n",
      "epoch: 000270  loss: -0.20961\n",
      "epoch: 000271  loss: -0.22838\n",
      "epoch: 000272  loss: -0.20389\n",
      "epoch: 000273  loss: -0.21737\n",
      "epoch: 000274  loss: -0.20417\n",
      "epoch: 000275  loss: -0.22144\n",
      "epoch: 000276  loss: -0.20599\n",
      "epoch: 000277  loss: -0.19391\n",
      "epoch: 000278  loss: -0.20556\n",
      "epoch: 000279  loss: -0.20151\n",
      "epoch: 000280  loss: -0.19028\n",
      "epoch: 000281  loss: -0.18957\n",
      "epoch: 000282  loss: -0.20117\n",
      "epoch: 000283  loss: -0.18098\n",
      "epoch: 000284  loss: -0.21919\n",
      "epoch: 000285  loss: -0.21388\n",
      "epoch: 000286  loss: -0.22923\n",
      "epoch: 000287  loss: -0.21135\n",
      "epoch: 000288  loss: -0.18286\n",
      "epoch: 000289  loss: -0.19269\n",
      "epoch: 000290  loss: -0.21354\n",
      "epoch: 000291  loss: -0.18882\n",
      "epoch: 000292  loss: -0.21855\n",
      "epoch: 000293  loss: -0.18332\n",
      "epoch: 000294  loss: -0.21430\n",
      "epoch: 000295  loss: -0.20839\n",
      "epoch: 000296  loss: -0.21542\n",
      "epoch: 000297  loss: -0.18514\n",
      "epoch: 000298  loss: -0.18289\n",
      "epoch: 000299  loss: -0.20267\n",
      "epoch: 000300  loss: -0.16801\n",
      "epoch: 000301  loss: -0.19548\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.333333333333333 2.138888888888889 0.6539186678834374 0.3330684142280654\n",
      "epoch: 000302  loss: -0.19301\n",
      "epoch: 000303  loss: -0.18959\n",
      "epoch: 000304  loss: -0.18533\n",
      "epoch: 000305  loss: -0.19354\n",
      "epoch: 000306  loss: -0.21355\n",
      "epoch: 000307  loss: -0.21529\n",
      "epoch: 000308  loss: -0.21971\n",
      "epoch: 000309  loss: -0.20790\n",
      "epoch: 000310  loss: -0.22966\n",
      "epoch: 000311  loss: -0.22515\n",
      "epoch: 000312  loss: -0.20942\n",
      "epoch: 000313  loss: -0.20904\n",
      "epoch: 000314  loss: -0.20486\n",
      "epoch: 000315  loss: -0.20759\n",
      "epoch: 000316  loss: -0.20252\n",
      "epoch: 000317  loss: -0.20328\n",
      "epoch: 000318  loss: -0.18689\n",
      "epoch: 000319  loss: -0.18586\n",
      "epoch: 000320  loss: -0.19665\n",
      "epoch: 000321  loss: -0.17916\n",
      "epoch: 000322  loss: -0.19305\n",
      "epoch: 000323  loss: -0.19679\n",
      "epoch: 000324  loss: -0.20061\n",
      "epoch: 000325  loss: -0.18558\n",
      "epoch: 000326  loss: -0.21191\n",
      "epoch: 000327  loss: -0.18430\n",
      "epoch: 000328  loss: -0.20703\n",
      "epoch: 000329  loss: -0.19804\n",
      "epoch: 000330  loss: -0.20259\n",
      "epoch: 000331  loss: -0.21806\n",
      "epoch: 000332  loss: -0.22085\n",
      "epoch: 000333  loss: -0.19053\n",
      "epoch: 000334  loss: -0.20088\n",
      "epoch: 000335  loss: -0.19601\n",
      "epoch: 000336  loss: -0.18037\n",
      "epoch: 000337  loss: -0.19979\n",
      "epoch: 000338  loss: -0.21392\n",
      "epoch: 000339  loss: -0.18856\n",
      "epoch: 000340  loss: -0.20589\n",
      "epoch: 000341  loss: -0.23118\n",
      "epoch: 000342  loss: -0.20697\n",
      "epoch: 000343  loss: -0.22887\n",
      "epoch: 000344  loss: -0.22538\n",
      "epoch: 000345  loss: -0.25636\n",
      "epoch: 000346  loss: -0.19485\n",
      "epoch: 000347  loss: -0.22008\n",
      "epoch: 000348  loss: -0.22445\n",
      "epoch: 000349  loss: -0.22855\n",
      "epoch: 000350  loss: -0.22482\n",
      "epoch: 000351  loss: -0.23821\n",
      "epoch: 000352  loss: -0.22406\n",
      "epoch: 000353  loss: -0.25069\n",
      "epoch: 000354  loss: -0.22316\n",
      "epoch: 000355  loss: -0.22257\n",
      "epoch: 000356  loss: -0.23359\n",
      "epoch: 000357  loss: -0.21324\n",
      "epoch: 000358  loss: -0.19162\n",
      "epoch: 000359  loss: -0.22456\n",
      "epoch: 000360  loss: -0.21422\n",
      "epoch: 000361  loss: -0.18676\n",
      "epoch: 000362  loss: -0.21985\n",
      "epoch: 000363  loss: -0.18221\n",
      "epoch: 000364  loss: -0.20767\n",
      "epoch: 000365  loss: -0.20094\n",
      "epoch: 000366  loss: -0.18389\n",
      "epoch: 000367  loss: -0.20855\n",
      "epoch: 000368  loss: -0.18864\n",
      "epoch: 000369  loss: -0.19974\n",
      "epoch: 000370  loss: -0.18170\n",
      "epoch: 000371  loss: -0.20810\n",
      "epoch: 000372  loss: -0.18834\n",
      "epoch: 000373  loss: -0.20507\n",
      "epoch: 000374  loss: -0.19545\n",
      "epoch: 000375  loss: -0.22323\n",
      "epoch: 000376  loss: -0.18703\n",
      "epoch: 000377  loss: -0.21291\n",
      "epoch: 000378  loss: -0.20507\n",
      "epoch: 000379  loss: -0.19828\n",
      "epoch: 000380  loss: -0.20068\n",
      "epoch: 000381  loss: -0.20881\n",
      "epoch: 000382  loss: -0.20034\n",
      "epoch: 000383  loss: -0.20759\n",
      "epoch: 000384  loss: -0.19987\n",
      "epoch: 000385  loss: -0.18208\n",
      "epoch: 000386  loss: -0.21540\n",
      "epoch: 000387  loss: -0.21690\n",
      "epoch: 000388  loss: -0.19784\n",
      "epoch: 000389  loss: -0.21081\n",
      "epoch: 000390  loss: -0.23114\n",
      "epoch: 000391  loss: -0.22696\n",
      "epoch: 000392  loss: -0.22065\n",
      "epoch: 000393  loss: -0.22336\n",
      "epoch: 000394  loss: -0.21726\n",
      "epoch: 000395  loss: -0.20655\n",
      "epoch: 000396  loss: -0.23488\n",
      "epoch: 000397  loss: -0.21731\n",
      "epoch: 000398  loss: -0.22382\n",
      "epoch: 000399  loss: -0.23372\n",
      "epoch: 000400  loss: -0.22522\n",
      "epoch: 000401  loss: -0.20933\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.233333333333333 2.322222222222222 0.5847421898920897 0.33305798344233983\n",
      "epoch: 000402  loss: -0.21780\n",
      "epoch: 000403  loss: -0.18409\n",
      "epoch: 000404  loss: -0.18687\n",
      "epoch: 000405  loss: -0.19411\n",
      "epoch: 000406  loss: -0.20649\n",
      "epoch: 000407  loss: -0.18305\n",
      "epoch: 000408  loss: -0.21101\n",
      "epoch: 000409  loss: -0.19110\n",
      "epoch: 000410  loss: -0.20889\n",
      "epoch: 000411  loss: -0.21606\n",
      "epoch: 000412  loss: -0.21766\n",
      "epoch: 000413  loss: -0.22663\n",
      "epoch: 000414  loss: -0.19163\n",
      "epoch: 000415  loss: -0.20780\n",
      "epoch: 000416  loss: -0.18536\n",
      "epoch: 000417  loss: -0.19195\n",
      "epoch: 000418  loss: -0.21143\n",
      "epoch: 000419  loss: -0.17891\n",
      "epoch: 000420  loss: -0.16853\n",
      "epoch: 000421  loss: -0.18161\n",
      "epoch: 000422  loss: -0.17966\n",
      "epoch: 000423  loss: -0.19308\n",
      "epoch: 000424  loss: -0.19579\n",
      "epoch: 000425  loss: -0.21849\n",
      "epoch: 000426  loss: -0.21119\n",
      "epoch: 000427  loss: -0.19884\n",
      "epoch: 000428  loss: -0.20558\n",
      "epoch: 000429  loss: -0.19032\n",
      "epoch: 000430  loss: -0.19721\n",
      "epoch: 000431  loss: -0.22236\n",
      "epoch: 000432  loss: -0.20303\n",
      "epoch: 000433  loss: -0.20773\n",
      "epoch: 000434  loss: -0.21042\n",
      "epoch: 000435  loss: -0.20475\n",
      "epoch: 000436  loss: -0.20077\n",
      "epoch: 000437  loss: -0.20096\n",
      "epoch: 000438  loss: -0.22568\n",
      "epoch: 000439  loss: -0.19207\n",
      "epoch: 000440  loss: -0.21290\n",
      "epoch: 000441  loss: -0.20395\n",
      "epoch: 000442  loss: -0.18565\n",
      "epoch: 000443  loss: -0.20865\n",
      "epoch: 000444  loss: -0.20741\n",
      "epoch: 000445  loss: -0.20699\n",
      "epoch: 000446  loss: -0.19124\n",
      "epoch: 000447  loss: -0.20616\n",
      "epoch: 000448  loss: -0.19263\n",
      "epoch: 000449  loss: -0.21158\n",
      "epoch: 000450  loss: -0.22163\n",
      "epoch: 000451  loss: -0.20478\n",
      "epoch: 000452  loss: -0.20185\n",
      "epoch: 000453  loss: -0.22030\n",
      "epoch: 000454  loss: -0.19079\n",
      "epoch: 000455  loss: -0.22100\n",
      "epoch: 000456  loss: -0.21888\n",
      "epoch: 000457  loss: -0.21469\n",
      "epoch: 000458  loss: -0.22992\n",
      "epoch: 000459  loss: -0.21095\n",
      "epoch: 000460  loss: -0.20403\n",
      "epoch: 000461  loss: -0.23118\n",
      "epoch: 000462  loss: -0.20999\n",
      "epoch: 000463  loss: -0.19987\n",
      "epoch: 000464  loss: -0.22780\n",
      "epoch: 000465  loss: -0.20000\n",
      "epoch: 000466  loss: -0.21841\n",
      "epoch: 000467  loss: -0.21224\n",
      "epoch: 000468  loss: -0.18938\n",
      "epoch: 000469  loss: -0.19615\n",
      "epoch: 000470  loss: -0.19221\n",
      "epoch: 000471  loss: -0.19683\n",
      "epoch: 000472  loss: -0.18778\n",
      "epoch: 000473  loss: -0.18979\n",
      "epoch: 000474  loss: -0.19501\n",
      "epoch: 000475  loss: -0.21406\n",
      "epoch: 000476  loss: -0.19827\n",
      "epoch: 000477  loss: -0.19660\n",
      "epoch: 000478  loss: -0.22447\n",
      "epoch: 000479  loss: -0.22216\n",
      "epoch: 000480  loss: -0.19489\n",
      "epoch: 000481  loss: -0.20734\n",
      "epoch: 000482  loss: -0.20661\n",
      "epoch: 000483  loss: -0.20851\n",
      "epoch: 000484  loss: -0.21446\n",
      "epoch: 000485  loss: -0.20912\n",
      "epoch: 000486  loss: -0.20673\n",
      "epoch: 000487  loss: -0.20039\n",
      "epoch: 000488  loss: -0.18403\n",
      "epoch: 000489  loss: -0.16688\n",
      "epoch: 000490  loss: -0.20210\n",
      "epoch: 000491  loss: -0.16881\n",
      "epoch: 000492  loss: -0.19404\n",
      "epoch: 000493  loss: -0.19428\n",
      "epoch: 000494  loss: -0.20902\n",
      "epoch: 000495  loss: -0.20500\n",
      "epoch: 000496  loss: -0.19240\n",
      "epoch: 000497  loss: -0.19302\n",
      "epoch: 000498  loss: -0.22200\n",
      "epoch: 000499  loss: -0.23596\n",
      "epoch: 000500  loss: -0.18360\n",
      "epoch: 000501  loss: -0.20985\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.583333333333333 2.563888888888889 0.5561641628839793 0.33128766869650866\n",
      "epoch: 000502  loss: -0.18718\n",
      "epoch: 000503  loss: -0.19959\n",
      "epoch: 000504  loss: -0.19392\n",
      "epoch: 000505  loss: -0.21674\n",
      "epoch: 000506  loss: -0.19238\n",
      "epoch: 000507  loss: -0.22157\n",
      "epoch: 000508  loss: -0.19509\n",
      "epoch: 000509  loss: -0.21643\n",
      "epoch: 000510  loss: -0.21337\n",
      "epoch: 000511  loss: -0.22411\n",
      "epoch: 000512  loss: -0.22741\n",
      "epoch: 000513  loss: -0.23053\n",
      "epoch: 000514  loss: -0.22826\n",
      "epoch: 000515  loss: -0.21523\n",
      "epoch: 000516  loss: -0.20228\n",
      "epoch: 000517  loss: -0.20476\n",
      "epoch: 000518  loss: -0.21660\n",
      "epoch: 000519  loss: -0.19323\n",
      "epoch: 000520  loss: -0.19596\n",
      "epoch: 000521  loss: -0.19385\n",
      "epoch: 000522  loss: -0.17017\n",
      "epoch: 000523  loss: -0.21099\n",
      "epoch: 000524  loss: -0.19737\n",
      "epoch: 000525  loss: -0.21024\n",
      "epoch: 000526  loss: -0.20038\n",
      "epoch: 000527  loss: -0.19338\n",
      "epoch: 000528  loss: -0.20517\n",
      "epoch: 000529  loss: -0.18171\n",
      "epoch: 000530  loss: -0.22530\n",
      "epoch: 000531  loss: -0.20861\n",
      "epoch: 000532  loss: -0.20818\n",
      "epoch: 000533  loss: -0.21058\n",
      "epoch: 000534  loss: -0.18942\n",
      "epoch: 000535  loss: -0.21973\n",
      "epoch: 000536  loss: -0.20260\n",
      "epoch: 000537  loss: -0.20048\n",
      "epoch: 000538  loss: -0.19531\n",
      "epoch: 000539  loss: -0.19269\n",
      "epoch: 000540  loss: -0.21418\n",
      "epoch: 000541  loss: -0.21300\n",
      "epoch: 000542  loss: -0.21310\n",
      "epoch: 000543  loss: -0.19983\n",
      "epoch: 000544  loss: -0.19236\n",
      "epoch: 000545  loss: -0.20904\n",
      "epoch: 000546  loss: -0.22930\n",
      "epoch: 000547  loss: -0.22523\n",
      "epoch: 000548  loss: -0.20215\n",
      "epoch: 000549  loss: -0.21793\n",
      "epoch: 000550  loss: -0.20661\n",
      "epoch: 000551  loss: -0.23473\n",
      "epoch: 000552  loss: -0.23604\n",
      "epoch: 000553  loss: -0.27845\n",
      "epoch: 000554  loss: -0.32660\n",
      "epoch: 000555  loss: -0.32703\n",
      "epoch: 000556  loss: -0.30664\n",
      "epoch: 000557  loss: -0.32167\n",
      "epoch: 000558  loss: -0.32463\n",
      "epoch: 000559  loss: -0.28228\n",
      "epoch: 000560  loss: -0.30656\n",
      "epoch: 000561  loss: -0.34370\n",
      "epoch: 000562  loss: -0.33720\n",
      "epoch: 000563  loss: -0.34783\n",
      "epoch: 000564  loss: -0.37653\n",
      "epoch: 000565  loss: -0.36386\n",
      "epoch: 000566  loss: -0.38630\n",
      "epoch: 000567  loss: -0.36468\n",
      "epoch: 000568  loss: -0.35416\n",
      "epoch: 000569  loss: -0.36798\n",
      "epoch: 000570  loss: -0.41545\n",
      "epoch: 000571  loss: -0.38674\n",
      "epoch: 000572  loss: -0.36525\n",
      "epoch: 000573  loss: -0.34915\n",
      "epoch: 000574  loss: -0.39272\n",
      "epoch: 000575  loss: -0.37936\n",
      "epoch: 000576  loss: -0.40425\n",
      "epoch: 000577  loss: -0.39854\n",
      "epoch: 000578  loss: -0.44819\n",
      "epoch: 000579  loss: -0.42988\n",
      "epoch: 000580  loss: -0.41547\n",
      "epoch: 000581  loss: -0.40055\n",
      "epoch: 000582  loss: -0.38863\n",
      "epoch: 000583  loss: -0.41786\n",
      "epoch: 000584  loss: -0.41946\n",
      "epoch: 000585  loss: -0.41118\n",
      "epoch: 000586  loss: -0.37983\n",
      "epoch: 000587  loss: -0.37808\n",
      "epoch: 000588  loss: -0.37807\n",
      "epoch: 000589  loss: -0.39332\n",
      "epoch: 000590  loss: -0.43556\n",
      "epoch: 000591  loss: -0.45906\n",
      "epoch: 000592  loss: -0.45851\n",
      "epoch: 000593  loss: -0.45982\n",
      "epoch: 000594  loss: -0.44047\n",
      "epoch: 000595  loss: -0.44746\n",
      "epoch: 000596  loss: -0.45183\n",
      "epoch: 000597  loss: -0.43102\n",
      "epoch: 000598  loss: -0.42600\n",
      "epoch: 000599  loss: -0.40134\n",
      "epoch: 000600  loss: -0.38541\n",
      "epoch: 000601  loss: -0.46472\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.066666666666667 2.6166666666666667 0.5934458284300925 0.3331456057931787\n",
      "epoch: 000602  loss: -0.45378\n",
      "epoch: 000603  loss: -0.43628\n",
      "epoch: 000604  loss: -0.41036\n",
      "epoch: 000605  loss: -0.39858\n",
      "epoch: 000606  loss: -0.40985\n",
      "epoch: 000607  loss: -0.42037\n",
      "epoch: 000608  loss: -0.45143\n",
      "epoch: 000609  loss: -0.46179\n",
      "epoch: 000610  loss: -0.40869\n",
      "epoch: 000611  loss: -0.42716\n",
      "epoch: 000612  loss: -0.44700\n",
      "epoch: 000613  loss: -0.42758\n",
      "epoch: 000614  loss: -0.42643\n",
      "epoch: 000615  loss: -0.41025\n",
      "epoch: 000616  loss: -0.44006\n",
      "epoch: 000617  loss: -0.44054\n",
      "epoch: 000618  loss: -0.44932\n",
      "epoch: 000619  loss: -0.43592\n",
      "epoch: 000620  loss: -0.45464\n",
      "epoch: 000621  loss: -0.44410\n",
      "epoch: 000622  loss: -0.44676\n",
      "epoch: 000623  loss: -0.40685\n",
      "epoch: 000624  loss: -0.41871\n",
      "epoch: 000625  loss: -0.41891\n",
      "epoch: 000626  loss: -0.47481\n",
      "epoch: 000627  loss: -0.45833\n",
      "epoch: 000628  loss: -0.44961\n",
      "epoch: 000629  loss: -0.43377\n",
      "epoch: 000630  loss: -0.46097\n",
      "epoch: 000631  loss: -0.46319\n",
      "epoch: 000632  loss: -0.46802\n",
      "epoch: 000633  loss: -0.43381\n",
      "epoch: 000634  loss: -0.43569\n",
      "epoch: 000635  loss: -0.42733\n",
      "epoch: 000636  loss: -0.46409\n",
      "epoch: 000637  loss: -0.43568\n",
      "epoch: 000638  loss: -0.43136\n",
      "epoch: 000639  loss: -0.45401\n",
      "epoch: 000640  loss: -0.45331\n",
      "epoch: 000641  loss: -0.43554\n",
      "epoch: 000642  loss: -0.47567\n",
      "epoch: 000643  loss: -0.47142\n",
      "epoch: 000644  loss: -0.48811\n",
      "epoch: 000645  loss: -0.46814\n",
      "epoch: 000646  loss: -0.43225\n",
      "epoch: 000647  loss: -0.44881\n",
      "epoch: 000648  loss: -0.44952\n",
      "epoch: 000649  loss: -0.46035\n",
      "epoch: 000650  loss: -0.40442\n",
      "epoch: 000651  loss: -0.39812\n",
      "epoch: 000652  loss: -0.43541\n",
      "epoch: 000653  loss: -0.44814\n",
      "epoch: 000654  loss: -0.48191\n",
      "epoch: 000655  loss: -0.45571\n",
      "epoch: 000656  loss: -0.41789\n",
      "epoch: 000657  loss: -0.42695\n",
      "epoch: 000658  loss: -0.45470\n",
      "epoch: 000659  loss: -0.45903\n",
      "epoch: 000660  loss: -0.46450\n",
      "epoch: 000661  loss: -0.43115\n",
      "epoch: 000662  loss: -0.44987\n",
      "epoch: 000663  loss: -0.43852\n",
      "epoch: 000664  loss: -0.46430\n",
      "epoch: 000665  loss: -0.47492\n",
      "epoch: 000666  loss: -0.47561\n",
      "epoch: 000667  loss: -0.44536\n",
      "epoch: 000668  loss: -0.43589\n",
      "epoch: 000669  loss: -0.43663\n",
      "epoch: 000670  loss: -0.44574\n",
      "epoch: 000671  loss: -0.46729\n",
      "epoch: 000672  loss: -0.47235\n",
      "epoch: 000673  loss: -0.43035\n",
      "epoch: 000674  loss: -0.46046\n",
      "epoch: 000675  loss: -0.46205\n",
      "epoch: 000676  loss: -0.40818\n",
      "epoch: 000677  loss: -0.44050\n",
      "epoch: 000678  loss: -0.45565\n",
      "epoch: 000679  loss: -0.43509\n",
      "epoch: 000680  loss: -0.45326\n",
      "epoch: 000681  loss: -0.47654\n",
      "epoch: 000682  loss: -0.43617\n",
      "epoch: 000683  loss: -0.46431\n",
      "epoch: 000684  loss: -0.44811\n",
      "epoch: 000685  loss: -0.45783\n",
      "epoch: 000686  loss: -0.45179\n",
      "epoch: 000687  loss: -0.43825\n",
      "epoch: 000688  loss: -0.43217\n",
      "epoch: 000689  loss: -0.44318\n",
      "epoch: 000690  loss: -0.48724\n",
      "epoch: 000691  loss: -0.49948\n",
      "epoch: 000692  loss: -0.49261\n",
      "epoch: 000693  loss: -0.46579\n",
      "epoch: 000694  loss: -0.44311\n",
      "epoch: 000695  loss: -0.44330\n",
      "epoch: 000696  loss: -0.47462\n",
      "epoch: 000697  loss: -0.46740\n",
      "epoch: 000698  loss: -0.47176\n",
      "epoch: 000699  loss: -0.43419\n",
      "epoch: 000700  loss: -0.46269\n",
      "epoch: 000701  loss: -0.47340\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.4 2.1 0.6164371507616913 0.33307769838982426\n",
      "epoch: 000702  loss: -0.43473\n",
      "epoch: 000703  loss: -0.43453\n",
      "epoch: 000704  loss: -0.44485\n",
      "epoch: 000705  loss: -0.43671\n",
      "epoch: 000706  loss: -0.45287\n",
      "epoch: 000707  loss: -0.44925\n",
      "epoch: 000708  loss: -0.47062\n",
      "epoch: 000709  loss: -0.43881\n",
      "epoch: 000710  loss: -0.44839\n",
      "epoch: 000711  loss: -0.45187\n",
      "epoch: 000712  loss: -0.44724\n",
      "epoch: 000713  loss: -0.45659\n",
      "epoch: 000714  loss: -0.46021\n",
      "epoch: 000715  loss: -0.46122\n",
      "epoch: 000716  loss: -0.47740\n",
      "epoch: 000717  loss: -0.42528\n",
      "epoch: 000718  loss: -0.45130\n",
      "epoch: 000719  loss: -0.48584\n",
      "epoch: 000720  loss: -0.48629\n",
      "epoch: 000721  loss: -0.48374\n",
      "epoch: 000722  loss: -0.46816\n",
      "epoch: 000723  loss: -0.45437\n",
      "epoch: 000724  loss: -0.47407\n",
      "epoch: 000725  loss: -0.48773\n",
      "epoch: 000726  loss: -0.43496\n",
      "epoch: 000727  loss: -0.46309\n",
      "epoch: 000728  loss: -0.44107\n",
      "epoch: 000729  loss: -0.45344\n",
      "epoch: 000730  loss: -0.47149\n",
      "epoch: 000731  loss: -0.43119\n",
      "epoch: 000732  loss: -0.44314\n",
      "epoch: 000733  loss: -0.46670\n",
      "epoch: 000734  loss: -0.42794\n",
      "epoch: 000735  loss: -0.44289\n",
      "epoch: 000736  loss: -0.42576\n",
      "epoch: 000737  loss: -0.48247\n",
      "epoch: 000738  loss: -0.45664\n",
      "epoch: 000739  loss: -0.45026\n",
      "epoch: 000740  loss: -0.46957\n",
      "epoch: 000741  loss: -0.44910\n",
      "epoch: 000742  loss: -0.46163\n",
      "epoch: 000743  loss: -0.46172\n",
      "epoch: 000744  loss: -0.48849\n",
      "epoch: 000745  loss: -0.48812\n",
      "epoch: 000746  loss: -0.48588\n",
      "epoch: 000747  loss: -0.45645\n",
      "epoch: 000748  loss: -0.47640\n",
      "epoch: 000749  loss: -0.45713\n",
      "epoch: 000750  loss: -0.43944\n",
      "epoch: 000751  loss: -0.45975\n",
      "epoch: 000752  loss: -0.45788\n",
      "epoch: 000753  loss: -0.45141\n",
      "epoch: 000754  loss: -0.42821\n",
      "epoch: 000755  loss: -0.44529\n",
      "epoch: 000756  loss: -0.44658\n",
      "epoch: 000757  loss: -0.45790\n",
      "epoch: 000758  loss: -0.45229\n",
      "epoch: 000759  loss: -0.47870\n",
      "epoch: 000760  loss: -0.49632\n",
      "epoch: 000761  loss: -0.46509\n",
      "epoch: 000762  loss: -0.44719\n",
      "epoch: 000763  loss: -0.44613\n",
      "epoch: 000764  loss: -0.46533\n",
      "epoch: 000765  loss: -0.41611\n",
      "epoch: 000766  loss: -0.42155\n",
      "epoch: 000767  loss: -0.44276\n",
      "epoch: 000768  loss: -0.44170\n",
      "epoch: 000769  loss: -0.44766\n",
      "epoch: 000770  loss: -0.48392\n",
      "epoch: 000771  loss: -0.42222\n",
      "epoch: 000772  loss: -0.42348\n",
      "epoch: 000773  loss: -0.44068\n",
      "epoch: 000774  loss: -0.44024\n",
      "epoch: 000775  loss: -0.44144\n",
      "epoch: 000776  loss: -0.47262\n",
      "epoch: 000777  loss: -0.45748\n",
      "epoch: 000778  loss: -0.45833\n",
      "epoch: 000779  loss: -0.46647\n",
      "epoch: 000780  loss: -0.45901\n",
      "epoch: 000781  loss: -0.45758\n",
      "epoch: 000782  loss: -0.45237\n",
      "epoch: 000783  loss: -0.43246\n",
      "epoch: 000784  loss: -0.45424\n",
      "epoch: 000785  loss: -0.47277\n",
      "epoch: 000786  loss: -0.43563\n",
      "epoch: 000787  loss: -0.44168\n",
      "epoch: 000788  loss: -0.44863\n",
      "epoch: 000789  loss: -0.42428\n",
      "epoch: 000790  loss: -0.43925\n",
      "epoch: 000791  loss: -0.42763\n",
      "epoch: 000792  loss: -0.47227\n",
      "epoch: 000793  loss: -0.45870\n",
      "epoch: 000794  loss: -0.44298\n",
      "epoch: 000795  loss: -0.44507\n",
      "epoch: 000796  loss: -0.46685\n",
      "epoch: 000797  loss: -0.45276\n",
      "epoch: 000798  loss: -0.47644\n",
      "epoch: 000799  loss: -0.46109\n",
      "epoch: 000800  loss: -0.48448\n",
      "epoch: 000801  loss: -0.42300\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.533333333333333 2.2666666666666666 0.6243703361359632 0.33306500685329676\n",
      "epoch: 000802  loss: -0.46719\n",
      "epoch: 000803  loss: -0.45487\n",
      "epoch: 000804  loss: -0.49474\n",
      "epoch: 000805  loss: -0.44224\n",
      "epoch: 000806  loss: -0.45829\n",
      "epoch: 000807  loss: -0.47871\n",
      "epoch: 000808  loss: -0.49105\n",
      "epoch: 000809  loss: -0.45766\n",
      "epoch: 000810  loss: -0.46350\n",
      "epoch: 000811  loss: -0.46774\n",
      "epoch: 000812  loss: -0.50824\n",
      "epoch: 000813  loss: -0.51963\n",
      "epoch: 000814  loss: -0.48371\n",
      "epoch: 000815  loss: -0.48288\n",
      "epoch: 000816  loss: -0.47209\n",
      "epoch: 000817  loss: -0.45351\n",
      "epoch: 000818  loss: -0.46121\n",
      "epoch: 000819  loss: -0.48358\n",
      "epoch: 000820  loss: -0.45809\n",
      "epoch: 000821  loss: -0.44714\n",
      "epoch: 000822  loss: -0.48760\n",
      "epoch: 000823  loss: -0.48707\n",
      "epoch: 000824  loss: -0.46110\n",
      "epoch: 000825  loss: -0.45150\n",
      "epoch: 000826  loss: -0.43354\n",
      "epoch: 000827  loss: -0.45432\n",
      "epoch: 000828  loss: -0.47246\n",
      "epoch: 000829  loss: -0.44168\n",
      "epoch: 000830  loss: -0.48211\n",
      "epoch: 000831  loss: -0.45686\n",
      "epoch: 000832  loss: -0.42216\n",
      "epoch: 000833  loss: -0.42633\n",
      "epoch: 000834  loss: -0.43690\n",
      "epoch: 000835  loss: -0.46024\n",
      "epoch: 000836  loss: -0.43555\n",
      "epoch: 000837  loss: -0.43874\n",
      "epoch: 000838  loss: -0.42275\n",
      "epoch: 000839  loss: -0.43601\n",
      "epoch: 000840  loss: -0.46708\n",
      "epoch: 000841  loss: -0.47487\n",
      "epoch: 000842  loss: -0.42930\n",
      "epoch: 000843  loss: -0.42751\n",
      "epoch: 000844  loss: -0.48236\n",
      "epoch: 000845  loss: -0.41935\n",
      "epoch: 000846  loss: -0.43850\n",
      "epoch: 000847  loss: -0.42727\n",
      "epoch: 000848  loss: -0.43679\n",
      "epoch: 000849  loss: -0.46929\n",
      "epoch: 000850  loss: -0.46535\n",
      "epoch: 000851  loss: -0.46926\n",
      "epoch: 000852  loss: -0.45811\n",
      "epoch: 000853  loss: -0.48280\n",
      "epoch: 000854  loss: -0.46556\n",
      "epoch: 000855  loss: -0.46228\n",
      "epoch: 000856  loss: -0.48812\n",
      "epoch: 000857  loss: -0.47787\n",
      "epoch: 000858  loss: -0.50498\n",
      "epoch: 000859  loss: -0.48736\n",
      "epoch: 000860  loss: -0.49216\n",
      "epoch: 000861  loss: -0.48797\n",
      "epoch: 000862  loss: -0.45977\n",
      "epoch: 000863  loss: -0.46608\n",
      "epoch: 000864  loss: -0.48334\n",
      "epoch: 000865  loss: -0.48142\n",
      "epoch: 000866  loss: -0.48611\n",
      "epoch: 000867  loss: -0.42845\n",
      "epoch: 000868  loss: -0.41115\n",
      "epoch: 000869  loss: -0.41628\n",
      "epoch: 000870  loss: -0.44116\n",
      "epoch: 000871  loss: -0.45827\n",
      "epoch: 000872  loss: -0.46494\n",
      "epoch: 000873  loss: -0.46552\n",
      "epoch: 000874  loss: -0.47275\n",
      "epoch: 000875  loss: -0.46055\n",
      "epoch: 000876  loss: -0.46337\n",
      "epoch: 000877  loss: -0.44418\n",
      "epoch: 000878  loss: -0.45579\n",
      "epoch: 000879  loss: -0.47796\n",
      "epoch: 000880  loss: -0.49078\n",
      "epoch: 000881  loss: -0.44220\n",
      "epoch: 000882  loss: -0.47445\n",
      "epoch: 000883  loss: -0.47056\n",
      "epoch: 000884  loss: -0.47037\n",
      "epoch: 000885  loss: -0.45851\n",
      "epoch: 000886  loss: -0.46467\n",
      "epoch: 000887  loss: -0.46659\n",
      "epoch: 000888  loss: -0.47006\n",
      "epoch: 000889  loss: -0.47198\n",
      "epoch: 000890  loss: -0.44317\n",
      "epoch: 000891  loss: -0.47428\n",
      "epoch: 000892  loss: -0.46434\n",
      "epoch: 000893  loss: -0.43771\n",
      "epoch: 000894  loss: -0.44897\n",
      "epoch: 000895  loss: -0.46191\n",
      "epoch: 000896  loss: -0.45512\n",
      "epoch: 000897  loss: -0.45831\n",
      "epoch: 000898  loss: -0.47396\n",
      "epoch: 000899  loss: -0.47526\n",
      "epoch: 000900  loss: -0.46540\n",
      "epoch: 000901  loss: -0.47129\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.7666666666666675 2.477777777777778 0.5975686362361372 0.3331387137965327\n",
      "epoch: 000902  loss: -0.45519\n",
      "epoch: 000903  loss: -0.46063\n",
      "epoch: 000904  loss: -0.46862\n",
      "epoch: 000905  loss: -0.43892\n",
      "epoch: 000906  loss: -0.44493\n",
      "epoch: 000907  loss: -0.46150\n",
      "epoch: 000908  loss: -0.44193\n",
      "epoch: 000909  loss: -0.48810\n",
      "epoch: 000910  loss: -0.46768\n",
      "epoch: 000911  loss: -0.47277\n",
      "epoch: 000912  loss: -0.47423\n",
      "epoch: 000913  loss: -0.46992\n",
      "epoch: 000914  loss: -0.45158\n",
      "epoch: 000915  loss: -0.45659\n",
      "epoch: 000916  loss: -0.46408\n",
      "epoch: 000917  loss: -0.46435\n",
      "epoch: 000918  loss: -0.45048\n",
      "epoch: 000919  loss: -0.46444\n",
      "epoch: 000920  loss: -0.47911\n",
      "epoch: 000921  loss: -0.40569\n",
      "epoch: 000922  loss: -0.45032\n",
      "epoch: 000923  loss: -0.48961\n",
      "epoch: 000924  loss: -0.46402\n",
      "epoch: 000925  loss: -0.48320\n",
      "epoch: 000926  loss: -0.45230\n",
      "epoch: 000927  loss: -0.49306\n",
      "epoch: 000928  loss: -0.48855\n",
      "epoch: 000929  loss: -0.46600\n",
      "epoch: 000930  loss: -0.42956\n",
      "epoch: 000931  loss: -0.43384\n",
      "epoch: 000932  loss: -0.43354\n",
      "epoch: 000933  loss: -0.45558\n",
      "epoch: 000934  loss: -0.44899\n",
      "epoch: 000935  loss: -0.47887\n",
      "epoch: 000936  loss: -0.48805\n",
      "epoch: 000937  loss: -0.48007\n",
      "epoch: 000938  loss: -0.48445\n",
      "epoch: 000939  loss: -0.49175\n",
      "epoch: 000940  loss: -0.47839\n",
      "epoch: 000941  loss: -0.48751\n",
      "epoch: 000942  loss: -0.43517\n",
      "epoch: 000943  loss: -0.43411\n",
      "epoch: 000944  loss: -0.45273\n",
      "epoch: 000945  loss: -0.48483\n",
      "epoch: 000946  loss: -0.47721\n",
      "epoch: 000947  loss: -0.46209\n",
      "epoch: 000948  loss: -0.44858\n",
      "epoch: 000949  loss: -0.44406\n",
      "epoch: 000950  loss: -0.43607\n",
      "epoch: 000951  loss: -0.45416\n",
      "epoch: 000952  loss: -0.44466\n",
      "epoch: 000953  loss: -0.46500\n",
      "epoch: 000954  loss: -0.47758\n",
      "epoch: 000955  loss: -0.51848\n",
      "epoch: 000956  loss: -0.47401\n",
      "epoch: 000957  loss: -0.45933\n",
      "epoch: 000958  loss: -0.49888\n",
      "epoch: 000959  loss: -0.47698\n",
      "epoch: 000960  loss: -0.47609\n",
      "epoch: 000961  loss: -0.43046\n",
      "epoch: 000962  loss: -0.45513\n",
      "epoch: 000963  loss: -0.45361\n",
      "epoch: 000964  loss: -0.48275\n",
      "epoch: 000965  loss: -0.49751\n",
      "epoch: 000966  loss: -0.46743\n",
      "epoch: 000967  loss: -0.47050\n",
      "epoch: 000968  loss: -0.46910\n",
      "epoch: 000969  loss: -0.43019\n",
      "epoch: 000970  loss: -0.45954\n",
      "epoch: 000971  loss: -0.47642\n",
      "epoch: 000972  loss: -0.44982\n",
      "epoch: 000973  loss: -0.44089\n",
      "epoch: 000974  loss: -0.46450\n",
      "epoch: 000975  loss: -0.45381\n",
      "epoch: 000976  loss: -0.47429\n",
      "epoch: 000977  loss: -0.50263\n",
      "epoch: 000978  loss: -0.44878\n",
      "epoch: 000979  loss: -0.42526\n",
      "epoch: 000980  loss: -0.42206\n",
      "epoch: 000981  loss: -0.44112\n",
      "epoch: 000982  loss: -0.42926\n",
      "epoch: 000983  loss: -0.46381\n",
      "epoch: 000984  loss: -0.49710\n",
      "epoch: 000985  loss: -0.44756\n",
      "epoch: 000986  loss: -0.45047\n",
      "epoch: 000987  loss: -0.45469\n",
      "epoch: 000988  loss: -0.49617\n",
      "epoch: 000989  loss: -0.44101\n",
      "epoch: 000990  loss: -0.48247\n",
      "epoch: 000991  loss: -0.47354\n",
      "epoch: 000992  loss: -0.48751\n",
      "epoch: 000993  loss: -0.47154\n",
      "epoch: 000994  loss: -0.47851\n",
      "epoch: 000995  loss: -0.44595\n",
      "epoch: 000996  loss: -0.46142\n",
      "epoch: 000997  loss: -0.49048\n",
      "epoch: 000998  loss: -0.48751\n",
      "epoch: 000999  loss: -0.48733\n",
      "epoch: 001000  loss: -0.45733\n",
      "start graph generation\n",
      "finish graph generation\n",
      "5.7333333333333325 2.4 0.6552549620137174 0.3332999233398704\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_MODE\"] = WANDB_MODE \n",
    "\n",
    "SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
    "wandb.agent(SWEEP_ID, project=PROJECT_NAME, function=start_agent_envelope, count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHcAvy-w6Zlu"
   },
   "outputs": [],
   "source": [
    "jljkl = sdfljldf/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg-wy3DnJRA9"
   },
   "source": [
    "# Load graph datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWlIj_iUJUtp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"git clone https://github.com/KarolisMart/SPECTRE.git\")\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#import pytorch_lightning as pl\n",
    "import networkx as nx\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from scipy.spatial import Delaunay\n",
    "#from torch_geometric.datasets import QM9\n",
    "#from rdkit import Chem\n",
    "#from torch_geometric.utils import dense_to_sparse, to_dense_adj, to_networkx\n",
    "\n",
    "#from util.eval_helper import degree_stats, orbit_stats_all, clustering_stats, spectral_stats, eigval_stats, compute_list_eigh, spectral_filter_stats\n",
    "#from util.molecular_eval import BasicMolecularMetrics\n",
    "\n",
    "\n",
    "class SpectreGraphDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\" This class can be used to load the comm20, sbm and planar datasets. \"\"\"\n",
    "        base_path = 'SPECTRE/data/'\n",
    "        filename = os.path.join(base_path, data_file)\n",
    "        self.adjs, self.eigvals, self.eigvecs, self.n_nodes, self.max_eigval, self.min_eigval, self.same_sample, self.n_max = torch.load(\n",
    "            filename)\n",
    "        print(f'Dataset {filename} loaded from file')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adjs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adj = self.adjs[idx]\n",
    "        n = adj.shape[-1]\n",
    "        X = torch.ones(n, 1, dtype=torch.float)\n",
    "        y = torch.zeros([1, 0]).float()\n",
    "        edge_index, _ = torch_geometric.utils.dense_to_sparse(adj)\n",
    "        edge_attr = torch.zeros(edge_index.shape[-1], 2, dtype=torch.float)\n",
    "        edge_attr[:, 1] = 1\n",
    "        num_nodes = n * torch.ones(1, dtype=torch.long)\n",
    "        data = torch_geometric.data.Data(x=X, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                                         y=y, idx=idx, n_nodes=num_nodes)\n",
    "        return data\n",
    "\n",
    "\n",
    "class PlanarDataset(SpectreGraphDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__('planar_64_200.pt')\n",
    "\n",
    "\n",
    "PlanarDataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9QJ8aL44va_"
   },
   "outputs": [],
   "source": [
    "\n",
    "adjs, eigvals, eigvecs, n_nodes, max_eigval, min_eigval, same_sample, n_max = torch.load('SPECTRE/data/planar_64_200.pt')\n",
    "adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rZedjy0wJxy"
   },
   "outputs": [],
   "source": [
    "graphs = adjs\n",
    "test_len = int(round(len(graphs) * 0.2))\n",
    "train_len = int(round((len(graphs) - test_len) * 0.8))\n",
    "val_len = len(graphs) - train_len - test_len\n",
    "print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
    "splits = random_split(graphs, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK2hS79yrqST"
   },
   "source": [
    "### next try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_GqY9tyuKpA"
   },
   "outputs": [],
   "source": [
    "os.system(\"pip install omegaconf\")\n",
    "os.system(\"pip install pytorch_lightning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJJ8ICD9vgCQ"
   },
   "outputs": [],
   "source": [
    "os.system(\"pip install overrides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGc8eQCbrrd0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.system(\"git clone https://github.com/cvignac/DiGress.git\")\n",
    "os.system(\"cp -r DiGress/src src/\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset\n",
    "import torch_geometric.utils\n",
    "\n",
    "from src.datasets.abstract_dataset import AbstractDataModule, AbstractDatasetInfos\n",
    "\n",
    "\n",
    "class SpectreGraphDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\" This class can be used to load the comm20, sbm and planar datasets. \"\"\"\n",
    "        base_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, 'data')\n",
    "        filename = os.path.join(base_path, data_file)\n",
    "        self.adjs, self.eigvals, self.eigvecs, self.n_nodes, self.max_eigval, self.min_eigval, self.same_sample, self.n_max = torch.load(\n",
    "            filename)\n",
    "        print(f'Dataset {filename} loaded from file')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adjs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adj = self.adjs[idx]\n",
    "        n = adj.shape[-1]\n",
    "        X = torch.ones(n, 1, dtype=torch.float)\n",
    "        y = torch.zeros([1, 0]).float()\n",
    "        edge_index, _ = torch_geometric.utils.dense_to_sparse(adj)\n",
    "        edge_attr = torch.zeros(edge_index.shape[-1], 2, dtype=torch.float)\n",
    "        edge_attr[:, 1] = 1\n",
    "        num_nodes = n * torch.ones(1, dtype=torch.long)\n",
    "        data = torch_geometric.data.Data(x=X, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                                         y=y, idx=idx, n_nodes=num_nodes)\n",
    "        return data\n",
    "\n",
    "class SpectreGraphDataModule(AbstractDataModule):\n",
    "    def __init__(self, cfg, n_graphs=200):\n",
    "        super().__init__(cfg)\n",
    "        self.n_graphs = n_graphs\n",
    "        self.prepare_data()\n",
    "        self.inner = self.train_dataloader()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.inner[item]\n",
    "\n",
    "    def prepare_data(self, graphs):\n",
    "        test_len = int(round(len(graphs) * 0.2))\n",
    "        train_len = int(round((len(graphs) - test_len) * 0.8))\n",
    "        val_len = len(graphs) - train_len - test_len\n",
    "        print(f'Dataset sizes: train {train_len}, val {val_len}, test {test_len}')\n",
    "        splits = random_split(graphs, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(1234))\n",
    "\n",
    "        datasets = {'train': splits[0], 'val': splits[1], 'test': splits[2]}\n",
    "        super().prepare_data(datasets)\n",
    "\n",
    "class Comm20Dataset(SpectreGraphDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__('community_12_21_100.pt')\n",
    "\n",
    "\n",
    "class SBMDataset(SpectreGraphDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__('sbm_200.pt')\n",
    "\n",
    "\n",
    "class PlanarDataset(SpectreGraphDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__('planar_64_200.pt')\n",
    "\n",
    "dataset = PlanarDataset()\n",
    "dataset.prepare_data\n",
    "dataset.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbPFtDS8x810"
   },
   "source": [
    "# Quality Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oSqDFX4x_7C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCDeFcFFyVRQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"pip install pyemd\")\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s_ZCqiiyVfj"
   },
   "outputs": [],
   "source": [
    "import pyemd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PRINT_TIME = False\n",
    "\n",
    "def degree_worker(G):\n",
    "    return np.array(nx.degree_histogram(G))\n",
    "\n",
    "def degree_stats(graph_ref_list, graph_pred_list, is_parallel=True, compute_emd=False):\n",
    "    ''' Compute the distance between the degree distributions of two unordered sets of graphs.\n",
    "        Args:\n",
    "            graph_ref_list, graph_target_list: two lists of networkx graphs to be evaluated\n",
    "        '''\n",
    "    sample_ref = []\n",
    "    sample_pred = []\n",
    "    # in case an empty graph is generated\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    prev = datetime.now()\n",
    "    if is_parallel:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_ref_list):\n",
    "                sample_ref.append(deg_hist)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_pred_list_remove_empty):\n",
    "                sample_pred.append(deg_hist)\n",
    "    else:\n",
    "        for i in range(len(graph_ref_list)):\n",
    "            degree_temp = np.array(nx.degree_histogram(graph_ref_list[i]))\n",
    "            sample_ref.append(degree_temp)\n",
    "        for i in range(len(graph_pred_list_remove_empty)):\n",
    "            degree_temp = np.array(\n",
    "                nx.degree_histogram(graph_pred_list_remove_empty[i]))\n",
    "            sample_pred.append(degree_temp)\n",
    "\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
    "    if compute_emd:\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd)\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv)\n",
    "    # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian)\n",
    "\n",
    "    elapsed = datetime.now() - prev\n",
    "    if PRINT_TIME:\n",
    "        print('Time computing degree mmd: ', elapsed)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "def clustering_worker(param):\n",
    "    G, bins = param\n",
    "    clustering_coeffs_list = list(nx.clustering(G).values())\n",
    "    hist, _ = np.histogram(\n",
    "        clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def clustering_stats(graph_ref_list,\n",
    "                     graph_pred_list,\n",
    "                     bins=100,\n",
    "                     is_parallel=True, compute_emd=False):\n",
    "    sample_ref = []\n",
    "    sample_pred = []\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    prev = datetime.now()\n",
    "    if is_parallel:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for clustering_hist in executor.map(clustering_worker,\n",
    "                                                [(G, bins) for G in graph_ref_list]):\n",
    "                sample_ref.append(clustering_hist)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for clustering_hist in executor.map(\n",
    "                    clustering_worker, [(G, bins) for G in graph_pred_list_remove_empty]):\n",
    "                sample_pred.append(clustering_hist)\n",
    "\n",
    "        # check non-zero elements in hist\n",
    "        # total = 0\n",
    "        # for i in range(len(sample_pred)):\n",
    "        #    nz = np.nonzero(sample_pred[i])[0].shape[0]\n",
    "        #    total += nz\n",
    "        # print(total)\n",
    "    else:\n",
    "        for i in range(len(graph_ref_list)):\n",
    "            clustering_coeffs_list = list(nx.clustering(graph_ref_list[i]).values())\n",
    "            hist, _ = np.histogram(\n",
    "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "            sample_ref.append(hist)\n",
    "\n",
    "        for i in range(len(graph_pred_list_remove_empty)):\n",
    "            clustering_coeffs_list = list(\n",
    "                nx.clustering(graph_pred_list_remove_empty[i]).values())\n",
    "            hist, _ = np.histogram(\n",
    "                clustering_coeffs_list, bins=bins, range=(0.0, 1.0), density=False)\n",
    "            sample_pred.append(hist)\n",
    "\n",
    "    if compute_emd:\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        # mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=emd, sigma=1.0 / 10)\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd, sigma=1.0 / 10, distance_scaling=bins)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv, sigma=1.0 / 10)\n",
    "\n",
    "    elapsed = datetime.now() - prev\n",
    "    if PRINT_TIME:\n",
    "        print('Time computing clustering mmd: ', elapsed)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "# maps motif/orbit name string to its corresponding list of indices from orca output\n",
    "motif_to_indices = {\n",
    "    '3path': [1, 2],\n",
    "    '4cycle': [8],\n",
    "}\n",
    "COUNT_START_STR = 'orbit counts:'\n",
    "\n",
    "\n",
    "def edge_list_reindexed(G):\n",
    "    idx = 0\n",
    "    id2idx = dict()\n",
    "    for u in G.nodes():\n",
    "        id2idx[str(u)] = idx\n",
    "        idx += 1\n",
    "\n",
    "    edges = []\n",
    "    for (u, v) in G.edges():\n",
    "        edges.append((id2idx[str(u)], id2idx[str(v)]))\n",
    "    return edges\n",
    "\n",
    "def orca(graph):\n",
    "    # tmp_fname = f'analysis/orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
    "    tmp_fname = f'orca/tmp_{\"\".join(secrets.choice(ascii_uppercase + digits) for i in range(8))}.txt'\n",
    "    tmp_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)), tmp_fname)\n",
    "    # print(tmp_fname, flush=True)\n",
    "    f = open(tmp_fname, 'w')\n",
    "    f.write(\n",
    "        str(graph.number_of_nodes()) + ' ' + str(graph.number_of_edges()) + '\\n')\n",
    "    for (u, v) in edge_list_reindexed(graph):\n",
    "        f.write(str(u) + ' ' + str(v) + '\\n')\n",
    "    f.close()\n",
    "    output = sp.check_output(\n",
    "        [str(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'orca/orca')), 'node', '4', tmp_fname, 'std'])\n",
    "    output = output.decode('utf8').strip()\n",
    "    idx = output.find(COUNT_START_STR) + len(COUNT_START_STR) + 2\n",
    "    output = output[idx:]\n",
    "    node_orbit_counts = np.array([\n",
    "        list(map(int,\n",
    "                 node_cnts.strip().split(' ')))\n",
    "        for node_cnts in output.strip('\\n').split('\\n')\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        os.remove(tmp_fname)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    return node_orbit_counts\n",
    "\n",
    "\n",
    "def orbit_stats_all(graph_ref_list, graph_pred_list, compute_emd=False):\n",
    "    total_counts_ref = []\n",
    "    total_counts_pred = []\n",
    "\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    for G in graph_ref_list:\n",
    "        orbit_counts = orca(G)\n",
    "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
    "        total_counts_ref.append(orbit_counts_graph)\n",
    "\n",
    "    for G in graph_pred_list:\n",
    "        orbit_counts = orca(G)\n",
    "        orbit_counts_graph = np.sum(orbit_counts, axis=0) / G.number_of_nodes()\n",
    "        total_counts_pred.append(orbit_counts_graph)\n",
    "\n",
    "    total_counts_ref = np.array(total_counts_ref)\n",
    "    total_counts_pred = np.array(total_counts_pred)\n",
    "\n",
    "    # mmd_dist = compute_mmd(\n",
    "    #     total_counts_ref,\n",
    "    #     total_counts_pred,\n",
    "    #     kernel=gaussian,\n",
    "    #     is_hist=False,\n",
    "    #     sigma=30.0)\n",
    "\n",
    "    # mmd_dist = compute_mmd(\n",
    "    #         total_counts_ref,\n",
    "    #         total_counts_pred,\n",
    "    #         kernel=gaussian_tv,\n",
    "    #         is_hist=False,\n",
    "    #         sigma=30.0)  \n",
    "\n",
    "    if compute_emd:\n",
    "        # mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=emd, sigma=30.0)\n",
    "        # EMD option uses the same computation as GraphRNN, the alternative is MMD as computed by GRAN\n",
    "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian, is_hist=False, sigma=30.0)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(total_counts_ref, total_counts_pred, kernel=gaussian_tv, is_hist=False, sigma=30.0)\n",
    "    return mmd_dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gaussian(x, y, sigma=1.0):\n",
    "    support_size = max(len(x), len(y))\n",
    "    # convert histogram values x and y to float, and make them equal len\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    if len(x) < len(y):\n",
    "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
    "    elif len(y) < len(x):\n",
    "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
    "\n",
    "    dist = np.linalg.norm(x - y, 2)\n",
    "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
    "\n",
    "\n",
    "def gaussian_tv(x, y, sigma=1.0):  \n",
    "    support_size = max(len(x), len(y))\n",
    "    # convert histogram values x and y to float, and make them equal len\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    if len(x) < len(y):\n",
    "        x = np.hstack((x, [0.0] * (support_size - len(x))))\n",
    "    elif len(y) < len(x):\n",
    "        y = np.hstack((y, [0.0] * (support_size - len(y))))\n",
    "\n",
    "    dist = np.abs(x - y).sum() / 2.0\n",
    "    return np.exp(-dist * dist / (2 * sigma * sigma))\n",
    "\n",
    "\n",
    "def kernel_parallel_unpacked(x, samples2, kernel):\n",
    "    d = 0\n",
    "    for s2 in samples2:\n",
    "        d += kernel(x, s2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def kernel_parallel_worker(t):\n",
    "    return kernel_parallel_unpacked(*t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def disc(samples1, samples2, kernel, is_parallel=True, *args, **kwargs):\n",
    "    ''' Discrepancy between 2 samples '''\n",
    "    d = 0\n",
    "\n",
    "    if not is_parallel:\n",
    "        for s1 in samples1:\n",
    "            for s2 in samples2:\n",
    "                d += kernel(s1, s2, *args, **kwargs)\n",
    "    else:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for dist in executor.map(kernel_parallel_worker, [\n",
    "                    (s1, samples2, partial(kernel, *args, **kwargs)) for s1 in samples1\n",
    "            ]):\n",
    "                d += dist\n",
    "    if len(samples1) * len(samples2) > 0:\n",
    "        d /= len(samples1) * len(samples2)\n",
    "    else:\n",
    "        d = 1e+6\n",
    "    #print('error')\n",
    "    return d\n",
    "\n",
    "\n",
    "def compute_mmd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' MMD between two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [s1 / (np.sum(s1) + 1e-6) for s1 in samples1]\n",
    "        samples2 = [s2 / (np.sum(s2) + 1e-6) for s2 in samples2]\n",
    "    print(samples1)\n",
    "    return disc(samples1, samples1, kernel, *args, **kwargs) + disc(samples2, samples2, kernel, *args, **kwargs) - \\\n",
    "                2 * disc(samples1, samples2, kernel, *args, **kwargs)\n",
    "\n",
    "\n",
    "def compute_emd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' EMD between average of two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [np.mean(samples1)]\n",
    "        samples2 = [np.mean(samples2)]\n",
    "    return disc(samples1, samples2, kernel, *args,\n",
    "                            **kwargs), [samples1[0], samples2[0]]\n",
    "\n",
    "\n",
    "\n",
    "def compute_mmd_stats(graphs_ref_nx, graphs_pred_nx):\n",
    "    return degree_stats(graphs_ref_nx, graphs_pred_nx), clustering_stats(graphs_ref_nx, graphs_pred_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pscy53ZLz9Se"
   },
   "outputs": [],
   "source": [
    "samples1 = [np.array([1,2,3.0]),  np.array([1,20,4.0])]\n",
    "samples2 = [np.array([1,2,3.5]),  np.array([1,20,4.5])]\n",
    "\n",
    "compute_mmd(samples1, samples2, kernel=gaussian_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTAYm3qa2rr_"
   },
   "outputs": [],
   "source": [
    "samples1 = [np.array([1,200,3.0]),  np.array([1,20,4.0])]\n",
    "samples2 = [np.array([1,2,3.01]),  np.array([1,20,4.01])]\n",
    "\n",
    "compute_mmd(samples1, samples2, kernel=gaussian_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpfa0Ldw0Pkm"
   },
   "outputs": [],
   "source": [
    "graph_ref_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
    "graph_pred_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
    "\n",
    "degree_stats(graph_ref_list, graph_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVv3mnMY-fal"
   },
   "outputs": [],
   "source": [
    "graph_ref_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
    "graph_pred_list = [nx.erdos_renyi_graph(15, 0.9) for _ in range(15) ]\n",
    "\n",
    "degree_stats(graph_ref_list, graph_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ScNBY6Z-vqM"
   },
   "outputs": [],
   "source": [
    "graph_ref_list = [nx.erdos_renyi_graph(15, 0.1) for _ in range(15) ]\n",
    "graph_pred_list = [nx.erdos_renyi_graph(15, 0.5) for _ in range(15) ]\n",
    "\n",
    "clustering_stats(graph_ref_list, graph_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O9ZYYVk4rWL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMR74OtRj8zkHMB/tLBN7/H",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
